[{"content":"Demo Demo ","permalink":"http://localhost:1313/blogs/cara-menggunakan-tensorflow/","summary":"Cara Menggunakan Tensorflow","title":"Cara Menggunakan Tensorflow"},{"content":"Panduan Preprocessing Data yang Baik dalam Machine Learning Preprocessing adalah langkah penting dalam alur kerja data science dan machine learning. Data yang baik adalah kunci untuk model yang akurat dan dapat diandalkan. Artikel ini akan membahas pentingnya preprocessing data dan beberapa teknik yang umum digunakan untuk mempersiapkan data sebelum digunakan dalam pelatihan model machine learning.\nApa itu Preprocessing Data? Preprocessing data adalah serangkaian langkah yang dilakukan untuk membersihkan, memformat, dan mengubah data mentah agar siap digunakan dalam analisis atau pembuatan model machine learning. Langkah-langkah ini bertujuan untuk meningkatkan kualitas data dan memastikan model yang dibangun dapat memberikan hasil yang optimal.\nMengapa Preprocessing Data Itu Penting? Data yang tidak diproses dengan baik dapat mengarah pada model yang buruk atau bahkan tidak dapat digunakan sama sekali. Berikut beberapa alasan mengapa preprocessing sangat penting:\nMembersihkan Data: Menghapus noise, menangani nilai yang hilang, atau memperbaiki kesalahan data. Meningkatkan Akurasi Model: Dengan data yang bersih dan relevan, model machine learning dapat belajar dengan lebih baik. Menghindari Bias: Preprocessing membantu menghindari bias dalam data yang dapat memengaruhi prediksi model. Meningkatkan Kecepatan Pelatihan: Beberapa teknik preprocessing dapat mempercepat waktu pelatihan model. Langkah-langkah Preprocessing yang Umum 1. Menangani Nilai yang Hilang Data yang hilang adalah masalah umum dalam dataset dunia nyata. Ada beberapa cara untuk menangani nilai yang hilang:\nMenghapus Baris atau Kolom: Jika proporsi data yang hilang terlalu besar, bisa saja lebih baik untuk menghapusnya. Imputasi: Mengisi nilai yang hilang dengan statistik seperti rata-rata, median, atau modus. Interpolasi: Menggunakan nilai terdekat untuk mengisi data yang hilang, sering digunakan dalam data time series. 1 2 3 4 import pandas as pd # Imputasi menggunakan rata-rata df.fillna(df.mean(), inplace=True) 2. Normalisasi dan Standarisasi Proses ini penting ketika fitur dalam dataset memiliki skala yang sangat berbeda. Model machine learning seperti K-Nearest Neighbors (KNN) atau SVM sangat dipengaruhi oleh skala fitur.\nNormalisasi: Menyelaraskan data ke dalam rentang [0, 1] atau [-1, 1]. Standarisasi: Mengubah data sehingga memiliki rata-rata 0 dan deviasi standar 1. 1 2 3 4 5 6 7 8 9 from sklearn.preprocessing import StandardScaler, MinMaxScaler # Standarisasi scaler = StandardScaler() df_scaled = scaler.fit_transform(df) # Normalisasi min_max_scaler = MinMaxScaler() df_normalized = min_max_scaler.fit_transform(df) 3. Pengkodean Kategorikal Data kategorikal harus dikonversi ke format numerik agar dapat digunakan dalam model machine learning. Ada beberapa cara untuk melakukan ini:\nLabel Encoding: Mengonversi kategori menjadi angka. One-Hot Encoding: Membuat kolom biner untuk setiap kategori. 1 2 3 4 5 6 7 8 from sklearn.preprocessing import LabelEncoder, OneHotEncoder # Label Encoding encoder = LabelEncoder() df[\u0026#39;encoded_column\u0026#39;] = encoder.fit_transform(df[\u0026#39;category_column\u0026#39;]) # One-Hot Encoding df_encoded = pd.get_dummies(df[\u0026#39;category_column\u0026#39;]) 4. Deteksi dan Penanganan Outlier Outlier adalah nilai ekstrem yang bisa mempengaruhi model dan analisis. Ada beberapa cara untuk menangani outlier:\nMenghapus Outlier: Menghapus nilai yang terlalu jauh dari rata-rata. Transformasi: Menggunakan transformasi matematis seperti log atau kuadrat untuk meredam efek outlier. 1 2 3 4 5 6 7 # Deteksi outlier menggunakan IQR Q1 = df[\u0026#39;column\u0026#39;].quantile(0.25) Q3 = df[\u0026#39;column\u0026#39;].quantile(0.75) IQR = Q3 - Q1 # Menghapus outlier df_no_outliers = df[(df[\u0026#39;column\u0026#39;] \u0026gt;= (Q1 - 1.5 * IQR)) \u0026amp; (df[\u0026#39;column\u0026#39;] \u0026lt;= (Q3 + 1.5 * IQR))] 5. Pembagian Data untuk Pelatihan dan Pengujian Sebelum melatih model, penting untuk membagi data menjadi dua set: data pelatihan dan data pengujian. Pembagian yang umum adalah 80% untuk pelatihan dan 20% untuk pengujian.\n1 2 3 4 from sklearn.model_selection import train_test_split # Membagi data X_train, X_test, y_train, y_test = train_test_split(df.drop(\u0026#39;target\u0026#39;, axis=1), df[\u0026#39;target\u0026#39;], test_size=0.2, random_state=42) Kesimpulan Preprocessing data adalah langkah krusial dalam pipeline machine learning. Dengan menangani nilai yang hilang, normalisasi, pengkodean kategorikal, dan deteksi outlier, kita dapat memastikan bahwa model machine learning kita dilatih dengan data yang berkualitas tinggi. Meskipun preprocessing bisa memakan waktu, langkah ini sangat berpengaruh terhadap kinerja dan akurasi model yang dihasilkan. Pastikan untuk selalu memeriksa dataset Anda dan menerapkan teknik preprocessing yang tepat untuk mencapai hasil yang terbaik.\n","permalink":"http://localhost:1313/blogs/cara-preprocessing-data-dengan-baik/","summary":"Cara Preprocessing Data dengan Baik","title":"Cara Preprocessing Data dengan Baik"},{"content":"Demo Demo ","permalink":"http://localhost:1313/blogs/cara-menggunakan-numpy/","summary":"Exploratory Data Analysis of Amazon\u0026rsquo;s top 50 bestselling books 2009 - 2019","title":"Cara menggunakan numpy"},{"content":"Introduction The Nota Fiscal Goiana is an economic program implemented by the state of Goiás, Brazil. This program is designed to boost the collection of the Tax on Circulation of Goods and Services (ICMS) by encouraging people to ask for receipts when they buy goods and services within the state. Additionally, the program includes monthly raffles that distribute cash prizes to participating citizens.\nAs I regularly checked the Nota Fiscal Goiana portal and realized the website\u0026rsquo;s simplicity, I decided to write a scraper to collect the raffle results. Initially, I aimed to simplify the process of checking if I had won, but as I delved deeper, I realized the potential to analyze the dataset more comprehensively.\nEarly Design and Idea Phase The original idea was to scrape raffle results, store them in a database, and give people the option to access the data directly from the database or just view it through a good-looking Power BI dashboard.\nThis led me to start writing the code for scraping the website and parsing the tables. However, since there were approximately 50 published results not available on the website, I had to resort to checking the Diário Oficial do Estado de Goiás, where the official government documents of Goiás are published in Brazil.\nChallenges Encountered Not all results were completely published on the Diário Oficial, leading to some results being unavailable.\nThe project doesn\u0026rsquo;t have a budget, so automation can\u0026rsquo;t require VM and online databases. Which Power BI requires both.\nSolutions The results that I could find published on the Diário Oficial were saved in the database along with results available on the website.\nInitially, I was using free options of online databases, and the backup of the old data was on an SQLite available on the project repository. However, I realized that when running the scraper I could save the data on the SQLite and commit the changes, when I did the information persisted on the SQLite file and I could just read it.\nTo help automate the dashboard I resorted to using Streamlit instead of Power BI. Because Streamlit allows me to read the SQLite file directly from the GitHub repository and every time someone opens the Streamlit dashboard it reads the current data, so I don\u0026rsquo;t need to set up refreshes.\nCurrent State of the Project Currently, the project extracts monthly the raffle results from the Nota Fiscal Goiana and the state ICMS collection and saves the results in the SQLite file, using GitHub Actions. The Streamlit dashboard reads the SQLite file and displays the information on the Streamlit Community Cloud.\nLessons Learned While the Power BI dashboard was useful, I faced challenges with refreshing it easily. Initially using an SQLite database, I couldn\u0026rsquo;t connect to the file on the repository, necessitating manual updates on my local machine. To overcome this, I explored alternatives like MySQL and PostgreSQL, enabling automated monthly scraping with GitHub Actions and updating the dashboard through the Power BI website.\nThe SQLite database is really helpful and depending on the size and scope of your project, you should look at it as a possible option to store your data.\nGoing forward I intend to maintain the project, making future patches to the scraping script if the website changes. And improving the code as my knowledge matures.\nExplore Further GitHub Repository: Access the project\u0026rsquo;s source files and codes on our GitHub Repository. Currently, everything is in Portuguese, but I plan to translate it in the future.\nStreamlit Dashboard: Explore the live Streamlit dashboard here to interact with the project\u0026rsquo;s data visualization directly.\nAcademic Article: If you\u0026rsquo;re interested in a detailed academic analysis of this project, check out our article here. Please note that the article is in Portuguese.\n","permalink":"http://localhost:1313/blogs/bts-nota-fiscal-goiana/","summary":"A behind-the-scenes decision on the decision-making process of the project Nota Fiscal Goiana.","title":"[BTS] Nota Fiscal Goiana"},{"content":"Have you ever had a purchase blocked by your credit card issuer without you taking any action? If this happened to you, your credit card issuer probably had detected that your purchase didn\u0026rsquo;t match your normal spending habit, maybe you were at a different location or spend a lot of money suddenly. Anyway, the credit card company was using anomaly detection techniques to prevent fraudulent activities.\nWhat is Anomaly Detection? Anomaly detection refers to the problem of finding patterns in data that do not conform to expected behavior. 1\nIn statistics there is a famous distribution that is called \u0026ldquo;The Normal Distribution\u0026rdquo;, this name is not in vain. A lot of random things are normally distributed, for instance, if you select 100 random people and measure their height, and made a distribution plot, you\u0026rsquo;d end up with something looking like a symmetrical bell curve.\nEven though abnormal results are possible, it is expected that most people or things will fall within a normal distribution. When something is divergent from this normality it is said to be an outlier or an anomaly.\nWhen dealing with something like credit card fraud you can create an expectation of a person expanding habits, so if your daily routine consists of waking up and driving to your workplace and buying a coffee, then lunch in New York. Your credit card company would find it weird if you suddenly made a physical purchase in Rio de Janeiro.\nIn simple terms, that is what anomaly detection is trying to do, understand how something works so it can identify if it starts acting weird.\nAnomaly Detection x Class Imbalance in Datasets If you are aware of what class imbalance is, you may be wondering \u0026ldquo;can\u0026rsquo;t I use anomaly detection to solve this class imbalance problem?\u0026rdquo; - sometimes you can, but it\u0026rsquo;s not appropriate.\nClass imbalance occurs when one class has significantly more samples than the other class(es). This can lead to biased models that favor the majority class and perform poorly on minority classes. While anomaly detection focuses on identifying rare or abnormal data points that deviate significantly from the normal patterns in the dataset.\nEven though it has some overlap on what is trying to do, unless you have a class imbalance problem that the minority class can be considered an abnormal behavior, you won\u0026rsquo;t be able to use anomaly detection.\nI talked about class imbalance before on a churn rate problem in a fictitious telecommunication company, if you are interested you can check it here.\nChandola, V., Banerjee, A., and Kumar, V. 2009. Anomaly detection: A survey. ACM Comput. Surv. 41, 3, Article 15 (July 2009), 58 pages. DOI = 10.1145/1541880.1541882\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/blogs/anomaly-detection/","summary":"How can your credit card issuer know if a purchase is made by you or a cloned version of your credit card?","title":"What is Anomaly Detection?"},{"content":"Demo Demo ","permalink":"http://localhost:1313/blogs/cara-menggunakan-tensorflow/","summary":"Cara Menggunakan Tensorflow","title":"Cara Menggunakan Tensorflow"},{"content":"Panduan Preprocessing Data yang Baik dalam Machine Learning Preprocessing adalah langkah penting dalam alur kerja data science dan machine learning. Data yang baik adalah kunci untuk model yang akurat dan dapat diandalkan. Artikel ini akan membahas pentingnya preprocessing data dan beberapa teknik yang umum digunakan untuk mempersiapkan data sebelum digunakan dalam pelatihan model machine learning.\nApa itu Preprocessing Data? Preprocessing data adalah serangkaian langkah yang dilakukan untuk membersihkan, memformat, dan mengubah data mentah agar siap digunakan dalam analisis atau pembuatan model machine learning. Langkah-langkah ini bertujuan untuk meningkatkan kualitas data dan memastikan model yang dibangun dapat memberikan hasil yang optimal.\nMengapa Preprocessing Data Itu Penting? Data yang tidak diproses dengan baik dapat mengarah pada model yang buruk atau bahkan tidak dapat digunakan sama sekali. Berikut beberapa alasan mengapa preprocessing sangat penting:\nMembersihkan Data: Menghapus noise, menangani nilai yang hilang, atau memperbaiki kesalahan data. Meningkatkan Akurasi Model: Dengan data yang bersih dan relevan, model machine learning dapat belajar dengan lebih baik. Menghindari Bias: Preprocessing membantu menghindari bias dalam data yang dapat memengaruhi prediksi model. Meningkatkan Kecepatan Pelatihan: Beberapa teknik preprocessing dapat mempercepat waktu pelatihan model. Langkah-langkah Preprocessing yang Umum 1. Menangani Nilai yang Hilang Data yang hilang adalah masalah umum dalam dataset dunia nyata. Ada beberapa cara untuk menangani nilai yang hilang:\nMenghapus Baris atau Kolom: Jika proporsi data yang hilang terlalu besar, bisa saja lebih baik untuk menghapusnya. Imputasi: Mengisi nilai yang hilang dengan statistik seperti rata-rata, median, atau modus. Interpolasi: Menggunakan nilai terdekat untuk mengisi data yang hilang, sering digunakan dalam data time series. 1 2 3 4 import pandas as pd # Imputasi menggunakan rata-rata df.fillna(df.mean(), inplace=True) 2. Normalisasi dan Standarisasi Proses ini penting ketika fitur dalam dataset memiliki skala yang sangat berbeda. Model machine learning seperti K-Nearest Neighbors (KNN) atau SVM sangat dipengaruhi oleh skala fitur.\nNormalisasi: Menyelaraskan data ke dalam rentang [0, 1] atau [-1, 1]. Standarisasi: Mengubah data sehingga memiliki rata-rata 0 dan deviasi standar 1. 1 2 3 4 5 6 7 8 9 from sklearn.preprocessing import StandardScaler, MinMaxScaler # Standarisasi scaler = StandardScaler() df_scaled = scaler.fit_transform(df) # Normalisasi min_max_scaler = MinMaxScaler() df_normalized = min_max_scaler.fit_transform(df) 3. Pengkodean Kategorikal Data kategorikal harus dikonversi ke format numerik agar dapat digunakan dalam model machine learning. Ada beberapa cara untuk melakukan ini:\nLabel Encoding: Mengonversi kategori menjadi angka. One-Hot Encoding: Membuat kolom biner untuk setiap kategori. 1 2 3 4 5 6 7 8 from sklearn.preprocessing import LabelEncoder, OneHotEncoder # Label Encoding encoder = LabelEncoder() df[\u0026#39;encoded_column\u0026#39;] = encoder.fit_transform(df[\u0026#39;category_column\u0026#39;]) # One-Hot Encoding df_encoded = pd.get_dummies(df[\u0026#39;category_column\u0026#39;]) 4. Deteksi dan Penanganan Outlier Outlier adalah nilai ekstrem yang bisa mempengaruhi model dan analisis. Ada beberapa cara untuk menangani outlier:\nMenghapus Outlier: Menghapus nilai yang terlalu jauh dari rata-rata. Transformasi: Menggunakan transformasi matematis seperti log atau kuadrat untuk meredam efek outlier. 1 2 3 4 5 6 7 # Deteksi outlier menggunakan IQR Q1 = df[\u0026#39;column\u0026#39;].quantile(0.25) Q3 = df[\u0026#39;column\u0026#39;].quantile(0.75) IQR = Q3 - Q1 # Menghapus outlier df_no_outliers = df[(df[\u0026#39;column\u0026#39;] \u0026gt;= (Q1 - 1.5 * IQR)) \u0026amp; (df[\u0026#39;column\u0026#39;] \u0026lt;= (Q3 + 1.5 * IQR))] 5. Pembagian Data untuk Pelatihan dan Pengujian Sebelum melatih model, penting untuk membagi data menjadi dua set: data pelatihan dan data pengujian. Pembagian yang umum adalah 80% untuk pelatihan dan 20% untuk pengujian.\n1 2 3 4 from sklearn.model_selection import train_test_split # Membagi data X_train, X_test, y_train, y_test = train_test_split(df.drop(\u0026#39;target\u0026#39;, axis=1), df[\u0026#39;target\u0026#39;], test_size=0.2, random_state=42) Kesimpulan Preprocessing data adalah langkah krusial dalam pipeline machine learning. Dengan menangani nilai yang hilang, normalisasi, pengkodean kategorikal, dan deteksi outlier, kita dapat memastikan bahwa model machine learning kita dilatih dengan data yang berkualitas tinggi. Meskipun preprocessing bisa memakan waktu, langkah ini sangat berpengaruh terhadap kinerja dan akurasi model yang dihasilkan. Pastikan untuk selalu memeriksa dataset Anda dan menerapkan teknik preprocessing yang tepat untuk mencapai hasil yang terbaik.\n","permalink":"http://localhost:1313/blogs/cara-preprocessing-data-dengan-baik/","summary":"Cara Preprocessing Data dengan Baik","title":"Cara Preprocessing Data dengan Baik"},{"content":"Demo Demo ","permalink":"http://localhost:1313/blogs/cara-menggunakan-numpy/","summary":"Exploratory Data Analysis of Amazon\u0026rsquo;s top 50 bestselling books 2009 - 2019","title":"Cara menggunakan numpy"},{"content":"Introduction The Nota Fiscal Goiana is an economic program implemented by the state of Goiás, Brazil. This program is designed to boost the collection of the Tax on Circulation of Goods and Services (ICMS) by encouraging people to ask for receipts when they buy goods and services within the state. Additionally, the program includes monthly raffles that distribute cash prizes to participating citizens.\nAs I regularly checked the Nota Fiscal Goiana portal and realized the website\u0026rsquo;s simplicity, I decided to write a scraper to collect the raffle results. Initially, I aimed to simplify the process of checking if I had won, but as I delved deeper, I realized the potential to analyze the dataset more comprehensively.\nEarly Design and Idea Phase The original idea was to scrape raffle results, store them in a database, and give people the option to access the data directly from the database or just view it through a good-looking Power BI dashboard.\nThis led me to start writing the code for scraping the website and parsing the tables. However, since there were approximately 50 published results not available on the website, I had to resort to checking the Diário Oficial do Estado de Goiás, where the official government documents of Goiás are published in Brazil.\nChallenges Encountered Not all results were completely published on the Diário Oficial, leading to some results being unavailable.\nThe project doesn\u0026rsquo;t have a budget, so automation can\u0026rsquo;t require VM and online databases. Which Power BI requires both.\nSolutions The results that I could find published on the Diário Oficial were saved in the database along with results available on the website.\nInitially, I was using free options of online databases, and the backup of the old data was on an SQLite available on the project repository. However, I realized that when running the scraper I could save the data on the SQLite and commit the changes, when I did the information persisted on the SQLite file and I could just read it.\nTo help automate the dashboard I resorted to using Streamlit instead of Power BI. Because Streamlit allows me to read the SQLite file directly from the GitHub repository and every time someone opens the Streamlit dashboard it reads the current data, so I don\u0026rsquo;t need to set up refreshes.\nCurrent State of the Project Currently, the project extracts monthly the raffle results from the Nota Fiscal Goiana and the state ICMS collection and saves the results in the SQLite file, using GitHub Actions. The Streamlit dashboard reads the SQLite file and displays the information on the Streamlit Community Cloud.\nLessons Learned While the Power BI dashboard was useful, I faced challenges with refreshing it easily. Initially using an SQLite database, I couldn\u0026rsquo;t connect to the file on the repository, necessitating manual updates on my local machine. To overcome this, I explored alternatives like MySQL and PostgreSQL, enabling automated monthly scraping with GitHub Actions and updating the dashboard through the Power BI website.\nThe SQLite database is really helpful and depending on the size and scope of your project, you should look at it as a possible option to store your data.\nGoing forward I intend to maintain the project, making future patches to the scraping script if the website changes. And improving the code as my knowledge matures.\nExplore Further GitHub Repository: Access the project\u0026rsquo;s source files and codes on our GitHub Repository. Currently, everything is in Portuguese, but I plan to translate it in the future.\nStreamlit Dashboard: Explore the live Streamlit dashboard here to interact with the project\u0026rsquo;s data visualization directly.\nAcademic Article: If you\u0026rsquo;re interested in a detailed academic analysis of this project, check out our article here. Please note that the article is in Portuguese.\n","permalink":"http://localhost:1313/blogs/bts-nota-fiscal-goiana/","summary":"A behind-the-scenes decision on the decision-making process of the project Nota Fiscal Goiana.","title":"[BTS] Nota Fiscal Goiana"},{"content":"Have you ever had a purchase blocked by your credit card issuer without you taking any action? If this happened to you, your credit card issuer probably had detected that your purchase didn\u0026rsquo;t match your normal spending habit, maybe you were at a different location or spend a lot of money suddenly. Anyway, the credit card company was using anomaly detection techniques to prevent fraudulent activities.\nWhat is Anomaly Detection? Anomaly detection refers to the problem of finding patterns in data that do not conform to expected behavior. 1\nIn statistics there is a famous distribution that is called \u0026ldquo;The Normal Distribution\u0026rdquo;, this name is not in vain. A lot of random things are normally distributed, for instance, if you select 100 random people and measure their height, and made a distribution plot, you\u0026rsquo;d end up with something looking like a symmetrical bell curve.\nEven though abnormal results are possible, it is expected that most people or things will fall within a normal distribution. When something is divergent from this normality it is said to be an outlier or an anomaly.\nWhen dealing with something like credit card fraud you can create an expectation of a person expanding habits, so if your daily routine consists of waking up and driving to your workplace and buying a coffee, then lunch in New York. Your credit card company would find it weird if you suddenly made a physical purchase in Rio de Janeiro.\nIn simple terms, that is what anomaly detection is trying to do, understand how something works so it can identify if it starts acting weird.\nAnomaly Detection x Class Imbalance in Datasets If you are aware of what class imbalance is, you may be wondering \u0026ldquo;can\u0026rsquo;t I use anomaly detection to solve this class imbalance problem?\u0026rdquo; - sometimes you can, but it\u0026rsquo;s not appropriate.\nClass imbalance occurs when one class has significantly more samples than the other class(es). This can lead to biased models that favor the majority class and perform poorly on minority classes. While anomaly detection focuses on identifying rare or abnormal data points that deviate significantly from the normal patterns in the dataset.\nEven though it has some overlap on what is trying to do, unless you have a class imbalance problem that the minority class can be considered an abnormal behavior, you won\u0026rsquo;t be able to use anomaly detection.\nI talked about class imbalance before on a churn rate problem in a fictitious telecommunication company, if you are interested you can check it here.\nChandola, V., Banerjee, A., and Kumar, V. 2009. Anomaly detection: A survey. ACM Comput. Surv. 41, 3, Article 15 (July 2009), 58 pages. DOI = 10.1145/1541880.1541882\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/blogs/anomaly-detection/","summary":"How can your credit card issuer know if a purchase is made by you or a cloned version of your credit card?","title":"What is Anomaly Detection?"},{"content":"Demo Demo ","permalink":"http://localhost:1313/blogs/cara-menggunakan-tensorflow/","summary":"Cara Menggunakan Tensorflow","title":"Cara Menggunakan Tensorflow"},{"content":"Panduan Preprocessing Data yang Baik dalam Machine Learning Preprocessing adalah langkah penting dalam alur kerja data science dan machine learning. Data yang baik adalah kunci untuk model yang akurat dan dapat diandalkan. Artikel ini akan membahas pentingnya preprocessing data dan beberapa teknik yang umum digunakan untuk mempersiapkan data sebelum digunakan dalam pelatihan model machine learning.\nApa itu Preprocessing Data? Preprocessing data adalah serangkaian langkah yang dilakukan untuk membersihkan, memformat, dan mengubah data mentah agar siap digunakan dalam analisis atau pembuatan model machine learning. Langkah-langkah ini bertujuan untuk meningkatkan kualitas data dan memastikan model yang dibangun dapat memberikan hasil yang optimal.\nMengapa Preprocessing Data Itu Penting? Data yang tidak diproses dengan baik dapat mengarah pada model yang buruk atau bahkan tidak dapat digunakan sama sekali. Berikut beberapa alasan mengapa preprocessing sangat penting:\nMembersihkan Data: Menghapus noise, menangani nilai yang hilang, atau memperbaiki kesalahan data. Meningkatkan Akurasi Model: Dengan data yang bersih dan relevan, model machine learning dapat belajar dengan lebih baik. Menghindari Bias: Preprocessing membantu menghindari bias dalam data yang dapat memengaruhi prediksi model. Meningkatkan Kecepatan Pelatihan: Beberapa teknik preprocessing dapat mempercepat waktu pelatihan model. Langkah-langkah Preprocessing yang Umum 1. Menangani Nilai yang Hilang Data yang hilang adalah masalah umum dalam dataset dunia nyata. Ada beberapa cara untuk menangani nilai yang hilang:\nMenghapus Baris atau Kolom: Jika proporsi data yang hilang terlalu besar, bisa saja lebih baik untuk menghapusnya. Imputasi: Mengisi nilai yang hilang dengan statistik seperti rata-rata, median, atau modus. Interpolasi: Menggunakan nilai terdekat untuk mengisi data yang hilang, sering digunakan dalam data time series. 1 2 3 4 import pandas as pd # Imputasi menggunakan rata-rata df.fillna(df.mean(), inplace=True) 2. Normalisasi dan Standarisasi Proses ini penting ketika fitur dalam dataset memiliki skala yang sangat berbeda. Model machine learning seperti K-Nearest Neighbors (KNN) atau SVM sangat dipengaruhi oleh skala fitur.\nNormalisasi: Menyelaraskan data ke dalam rentang [0, 1] atau [-1, 1]. Standarisasi: Mengubah data sehingga memiliki rata-rata 0 dan deviasi standar 1. 1 2 3 4 5 6 7 8 9 from sklearn.preprocessing import StandardScaler, MinMaxScaler # Standarisasi scaler = StandardScaler() df_scaled = scaler.fit_transform(df) # Normalisasi min_max_scaler = MinMaxScaler() df_normalized = min_max_scaler.fit_transform(df) 3. Pengkodean Kategorikal Data kategorikal harus dikonversi ke format numerik agar dapat digunakan dalam model machine learning. Ada beberapa cara untuk melakukan ini:\nLabel Encoding: Mengonversi kategori menjadi angka. One-Hot Encoding: Membuat kolom biner untuk setiap kategori. 1 2 3 4 5 6 7 8 from sklearn.preprocessing import LabelEncoder, OneHotEncoder # Label Encoding encoder = LabelEncoder() df[\u0026#39;encoded_column\u0026#39;] = encoder.fit_transform(df[\u0026#39;category_column\u0026#39;]) # One-Hot Encoding df_encoded = pd.get_dummies(df[\u0026#39;category_column\u0026#39;]) 4. Deteksi dan Penanganan Outlier Outlier adalah nilai ekstrem yang bisa mempengaruhi model dan analisis. Ada beberapa cara untuk menangani outlier:\nMenghapus Outlier: Menghapus nilai yang terlalu jauh dari rata-rata. Transformasi: Menggunakan transformasi matematis seperti log atau kuadrat untuk meredam efek outlier. 1 2 3 4 5 6 7 # Deteksi outlier menggunakan IQR Q1 = df[\u0026#39;column\u0026#39;].quantile(0.25) Q3 = df[\u0026#39;column\u0026#39;].quantile(0.75) IQR = Q3 - Q1 # Menghapus outlier df_no_outliers = df[(df[\u0026#39;column\u0026#39;] \u0026gt;= (Q1 - 1.5 * IQR)) \u0026amp; (df[\u0026#39;column\u0026#39;] \u0026lt;= (Q3 + 1.5 * IQR))] 5. Pembagian Data untuk Pelatihan dan Pengujian Sebelum melatih model, penting untuk membagi data menjadi dua set: data pelatihan dan data pengujian. Pembagian yang umum adalah 80% untuk pelatihan dan 20% untuk pengujian.\n1 2 3 4 from sklearn.model_selection import train_test_split # Membagi data X_train, X_test, y_train, y_test = train_test_split(df.drop(\u0026#39;target\u0026#39;, axis=1), df[\u0026#39;target\u0026#39;], test_size=0.2, random_state=42) Kesimpulan Preprocessing data adalah langkah krusial dalam pipeline machine learning. Dengan menangani nilai yang hilang, normalisasi, pengkodean kategorikal, dan deteksi outlier, kita dapat memastikan bahwa model machine learning kita dilatih dengan data yang berkualitas tinggi. Meskipun preprocessing bisa memakan waktu, langkah ini sangat berpengaruh terhadap kinerja dan akurasi model yang dihasilkan. Pastikan untuk selalu memeriksa dataset Anda dan menerapkan teknik preprocessing yang tepat untuk mencapai hasil yang terbaik.\n","permalink":"http://localhost:1313/blogs/cara-preprocessing-data-dengan-baik/","summary":"Cara Preprocessing Data dengan Baik","title":"Cara Preprocessing Data dengan Baik"},{"content":"Demo Demo ","permalink":"http://localhost:1313/blogs/cara-menggunakan-numpy/","summary":"Exploratory Data Analysis of Amazon\u0026rsquo;s top 50 bestselling books 2009 - 2019","title":"Cara menggunakan numpy"},{"content":"Introduction The Nota Fiscal Goiana is an economic program implemented by the state of Goiás, Brazil. This program is designed to boost the collection of the Tax on Circulation of Goods and Services (ICMS) by encouraging people to ask for receipts when they buy goods and services within the state. Additionally, the program includes monthly raffles that distribute cash prizes to participating citizens.\nAs I regularly checked the Nota Fiscal Goiana portal and realized the website\u0026rsquo;s simplicity, I decided to write a scraper to collect the raffle results. Initially, I aimed to simplify the process of checking if I had won, but as I delved deeper, I realized the potential to analyze the dataset more comprehensively.\nEarly Design and Idea Phase The original idea was to scrape raffle results, store them in a database, and give people the option to access the data directly from the database or just view it through a good-looking Power BI dashboard.\nThis led me to start writing the code for scraping the website and parsing the tables. However, since there were approximately 50 published results not available on the website, I had to resort to checking the Diário Oficial do Estado de Goiás, where the official government documents of Goiás are published in Brazil.\nChallenges Encountered Not all results were completely published on the Diário Oficial, leading to some results being unavailable.\nThe project doesn\u0026rsquo;t have a budget, so automation can\u0026rsquo;t require VM and online databases. Which Power BI requires both.\nSolutions The results that I could find published on the Diário Oficial were saved in the database along with results available on the website.\nInitially, I was using free options of online databases, and the backup of the old data was on an SQLite available on the project repository. However, I realized that when running the scraper I could save the data on the SQLite and commit the changes, when I did the information persisted on the SQLite file and I could just read it.\nTo help automate the dashboard I resorted to using Streamlit instead of Power BI. Because Streamlit allows me to read the SQLite file directly from the GitHub repository and every time someone opens the Streamlit dashboard it reads the current data, so I don\u0026rsquo;t need to set up refreshes.\nCurrent State of the Project Currently, the project extracts monthly the raffle results from the Nota Fiscal Goiana and the state ICMS collection and saves the results in the SQLite file, using GitHub Actions. The Streamlit dashboard reads the SQLite file and displays the information on the Streamlit Community Cloud.\nLessons Learned While the Power BI dashboard was useful, I faced challenges with refreshing it easily. Initially using an SQLite database, I couldn\u0026rsquo;t connect to the file on the repository, necessitating manual updates on my local machine. To overcome this, I explored alternatives like MySQL and PostgreSQL, enabling automated monthly scraping with GitHub Actions and updating the dashboard through the Power BI website.\nThe SQLite database is really helpful and depending on the size and scope of your project, you should look at it as a possible option to store your data.\nGoing forward I intend to maintain the project, making future patches to the scraping script if the website changes. And improving the code as my knowledge matures.\nExplore Further GitHub Repository: Access the project\u0026rsquo;s source files and codes on our GitHub Repository. Currently, everything is in Portuguese, but I plan to translate it in the future.\nStreamlit Dashboard: Explore the live Streamlit dashboard here to interact with the project\u0026rsquo;s data visualization directly.\nAcademic Article: If you\u0026rsquo;re interested in a detailed academic analysis of this project, check out our article here. Please note that the article is in Portuguese.\n","permalink":"http://localhost:1313/blogs/bts-nota-fiscal-goiana/","summary":"A behind-the-scenes decision on the decision-making process of the project Nota Fiscal Goiana.","title":"[BTS] Nota Fiscal Goiana"},{"content":"Have you ever had a purchase blocked by your credit card issuer without you taking any action? If this happened to you, your credit card issuer probably had detected that your purchase didn\u0026rsquo;t match your normal spending habit, maybe you were at a different location or spend a lot of money suddenly. Anyway, the credit card company was using anomaly detection techniques to prevent fraudulent activities.\nWhat is Anomaly Detection? Anomaly detection refers to the problem of finding patterns in data that do not conform to expected behavior. 1\nIn statistics there is a famous distribution that is called \u0026ldquo;The Normal Distribution\u0026rdquo;, this name is not in vain. A lot of random things are normally distributed, for instance, if you select 100 random people and measure their height, and made a distribution plot, you\u0026rsquo;d end up with something looking like a symmetrical bell curve.\nEven though abnormal results are possible, it is expected that most people or things will fall within a normal distribution. When something is divergent from this normality it is said to be an outlier or an anomaly.\nWhen dealing with something like credit card fraud you can create an expectation of a person expanding habits, so if your daily routine consists of waking up and driving to your workplace and buying a coffee, then lunch in New York. Your credit card company would find it weird if you suddenly made a physical purchase in Rio de Janeiro.\nIn simple terms, that is what anomaly detection is trying to do, understand how something works so it can identify if it starts acting weird.\nAnomaly Detection x Class Imbalance in Datasets If you are aware of what class imbalance is, you may be wondering \u0026ldquo;can\u0026rsquo;t I use anomaly detection to solve this class imbalance problem?\u0026rdquo; - sometimes you can, but it\u0026rsquo;s not appropriate.\nClass imbalance occurs when one class has significantly more samples than the other class(es). This can lead to biased models that favor the majority class and perform poorly on minority classes. While anomaly detection focuses on identifying rare or abnormal data points that deviate significantly from the normal patterns in the dataset.\nEven though it has some overlap on what is trying to do, unless you have a class imbalance problem that the minority class can be considered an abnormal behavior, you won\u0026rsquo;t be able to use anomaly detection.\nI talked about class imbalance before on a churn rate problem in a fictitious telecommunication company, if you are interested you can check it here.\nChandola, V., Banerjee, A., and Kumar, V. 2009. Anomaly detection: A survey. ACM Comput. Surv. 41, 3, Article 15 (July 2009), 58 pages. DOI = 10.1145/1541880.1541882\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/blogs/anomaly-detection/","summary":"How can your credit card issuer know if a purchase is made by you or a cloned version of your credit card?","title":"What is Anomaly Detection?"},{"content":"Demo Demo ","permalink":"http://localhost:1313/blogs/cara-menggunakan-tensorflow/","summary":"Cara Menggunakan Tensorflow","title":"Cara Menggunakan Tensorflow"},{"content":"Panduan Preprocessing Data yang Baik dalam Machine Learning Preprocessing adalah langkah penting dalam alur kerja data science dan machine learning. Data yang baik adalah kunci untuk model yang akurat dan dapat diandalkan. Artikel ini akan membahas pentingnya preprocessing data dan beberapa teknik yang umum digunakan untuk mempersiapkan data sebelum digunakan dalam pelatihan model machine learning.\nApa itu Preprocessing Data? Preprocessing data adalah serangkaian langkah yang dilakukan untuk membersihkan, memformat, dan mengubah data mentah agar siap digunakan dalam analisis atau pembuatan model machine learning. Langkah-langkah ini bertujuan untuk meningkatkan kualitas data dan memastikan model yang dibangun dapat memberikan hasil yang optimal.\nMengapa Preprocessing Data Itu Penting? Data yang tidak diproses dengan baik dapat mengarah pada model yang buruk atau bahkan tidak dapat digunakan sama sekali. Berikut beberapa alasan mengapa preprocessing sangat penting:\nMembersihkan Data: Menghapus noise, menangani nilai yang hilang, atau memperbaiki kesalahan data. Meningkatkan Akurasi Model: Dengan data yang bersih dan relevan, model machine learning dapat belajar dengan lebih baik. Menghindari Bias: Preprocessing membantu menghindari bias dalam data yang dapat memengaruhi prediksi model. Meningkatkan Kecepatan Pelatihan: Beberapa teknik preprocessing dapat mempercepat waktu pelatihan model. Langkah-langkah Preprocessing yang Umum 1. Menangani Nilai yang Hilang Data yang hilang adalah masalah umum dalam dataset dunia nyata. Ada beberapa cara untuk menangani nilai yang hilang:\nMenghapus Baris atau Kolom: Jika proporsi data yang hilang terlalu besar, bisa saja lebih baik untuk menghapusnya. Imputasi: Mengisi nilai yang hilang dengan statistik seperti rata-rata, median, atau modus. Interpolasi: Menggunakan nilai terdekat untuk mengisi data yang hilang, sering digunakan dalam data time series. 1 2 3 4 import pandas as pd # Imputasi menggunakan rata-rata df.fillna(df.mean(), inplace=True) 2. Normalisasi dan Standarisasi Proses ini penting ketika fitur dalam dataset memiliki skala yang sangat berbeda. Model machine learning seperti K-Nearest Neighbors (KNN) atau SVM sangat dipengaruhi oleh skala fitur.\nNormalisasi: Menyelaraskan data ke dalam rentang [0, 1] atau [-1, 1]. Standarisasi: Mengubah data sehingga memiliki rata-rata 0 dan deviasi standar 1. 1 2 3 4 5 6 7 8 9 from sklearn.preprocessing import StandardScaler, MinMaxScaler # Standarisasi scaler = StandardScaler() df_scaled = scaler.fit_transform(df) # Normalisasi min_max_scaler = MinMaxScaler() df_normalized = min_max_scaler.fit_transform(df) 3. Pengkodean Kategorikal Data kategorikal harus dikonversi ke format numerik agar dapat digunakan dalam model machine learning. Ada beberapa cara untuk melakukan ini:\nLabel Encoding: Mengonversi kategori menjadi angka. One-Hot Encoding: Membuat kolom biner untuk setiap kategori. 1 2 3 4 5 6 7 8 from sklearn.preprocessing import LabelEncoder, OneHotEncoder # Label Encoding encoder = LabelEncoder() df[\u0026#39;encoded_column\u0026#39;] = encoder.fit_transform(df[\u0026#39;category_column\u0026#39;]) # One-Hot Encoding df_encoded = pd.get_dummies(df[\u0026#39;category_column\u0026#39;]) 4. Deteksi dan Penanganan Outlier Outlier adalah nilai ekstrem yang bisa mempengaruhi model dan analisis. Ada beberapa cara untuk menangani outlier:\nMenghapus Outlier: Menghapus nilai yang terlalu jauh dari rata-rata. Transformasi: Menggunakan transformasi matematis seperti log atau kuadrat untuk meredam efek outlier. 1 2 3 4 5 6 7 # Deteksi outlier menggunakan IQR Q1 = df[\u0026#39;column\u0026#39;].quantile(0.25) Q3 = df[\u0026#39;column\u0026#39;].quantile(0.75) IQR = Q3 - Q1 # Menghapus outlier df_no_outliers = df[(df[\u0026#39;column\u0026#39;] \u0026gt;= (Q1 - 1.5 * IQR)) \u0026amp; (df[\u0026#39;column\u0026#39;] \u0026lt;= (Q3 + 1.5 * IQR))] 5. Pembagian Data untuk Pelatihan dan Pengujian Sebelum melatih model, penting untuk membagi data menjadi dua set: data pelatihan dan data pengujian. Pembagian yang umum adalah 80% untuk pelatihan dan 20% untuk pengujian.\n1 2 3 4 from sklearn.model_selection import train_test_split # Membagi data X_train, X_test, y_train, y_test = train_test_split(df.drop(\u0026#39;target\u0026#39;, axis=1), df[\u0026#39;target\u0026#39;], test_size=0.2, random_state=42) Kesimpulan Preprocessing data adalah langkah krusial dalam pipeline machine learning. Dengan menangani nilai yang hilang, normalisasi, pengkodean kategorikal, dan deteksi outlier, kita dapat memastikan bahwa model machine learning kita dilatih dengan data yang berkualitas tinggi. Meskipun preprocessing bisa memakan waktu, langkah ini sangat berpengaruh terhadap kinerja dan akurasi model yang dihasilkan. Pastikan untuk selalu memeriksa dataset Anda dan menerapkan teknik preprocessing yang tepat untuk mencapai hasil yang terbaik.\n","permalink":"http://localhost:1313/blogs/cara-preprocessing-data-dengan-baik/","summary":"Cara Preprocessing Data dengan Baik","title":"Cara Preprocessing Data dengan Baik"},{"content":"Demo Demo ","permalink":"http://localhost:1313/blogs/cara-menggunakan-numpy/","summary":"Exploratory Data Analysis of Amazon\u0026rsquo;s top 50 bestselling books 2009 - 2019","title":"Cara menggunakan numpy"},{"content":"Introduction The Nota Fiscal Goiana is an economic program implemented by the state of Goiás, Brazil. This program is designed to boost the collection of the Tax on Circulation of Goods and Services (ICMS) by encouraging people to ask for receipts when they buy goods and services within the state. Additionally, the program includes monthly raffles that distribute cash prizes to participating citizens.\nAs I regularly checked the Nota Fiscal Goiana portal and realized the website\u0026rsquo;s simplicity, I decided to write a scraper to collect the raffle results. Initially, I aimed to simplify the process of checking if I had won, but as I delved deeper, I realized the potential to analyze the dataset more comprehensively.\nEarly Design and Idea Phase The original idea was to scrape raffle results, store them in a database, and give people the option to access the data directly from the database or just view it through a good-looking Power BI dashboard.\nThis led me to start writing the code for scraping the website and parsing the tables. However, since there were approximately 50 published results not available on the website, I had to resort to checking the Diário Oficial do Estado de Goiás, where the official government documents of Goiás are published in Brazil.\nChallenges Encountered Not all results were completely published on the Diário Oficial, leading to some results being unavailable.\nThe project doesn\u0026rsquo;t have a budget, so automation can\u0026rsquo;t require VM and online databases. Which Power BI requires both.\nSolutions The results that I could find published on the Diário Oficial were saved in the database along with results available on the website.\nInitially, I was using free options of online databases, and the backup of the old data was on an SQLite available on the project repository. However, I realized that when running the scraper I could save the data on the SQLite and commit the changes, when I did the information persisted on the SQLite file and I could just read it.\nTo help automate the dashboard I resorted to using Streamlit instead of Power BI. Because Streamlit allows me to read the SQLite file directly from the GitHub repository and every time someone opens the Streamlit dashboard it reads the current data, so I don\u0026rsquo;t need to set up refreshes.\nCurrent State of the Project Currently, the project extracts monthly the raffle results from the Nota Fiscal Goiana and the state ICMS collection and saves the results in the SQLite file, using GitHub Actions. The Streamlit dashboard reads the SQLite file and displays the information on the Streamlit Community Cloud.\nLessons Learned While the Power BI dashboard was useful, I faced challenges with refreshing it easily. Initially using an SQLite database, I couldn\u0026rsquo;t connect to the file on the repository, necessitating manual updates on my local machine. To overcome this, I explored alternatives like MySQL and PostgreSQL, enabling automated monthly scraping with GitHub Actions and updating the dashboard through the Power BI website.\nThe SQLite database is really helpful and depending on the size and scope of your project, you should look at it as a possible option to store your data.\nGoing forward I intend to maintain the project, making future patches to the scraping script if the website changes. And improving the code as my knowledge matures.\nExplore Further GitHub Repository: Access the project\u0026rsquo;s source files and codes on our GitHub Repository. Currently, everything is in Portuguese, but I plan to translate it in the future.\nStreamlit Dashboard: Explore the live Streamlit dashboard here to interact with the project\u0026rsquo;s data visualization directly.\nAcademic Article: If you\u0026rsquo;re interested in a detailed academic analysis of this project, check out our article here. Please note that the article is in Portuguese.\n","permalink":"http://localhost:1313/blogs/bts-nota-fiscal-goiana/","summary":"A behind-the-scenes decision on the decision-making process of the project Nota Fiscal Goiana.","title":"[BTS] Nota Fiscal Goiana"},{"content":"Have you ever had a purchase blocked by your credit card issuer without you taking any action? If this happened to you, your credit card issuer probably had detected that your purchase didn\u0026rsquo;t match your normal spending habit, maybe you were at a different location or spend a lot of money suddenly. Anyway, the credit card company was using anomaly detection techniques to prevent fraudulent activities.\nWhat is Anomaly Detection? Anomaly detection refers to the problem of finding patterns in data that do not conform to expected behavior. 1\nIn statistics there is a famous distribution that is called \u0026ldquo;The Normal Distribution\u0026rdquo;, this name is not in vain. A lot of random things are normally distributed, for instance, if you select 100 random people and measure their height, and made a distribution plot, you\u0026rsquo;d end up with something looking like a symmetrical bell curve.\nEven though abnormal results are possible, it is expected that most people or things will fall within a normal distribution. When something is divergent from this normality it is said to be an outlier or an anomaly.\nWhen dealing with something like credit card fraud you can create an expectation of a person expanding habits, so if your daily routine consists of waking up and driving to your workplace and buying a coffee, then lunch in New York. Your credit card company would find it weird if you suddenly made a physical purchase in Rio de Janeiro.\nIn simple terms, that is what anomaly detection is trying to do, understand how something works so it can identify if it starts acting weird.\nAnomaly Detection x Class Imbalance in Datasets If you are aware of what class imbalance is, you may be wondering \u0026ldquo;can\u0026rsquo;t I use anomaly detection to solve this class imbalance problem?\u0026rdquo; - sometimes you can, but it\u0026rsquo;s not appropriate.\nClass imbalance occurs when one class has significantly more samples than the other class(es). This can lead to biased models that favor the majority class and perform poorly on minority classes. While anomaly detection focuses on identifying rare or abnormal data points that deviate significantly from the normal patterns in the dataset.\nEven though it has some overlap on what is trying to do, unless you have a class imbalance problem that the minority class can be considered an abnormal behavior, you won\u0026rsquo;t be able to use anomaly detection.\nI talked about class imbalance before on a churn rate problem in a fictitious telecommunication company, if you are interested you can check it here.\nChandola, V., Banerjee, A., and Kumar, V. 2009. Anomaly detection: A survey. ACM Comput. Surv. 41, 3, Article 15 (July 2009), 58 pages. DOI = 10.1145/1541880.1541882\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/blogs/anomaly-detection/","summary":"How can your credit card issuer know if a purchase is made by you or a cloned version of your credit card?","title":"What is Anomaly Detection?"},{"content":"Demo Demo ","permalink":"http://localhost:1313/blogs/cara-menggunakan-tensorflow/","summary":"Cara Menggunakan Tensorflow","title":"Cara Menggunakan Tensorflow"},{"content":"Panduan Preprocessing Data yang Baik dalam Machine Learning Preprocessing adalah langkah penting dalam alur kerja data science dan machine learning. Data yang baik adalah kunci untuk model yang akurat dan dapat diandalkan. Artikel ini akan membahas pentingnya preprocessing data dan beberapa teknik yang umum digunakan untuk mempersiapkan data sebelum digunakan dalam pelatihan model machine learning.\nApa itu Preprocessing Data? Preprocessing data adalah serangkaian langkah yang dilakukan untuk membersihkan, memformat, dan mengubah data mentah agar siap digunakan dalam analisis atau pembuatan model machine learning. Langkah-langkah ini bertujuan untuk meningkatkan kualitas data dan memastikan model yang dibangun dapat memberikan hasil yang optimal.\nMengapa Preprocessing Data Itu Penting? Data yang tidak diproses dengan baik dapat mengarah pada model yang buruk atau bahkan tidak dapat digunakan sama sekali. Berikut beberapa alasan mengapa preprocessing sangat penting:\nMembersihkan Data: Menghapus noise, menangani nilai yang hilang, atau memperbaiki kesalahan data. Meningkatkan Akurasi Model: Dengan data yang bersih dan relevan, model machine learning dapat belajar dengan lebih baik. Menghindari Bias: Preprocessing membantu menghindari bias dalam data yang dapat memengaruhi prediksi model. Meningkatkan Kecepatan Pelatihan: Beberapa teknik preprocessing dapat mempercepat waktu pelatihan model. Langkah-langkah Preprocessing yang Umum 1. Menangani Nilai yang Hilang Data yang hilang adalah masalah umum dalam dataset dunia nyata. Ada beberapa cara untuk menangani nilai yang hilang:\nMenghapus Baris atau Kolom: Jika proporsi data yang hilang terlalu besar, bisa saja lebih baik untuk menghapusnya. Imputasi: Mengisi nilai yang hilang dengan statistik seperti rata-rata, median, atau modus. Interpolasi: Menggunakan nilai terdekat untuk mengisi data yang hilang, sering digunakan dalam data time series. 1 2 3 4 import pandas as pd # Imputasi menggunakan rata-rata df.fillna(df.mean(), inplace=True) 2. Normalisasi dan Standarisasi Proses ini penting ketika fitur dalam dataset memiliki skala yang sangat berbeda. Model machine learning seperti K-Nearest Neighbors (KNN) atau SVM sangat dipengaruhi oleh skala fitur.\nNormalisasi: Menyelaraskan data ke dalam rentang [0, 1] atau [-1, 1]. Standarisasi: Mengubah data sehingga memiliki rata-rata 0 dan deviasi standar 1. 1 2 3 4 5 6 7 8 9 from sklearn.preprocessing import StandardScaler, MinMaxScaler # Standarisasi scaler = StandardScaler() df_scaled = scaler.fit_transform(df) # Normalisasi min_max_scaler = MinMaxScaler() df_normalized = min_max_scaler.fit_transform(df) 3. Pengkodean Kategorikal Data kategorikal harus dikonversi ke format numerik agar dapat digunakan dalam model machine learning. Ada beberapa cara untuk melakukan ini:\nLabel Encoding: Mengonversi kategori menjadi angka. One-Hot Encoding: Membuat kolom biner untuk setiap kategori. 1 2 3 4 5 6 7 8 from sklearn.preprocessing import LabelEncoder, OneHotEncoder # Label Encoding encoder = LabelEncoder() df[\u0026#39;encoded_column\u0026#39;] = encoder.fit_transform(df[\u0026#39;category_column\u0026#39;]) # One-Hot Encoding df_encoded = pd.get_dummies(df[\u0026#39;category_column\u0026#39;]) 4. Deteksi dan Penanganan Outlier Outlier adalah nilai ekstrem yang bisa mempengaruhi model dan analisis. Ada beberapa cara untuk menangani outlier:\nMenghapus Outlier: Menghapus nilai yang terlalu jauh dari rata-rata. Transformasi: Menggunakan transformasi matematis seperti log atau kuadrat untuk meredam efek outlier. 1 2 3 4 5 6 7 # Deteksi outlier menggunakan IQR Q1 = df[\u0026#39;column\u0026#39;].quantile(0.25) Q3 = df[\u0026#39;column\u0026#39;].quantile(0.75) IQR = Q3 - Q1 # Menghapus outlier df_no_outliers = df[(df[\u0026#39;column\u0026#39;] \u0026gt;= (Q1 - 1.5 * IQR)) \u0026amp; (df[\u0026#39;column\u0026#39;] \u0026lt;= (Q3 + 1.5 * IQR))] 5. Pembagian Data untuk Pelatihan dan Pengujian Sebelum melatih model, penting untuk membagi data menjadi dua set: data pelatihan dan data pengujian. Pembagian yang umum adalah 80% untuk pelatihan dan 20% untuk pengujian.\n1 2 3 4 from sklearn.model_selection import train_test_split # Membagi data X_train, X_test, y_train, y_test = train_test_split(df.drop(\u0026#39;target\u0026#39;, axis=1), df[\u0026#39;target\u0026#39;], test_size=0.2, random_state=42) Kesimpulan Preprocessing data adalah langkah krusial dalam pipeline machine learning. Dengan menangani nilai yang hilang, normalisasi, pengkodean kategorikal, dan deteksi outlier, kita dapat memastikan bahwa model machine learning kita dilatih dengan data yang berkualitas tinggi. Meskipun preprocessing bisa memakan waktu, langkah ini sangat berpengaruh terhadap kinerja dan akurasi model yang dihasilkan. Pastikan untuk selalu memeriksa dataset Anda dan menerapkan teknik preprocessing yang tepat untuk mencapai hasil yang terbaik.\n","permalink":"http://localhost:1313/blogs/cara-preprocessing-data-dengan-baik/","summary":"Cara Preprocessing Data dengan Baik","title":"Cara Preprocessing Data dengan Baik"},{"content":"Demo Demo ","permalink":"http://localhost:1313/blogs/cara-menggunakan-numpy/","summary":"Exploratory Data Analysis of Amazon\u0026rsquo;s top 50 bestselling books 2009 - 2019","title":"Cara menggunakan numpy"},{"content":"Introduction The Nota Fiscal Goiana is an economic program implemented by the state of Goiás, Brazil. This program is designed to boost the collection of the Tax on Circulation of Goods and Services (ICMS) by encouraging people to ask for receipts when they buy goods and services within the state. Additionally, the program includes monthly raffles that distribute cash prizes to participating citizens.\nAs I regularly checked the Nota Fiscal Goiana portal and realized the website\u0026rsquo;s simplicity, I decided to write a scraper to collect the raffle results. Initially, I aimed to simplify the process of checking if I had won, but as I delved deeper, I realized the potential to analyze the dataset more comprehensively.\nEarly Design and Idea Phase The original idea was to scrape raffle results, store them in a database, and give people the option to access the data directly from the database or just view it through a good-looking Power BI dashboard.\nThis led me to start writing the code for scraping the website and parsing the tables. However, since there were approximately 50 published results not available on the website, I had to resort to checking the Diário Oficial do Estado de Goiás, where the official government documents of Goiás are published in Brazil.\nChallenges Encountered Not all results were completely published on the Diário Oficial, leading to some results being unavailable.\nThe project doesn\u0026rsquo;t have a budget, so automation can\u0026rsquo;t require VM and online databases. Which Power BI requires both.\nSolutions The results that I could find published on the Diário Oficial were saved in the database along with results available on the website.\nInitially, I was using free options of online databases, and the backup of the old data was on an SQLite available on the project repository. However, I realized that when running the scraper I could save the data on the SQLite and commit the changes, when I did the information persisted on the SQLite file and I could just read it.\nTo help automate the dashboard I resorted to using Streamlit instead of Power BI. Because Streamlit allows me to read the SQLite file directly from the GitHub repository and every time someone opens the Streamlit dashboard it reads the current data, so I don\u0026rsquo;t need to set up refreshes.\nCurrent State of the Project Currently, the project extracts monthly the raffle results from the Nota Fiscal Goiana and the state ICMS collection and saves the results in the SQLite file, using GitHub Actions. The Streamlit dashboard reads the SQLite file and displays the information on the Streamlit Community Cloud.\nLessons Learned While the Power BI dashboard was useful, I faced challenges with refreshing it easily. Initially using an SQLite database, I couldn\u0026rsquo;t connect to the file on the repository, necessitating manual updates on my local machine. To overcome this, I explored alternatives like MySQL and PostgreSQL, enabling automated monthly scraping with GitHub Actions and updating the dashboard through the Power BI website.\nThe SQLite database is really helpful and depending on the size and scope of your project, you should look at it as a possible option to store your data.\nGoing forward I intend to maintain the project, making future patches to the scraping script if the website changes. And improving the code as my knowledge matures.\nExplore Further GitHub Repository: Access the project\u0026rsquo;s source files and codes on our GitHub Repository. Currently, everything is in Portuguese, but I plan to translate it in the future.\nStreamlit Dashboard: Explore the live Streamlit dashboard here to interact with the project\u0026rsquo;s data visualization directly.\nAcademic Article: If you\u0026rsquo;re interested in a detailed academic analysis of this project, check out our article here. Please note that the article is in Portuguese.\n","permalink":"http://localhost:1313/blogs/bts-nota-fiscal-goiana/","summary":"A behind-the-scenes decision on the decision-making process of the project Nota Fiscal Goiana.","title":"[BTS] Nota Fiscal Goiana"},{"content":"Have you ever had a purchase blocked by your credit card issuer without you taking any action? If this happened to you, your credit card issuer probably had detected that your purchase didn\u0026rsquo;t match your normal spending habit, maybe you were at a different location or spend a lot of money suddenly. Anyway, the credit card company was using anomaly detection techniques to prevent fraudulent activities.\nWhat is Anomaly Detection? Anomaly detection refers to the problem of finding patterns in data that do not conform to expected behavior. 1\nIn statistics there is a famous distribution that is called \u0026ldquo;The Normal Distribution\u0026rdquo;, this name is not in vain. A lot of random things are normally distributed, for instance, if you select 100 random people and measure their height, and made a distribution plot, you\u0026rsquo;d end up with something looking like a symmetrical bell curve.\nEven though abnormal results are possible, it is expected that most people or things will fall within a normal distribution. When something is divergent from this normality it is said to be an outlier or an anomaly.\nWhen dealing with something like credit card fraud you can create an expectation of a person expanding habits, so if your daily routine consists of waking up and driving to your workplace and buying a coffee, then lunch in New York. Your credit card company would find it weird if you suddenly made a physical purchase in Rio de Janeiro.\nIn simple terms, that is what anomaly detection is trying to do, understand how something works so it can identify if it starts acting weird.\nAnomaly Detection x Class Imbalance in Datasets If you are aware of what class imbalance is, you may be wondering \u0026ldquo;can\u0026rsquo;t I use anomaly detection to solve this class imbalance problem?\u0026rdquo; - sometimes you can, but it\u0026rsquo;s not appropriate.\nClass imbalance occurs when one class has significantly more samples than the other class(es). This can lead to biased models that favor the majority class and perform poorly on minority classes. While anomaly detection focuses on identifying rare or abnormal data points that deviate significantly from the normal patterns in the dataset.\nEven though it has some overlap on what is trying to do, unless you have a class imbalance problem that the minority class can be considered an abnormal behavior, you won\u0026rsquo;t be able to use anomaly detection.\nI talked about class imbalance before on a churn rate problem in a fictitious telecommunication company, if you are interested you can check it here.\nChandola, V., Banerjee, A., and Kumar, V. 2009. Anomaly detection: A survey. ACM Comput. Surv. 41, 3, Article 15 (July 2009), 58 pages. DOI = 10.1145/1541880.1541882\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/blogs/anomaly-detection/","summary":"How can your credit card issuer know if a purchase is made by you or a cloned version of your credit card?","title":"What is Anomaly Detection?"},{"content":"Demo Demo ","permalink":"http://localhost:1313/blogs/cara-menggunakan-tensorflow/","summary":"Cara Menggunakan Tensorflow","title":"Cara Menggunakan Tensorflow"},{"content":"Panduan Preprocessing Data yang Baik dalam Machine Learning Preprocessing adalah langkah penting dalam alur kerja data science dan machine learning. Data yang baik adalah kunci untuk model yang akurat dan dapat diandalkan. Artikel ini akan membahas pentingnya preprocessing data dan beberapa teknik yang umum digunakan untuk mempersiapkan data sebelum digunakan dalam pelatihan model machine learning.\nApa itu Preprocessing Data? Preprocessing data adalah serangkaian langkah yang dilakukan untuk membersihkan, memformat, dan mengubah data mentah agar siap digunakan dalam analisis atau pembuatan model machine learning. Langkah-langkah ini bertujuan untuk meningkatkan kualitas data dan memastikan model yang dibangun dapat memberikan hasil yang optimal.\nMengapa Preprocessing Data Itu Penting? Data yang tidak diproses dengan baik dapat mengarah pada model yang buruk atau bahkan tidak dapat digunakan sama sekali. Berikut beberapa alasan mengapa preprocessing sangat penting:\nMembersihkan Data: Menghapus noise, menangani nilai yang hilang, atau memperbaiki kesalahan data. Meningkatkan Akurasi Model: Dengan data yang bersih dan relevan, model machine learning dapat belajar dengan lebih baik. Menghindari Bias: Preprocessing membantu menghindari bias dalam data yang dapat memengaruhi prediksi model. Meningkatkan Kecepatan Pelatihan: Beberapa teknik preprocessing dapat mempercepat waktu pelatihan model. Langkah-langkah Preprocessing yang Umum 1. Menangani Nilai yang Hilang Data yang hilang adalah masalah umum dalam dataset dunia nyata. Ada beberapa cara untuk menangani nilai yang hilang:\nMenghapus Baris atau Kolom: Jika proporsi data yang hilang terlalu besar, bisa saja lebih baik untuk menghapusnya. Imputasi: Mengisi nilai yang hilang dengan statistik seperti rata-rata, median, atau modus. Interpolasi: Menggunakan nilai terdekat untuk mengisi data yang hilang, sering digunakan dalam data time series. 1 2 3 4 import pandas as pd # Imputasi menggunakan rata-rata df.fillna(df.mean(), inplace=True) 2. Normalisasi dan Standarisasi Proses ini penting ketika fitur dalam dataset memiliki skala yang sangat berbeda. Model machine learning seperti K-Nearest Neighbors (KNN) atau SVM sangat dipengaruhi oleh skala fitur.\nNormalisasi: Menyelaraskan data ke dalam rentang [0, 1] atau [-1, 1]. Standarisasi: Mengubah data sehingga memiliki rata-rata 0 dan deviasi standar 1. 1 2 3 4 5 6 7 8 9 from sklearn.preprocessing import StandardScaler, MinMaxScaler # Standarisasi scaler = StandardScaler() df_scaled = scaler.fit_transform(df) # Normalisasi min_max_scaler = MinMaxScaler() df_normalized = min_max_scaler.fit_transform(df) 3. Pengkodean Kategorikal Data kategorikal harus dikonversi ke format numerik agar dapat digunakan dalam model machine learning. Ada beberapa cara untuk melakukan ini:\nLabel Encoding: Mengonversi kategori menjadi angka. One-Hot Encoding: Membuat kolom biner untuk setiap kategori. 1 2 3 4 5 6 7 8 from sklearn.preprocessing import LabelEncoder, OneHotEncoder # Label Encoding encoder = LabelEncoder() df[\u0026#39;encoded_column\u0026#39;] = encoder.fit_transform(df[\u0026#39;category_column\u0026#39;]) # One-Hot Encoding df_encoded = pd.get_dummies(df[\u0026#39;category_column\u0026#39;]) 4. Deteksi dan Penanganan Outlier Outlier adalah nilai ekstrem yang bisa mempengaruhi model dan analisis. Ada beberapa cara untuk menangani outlier:\nMenghapus Outlier: Menghapus nilai yang terlalu jauh dari rata-rata. Transformasi: Menggunakan transformasi matematis seperti log atau kuadrat untuk meredam efek outlier. 1 2 3 4 5 6 7 # Deteksi outlier menggunakan IQR Q1 = df[\u0026#39;column\u0026#39;].quantile(0.25) Q3 = df[\u0026#39;column\u0026#39;].quantile(0.75) IQR = Q3 - Q1 # Menghapus outlier df_no_outliers = df[(df[\u0026#39;column\u0026#39;] \u0026gt;= (Q1 - 1.5 * IQR)) \u0026amp; (df[\u0026#39;column\u0026#39;] \u0026lt;= (Q3 + 1.5 * IQR))] 5. Pembagian Data untuk Pelatihan dan Pengujian Sebelum melatih model, penting untuk membagi data menjadi dua set: data pelatihan dan data pengujian. Pembagian yang umum adalah 80% untuk pelatihan dan 20% untuk pengujian.\n1 2 3 4 from sklearn.model_selection import train_test_split # Membagi data X_train, X_test, y_train, y_test = train_test_split(df.drop(\u0026#39;target\u0026#39;, axis=1), df[\u0026#39;target\u0026#39;], test_size=0.2, random_state=42) Kesimpulan Preprocessing data adalah langkah krusial dalam pipeline machine learning. Dengan menangani nilai yang hilang, normalisasi, pengkodean kategorikal, dan deteksi outlier, kita dapat memastikan bahwa model machine learning kita dilatih dengan data yang berkualitas tinggi. Meskipun preprocessing bisa memakan waktu, langkah ini sangat berpengaruh terhadap kinerja dan akurasi model yang dihasilkan. Pastikan untuk selalu memeriksa dataset Anda dan menerapkan teknik preprocessing yang tepat untuk mencapai hasil yang terbaik.\n","permalink":"http://localhost:1313/blogs/cara-preprocessing-data-dengan-baik/","summary":"Cara Preprocessing Data dengan Baik","title":"Cara Preprocessing Data dengan Baik"},{"content":"Demo Demo ","permalink":"http://localhost:1313/blogs/cara-menggunakan-numpy/","summary":"Exploratory Data Analysis of Amazon\u0026rsquo;s top 50 bestselling books 2009 - 2019","title":"Cara menggunakan numpy"},{"content":"Introduction The Nota Fiscal Goiana is an economic program implemented by the state of Goiás, Brazil. This program is designed to boost the collection of the Tax on Circulation of Goods and Services (ICMS) by encouraging people to ask for receipts when they buy goods and services within the state. Additionally, the program includes monthly raffles that distribute cash prizes to participating citizens.\nAs I regularly checked the Nota Fiscal Goiana portal and realized the website\u0026rsquo;s simplicity, I decided to write a scraper to collect the raffle results. Initially, I aimed to simplify the process of checking if I had won, but as I delved deeper, I realized the potential to analyze the dataset more comprehensively.\nEarly Design and Idea Phase The original idea was to scrape raffle results, store them in a database, and give people the option to access the data directly from the database or just view it through a good-looking Power BI dashboard.\nThis led me to start writing the code for scraping the website and parsing the tables. However, since there were approximately 50 published results not available on the website, I had to resort to checking the Diário Oficial do Estado de Goiás, where the official government documents of Goiás are published in Brazil.\nChallenges Encountered Not all results were completely published on the Diário Oficial, leading to some results being unavailable.\nThe project doesn\u0026rsquo;t have a budget, so automation can\u0026rsquo;t require VM and online databases. Which Power BI requires both.\nSolutions The results that I could find published on the Diário Oficial were saved in the database along with results available on the website.\nInitially, I was using free options of online databases, and the backup of the old data was on an SQLite available on the project repository. However, I realized that when running the scraper I could save the data on the SQLite and commit the changes, when I did the information persisted on the SQLite file and I could just read it.\nTo help automate the dashboard I resorted to using Streamlit instead of Power BI. Because Streamlit allows me to read the SQLite file directly from the GitHub repository and every time someone opens the Streamlit dashboard it reads the current data, so I don\u0026rsquo;t need to set up refreshes.\nCurrent State of the Project Currently, the project extracts monthly the raffle results from the Nota Fiscal Goiana and the state ICMS collection and saves the results in the SQLite file, using GitHub Actions. The Streamlit dashboard reads the SQLite file and displays the information on the Streamlit Community Cloud.\nLessons Learned While the Power BI dashboard was useful, I faced challenges with refreshing it easily. Initially using an SQLite database, I couldn\u0026rsquo;t connect to the file on the repository, necessitating manual updates on my local machine. To overcome this, I explored alternatives like MySQL and PostgreSQL, enabling automated monthly scraping with GitHub Actions and updating the dashboard through the Power BI website.\nThe SQLite database is really helpful and depending on the size and scope of your project, you should look at it as a possible option to store your data.\nGoing forward I intend to maintain the project, making future patches to the scraping script if the website changes. And improving the code as my knowledge matures.\nExplore Further GitHub Repository: Access the project\u0026rsquo;s source files and codes on our GitHub Repository. Currently, everything is in Portuguese, but I plan to translate it in the future.\nStreamlit Dashboard: Explore the live Streamlit dashboard here to interact with the project\u0026rsquo;s data visualization directly.\nAcademic Article: If you\u0026rsquo;re interested in a detailed academic analysis of this project, check out our article here. Please note that the article is in Portuguese.\n","permalink":"http://localhost:1313/blogs/bts-nota-fiscal-goiana/","summary":"A behind-the-scenes decision on the decision-making process of the project Nota Fiscal Goiana.","title":"[BTS] Nota Fiscal Goiana"},{"content":"Have you ever had a purchase blocked by your credit card issuer without you taking any action? If this happened to you, your credit card issuer probably had detected that your purchase didn\u0026rsquo;t match your normal spending habit, maybe you were at a different location or spend a lot of money suddenly. Anyway, the credit card company was using anomaly detection techniques to prevent fraudulent activities.\nWhat is Anomaly Detection? Anomaly detection refers to the problem of finding patterns in data that do not conform to expected behavior. 1\nIn statistics there is a famous distribution that is called \u0026ldquo;The Normal Distribution\u0026rdquo;, this name is not in vain. A lot of random things are normally distributed, for instance, if you select 100 random people and measure their height, and made a distribution plot, you\u0026rsquo;d end up with something looking like a symmetrical bell curve.\nEven though abnormal results are possible, it is expected that most people or things will fall within a normal distribution. When something is divergent from this normality it is said to be an outlier or an anomaly.\nWhen dealing with something like credit card fraud you can create an expectation of a person expanding habits, so if your daily routine consists of waking up and driving to your workplace and buying a coffee, then lunch in New York. Your credit card company would find it weird if you suddenly made a physical purchase in Rio de Janeiro.\nIn simple terms, that is what anomaly detection is trying to do, understand how something works so it can identify if it starts acting weird.\nAnomaly Detection x Class Imbalance in Datasets If you are aware of what class imbalance is, you may be wondering \u0026ldquo;can\u0026rsquo;t I use anomaly detection to solve this class imbalance problem?\u0026rdquo; - sometimes you can, but it\u0026rsquo;s not appropriate.\nClass imbalance occurs when one class has significantly more samples than the other class(es). This can lead to biased models that favor the majority class and perform poorly on minority classes. While anomaly detection focuses on identifying rare or abnormal data points that deviate significantly from the normal patterns in the dataset.\nEven though it has some overlap on what is trying to do, unless you have a class imbalance problem that the minority class can be considered an abnormal behavior, you won\u0026rsquo;t be able to use anomaly detection.\nI talked about class imbalance before on a churn rate problem in a fictitious telecommunication company, if you are interested you can check it here.\nChandola, V., Banerjee, A., and Kumar, V. 2009. Anomaly detection: A survey. ACM Comput. Surv. 41, 3, Article 15 (July 2009), 58 pages. DOI = 10.1145/1541880.1541882\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/blogs/anomaly-detection/","summary":"How can your credit card issuer know if a purchase is made by you or a cloned version of your credit card?","title":"What is Anomaly Detection?"},{"content":"Demo Demo ","permalink":"http://localhost:1313/blogs/cara-menggunakan-tensorflow/","summary":"Cara Menggunakan Tensorflow","title":"Cara Menggunakan Tensorflow"},{"content":"Panduan Preprocessing Data yang Baik dalam Machine Learning Preprocessing adalah langkah penting dalam alur kerja data science dan machine learning. Data yang baik adalah kunci untuk model yang akurat dan dapat diandalkan. Artikel ini akan membahas pentingnya preprocessing data dan beberapa teknik yang umum digunakan untuk mempersiapkan data sebelum digunakan dalam pelatihan model machine learning.\nApa itu Preprocessing Data? Preprocessing data adalah serangkaian langkah yang dilakukan untuk membersihkan, memformat, dan mengubah data mentah agar siap digunakan dalam analisis atau pembuatan model machine learning. Langkah-langkah ini bertujuan untuk meningkatkan kualitas data dan memastikan model yang dibangun dapat memberikan hasil yang optimal.\nMengapa Preprocessing Data Itu Penting? Data yang tidak diproses dengan baik dapat mengarah pada model yang buruk atau bahkan tidak dapat digunakan sama sekali. Berikut beberapa alasan mengapa preprocessing sangat penting:\nMembersihkan Data: Menghapus noise, menangani nilai yang hilang, atau memperbaiki kesalahan data. Meningkatkan Akurasi Model: Dengan data yang bersih dan relevan, model machine learning dapat belajar dengan lebih baik. Menghindari Bias: Preprocessing membantu menghindari bias dalam data yang dapat memengaruhi prediksi model. Meningkatkan Kecepatan Pelatihan: Beberapa teknik preprocessing dapat mempercepat waktu pelatihan model. Langkah-langkah Preprocessing yang Umum 1. Menangani Nilai yang Hilang Data yang hilang adalah masalah umum dalam dataset dunia nyata. Ada beberapa cara untuk menangani nilai yang hilang:\nMenghapus Baris atau Kolom: Jika proporsi data yang hilang terlalu besar, bisa saja lebih baik untuk menghapusnya. Imputasi: Mengisi nilai yang hilang dengan statistik seperti rata-rata, median, atau modus. Interpolasi: Menggunakan nilai terdekat untuk mengisi data yang hilang, sering digunakan dalam data time series. 1 2 3 4 import pandas as pd # Imputasi menggunakan rata-rata df.fillna(df.mean(), inplace=True) 2. Normalisasi dan Standarisasi Proses ini penting ketika fitur dalam dataset memiliki skala yang sangat berbeda. Model machine learning seperti K-Nearest Neighbors (KNN) atau SVM sangat dipengaruhi oleh skala fitur.\nNormalisasi: Menyelaraskan data ke dalam rentang [0, 1] atau [-1, 1]. Standarisasi: Mengubah data sehingga memiliki rata-rata 0 dan deviasi standar 1. 1 2 3 4 5 6 7 8 9 from sklearn.preprocessing import StandardScaler, MinMaxScaler # Standarisasi scaler = StandardScaler() df_scaled = scaler.fit_transform(df) # Normalisasi min_max_scaler = MinMaxScaler() df_normalized = min_max_scaler.fit_transform(df) 3. Pengkodean Kategorikal Data kategorikal harus dikonversi ke format numerik agar dapat digunakan dalam model machine learning. Ada beberapa cara untuk melakukan ini:\nLabel Encoding: Mengonversi kategori menjadi angka. One-Hot Encoding: Membuat kolom biner untuk setiap kategori. 1 2 3 4 5 6 7 8 from sklearn.preprocessing import LabelEncoder, OneHotEncoder # Label Encoding encoder = LabelEncoder() df[\u0026#39;encoded_column\u0026#39;] = encoder.fit_transform(df[\u0026#39;category_column\u0026#39;]) # One-Hot Encoding df_encoded = pd.get_dummies(df[\u0026#39;category_column\u0026#39;]) 4. Deteksi dan Penanganan Outlier Outlier adalah nilai ekstrem yang bisa mempengaruhi model dan analisis. Ada beberapa cara untuk menangani outlier:\nMenghapus Outlier: Menghapus nilai yang terlalu jauh dari rata-rata. Transformasi: Menggunakan transformasi matematis seperti log atau kuadrat untuk meredam efek outlier. 1 2 3 4 5 6 7 # Deteksi outlier menggunakan IQR Q1 = df[\u0026#39;column\u0026#39;].quantile(0.25) Q3 = df[\u0026#39;column\u0026#39;].quantile(0.75) IQR = Q3 - Q1 # Menghapus outlier df_no_outliers = df[(df[\u0026#39;column\u0026#39;] \u0026gt;= (Q1 - 1.5 * IQR)) \u0026amp; (df[\u0026#39;column\u0026#39;] \u0026lt;= (Q3 + 1.5 * IQR))] 5. Pembagian Data untuk Pelatihan dan Pengujian Sebelum melatih model, penting untuk membagi data menjadi dua set: data pelatihan dan data pengujian. Pembagian yang umum adalah 80% untuk pelatihan dan 20% untuk pengujian.\n1 2 3 4 from sklearn.model_selection import train_test_split # Membagi data X_train, X_test, y_train, y_test = train_test_split(df.drop(\u0026#39;target\u0026#39;, axis=1), df[\u0026#39;target\u0026#39;], test_size=0.2, random_state=42) Kesimpulan Preprocessing data adalah langkah krusial dalam pipeline machine learning. Dengan menangani nilai yang hilang, normalisasi, pengkodean kategorikal, dan deteksi outlier, kita dapat memastikan bahwa model machine learning kita dilatih dengan data yang berkualitas tinggi. Meskipun preprocessing bisa memakan waktu, langkah ini sangat berpengaruh terhadap kinerja dan akurasi model yang dihasilkan. Pastikan untuk selalu memeriksa dataset Anda dan menerapkan teknik preprocessing yang tepat untuk mencapai hasil yang terbaik.\n","permalink":"http://localhost:1313/blogs/cara-preprocessing-data-dengan-baik/","summary":"Cara Preprocessing Data dengan Baik","title":"Cara Preprocessing Data dengan Baik"},{"content":"Demo Demo ","permalink":"http://localhost:1313/blogs/cara-menggunakan-numpy/","summary":"Exploratory Data Analysis of Amazon\u0026rsquo;s top 50 bestselling books 2009 - 2019","title":"Cara menggunakan numpy"},{"content":"Introduction The Nota Fiscal Goiana is an economic program implemented by the state of Goiás, Brazil. This program is designed to boost the collection of the Tax on Circulation of Goods and Services (ICMS) by encouraging people to ask for receipts when they buy goods and services within the state. Additionally, the program includes monthly raffles that distribute cash prizes to participating citizens.\nAs I regularly checked the Nota Fiscal Goiana portal and realized the website\u0026rsquo;s simplicity, I decided to write a scraper to collect the raffle results. Initially, I aimed to simplify the process of checking if I had won, but as I delved deeper, I realized the potential to analyze the dataset more comprehensively.\nEarly Design and Idea Phase The original idea was to scrape raffle results, store them in a database, and give people the option to access the data directly from the database or just view it through a good-looking Power BI dashboard.\nThis led me to start writing the code for scraping the website and parsing the tables. However, since there were approximately 50 published results not available on the website, I had to resort to checking the Diário Oficial do Estado de Goiás, where the official government documents of Goiás are published in Brazil.\nChallenges Encountered Not all results were completely published on the Diário Oficial, leading to some results being unavailable.\nThe project doesn\u0026rsquo;t have a budget, so automation can\u0026rsquo;t require VM and online databases. Which Power BI requires both.\nSolutions The results that I could find published on the Diário Oficial were saved in the database along with results available on the website.\nInitially, I was using free options of online databases, and the backup of the old data was on an SQLite available on the project repository. However, I realized that when running the scraper I could save the data on the SQLite and commit the changes, when I did the information persisted on the SQLite file and I could just read it.\nTo help automate the dashboard I resorted to using Streamlit instead of Power BI. Because Streamlit allows me to read the SQLite file directly from the GitHub repository and every time someone opens the Streamlit dashboard it reads the current data, so I don\u0026rsquo;t need to set up refreshes.\nCurrent State of the Project Currently, the project extracts monthly the raffle results from the Nota Fiscal Goiana and the state ICMS collection and saves the results in the SQLite file, using GitHub Actions. The Streamlit dashboard reads the SQLite file and displays the information on the Streamlit Community Cloud.\nLessons Learned While the Power BI dashboard was useful, I faced challenges with refreshing it easily. Initially using an SQLite database, I couldn\u0026rsquo;t connect to the file on the repository, necessitating manual updates on my local machine. To overcome this, I explored alternatives like MySQL and PostgreSQL, enabling automated monthly scraping with GitHub Actions and updating the dashboard through the Power BI website.\nThe SQLite database is really helpful and depending on the size and scope of your project, you should look at it as a possible option to store your data.\nGoing forward I intend to maintain the project, making future patches to the scraping script if the website changes. And improving the code as my knowledge matures.\nExplore Further GitHub Repository: Access the project\u0026rsquo;s source files and codes on our GitHub Repository. Currently, everything is in Portuguese, but I plan to translate it in the future.\nStreamlit Dashboard: Explore the live Streamlit dashboard here to interact with the project\u0026rsquo;s data visualization directly.\nAcademic Article: If you\u0026rsquo;re interested in a detailed academic analysis of this project, check out our article here. Please note that the article is in Portuguese.\n","permalink":"http://localhost:1313/blogs/bts-nota-fiscal-goiana/","summary":"A behind-the-scenes decision on the decision-making process of the project Nota Fiscal Goiana.","title":"[BTS] Nota Fiscal Goiana"},{"content":"Have you ever had a purchase blocked by your credit card issuer without you taking any action? If this happened to you, your credit card issuer probably had detected that your purchase didn\u0026rsquo;t match your normal spending habit, maybe you were at a different location or spend a lot of money suddenly. Anyway, the credit card company was using anomaly detection techniques to prevent fraudulent activities.\nWhat is Anomaly Detection? Anomaly detection refers to the problem of finding patterns in data that do not conform to expected behavior. 1\nIn statistics there is a famous distribution that is called \u0026ldquo;The Normal Distribution\u0026rdquo;, this name is not in vain. A lot of random things are normally distributed, for instance, if you select 100 random people and measure their height, and made a distribution plot, you\u0026rsquo;d end up with something looking like a symmetrical bell curve.\nEven though abnormal results are possible, it is expected that most people or things will fall within a normal distribution. When something is divergent from this normality it is said to be an outlier or an anomaly.\nWhen dealing with something like credit card fraud you can create an expectation of a person expanding habits, so if your daily routine consists of waking up and driving to your workplace and buying a coffee, then lunch in New York. Your credit card company would find it weird if you suddenly made a physical purchase in Rio de Janeiro.\nIn simple terms, that is what anomaly detection is trying to do, understand how something works so it can identify if it starts acting weird.\nAnomaly Detection x Class Imbalance in Datasets If you are aware of what class imbalance is, you may be wondering \u0026ldquo;can\u0026rsquo;t I use anomaly detection to solve this class imbalance problem?\u0026rdquo; - sometimes you can, but it\u0026rsquo;s not appropriate.\nClass imbalance occurs when one class has significantly more samples than the other class(es). This can lead to biased models that favor the majority class and perform poorly on minority classes. While anomaly detection focuses on identifying rare or abnormal data points that deviate significantly from the normal patterns in the dataset.\nEven though it has some overlap on what is trying to do, unless you have a class imbalance problem that the minority class can be considered an abnormal behavior, you won\u0026rsquo;t be able to use anomaly detection.\nI talked about class imbalance before on a churn rate problem in a fictitious telecommunication company, if you are interested you can check it here.\nChandola, V., Banerjee, A., and Kumar, V. 2009. Anomaly detection: A survey. ACM Comput. Surv. 41, 3, Article 15 (July 2009), 58 pages. DOI = 10.1145/1541880.1541882\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/blogs/anomaly-detection/","summary":"How can your credit card issuer know if a purchase is made by you or a cloned version of your credit card?","title":"What is Anomaly Detection?"},{"content":"Demo Demo ","permalink":"http://localhost:1313/blogs/cara-menggunakan-tensorflow/","summary":"Cara Menggunakan Tensorflow","title":"Cara Menggunakan Tensorflow"},{"content":"Panduan Preprocessing Data yang Baik dalam Machine Learning Preprocessing adalah langkah penting dalam alur kerja data science dan machine learning. Data yang baik adalah kunci untuk model yang akurat dan dapat diandalkan. Artikel ini akan membahas pentingnya preprocessing data dan beberapa teknik yang umum digunakan untuk mempersiapkan data sebelum digunakan dalam pelatihan model machine learning.\nApa itu Preprocessing Data? Preprocessing data adalah serangkaian langkah yang dilakukan untuk membersihkan, memformat, dan mengubah data mentah agar siap digunakan dalam analisis atau pembuatan model machine learning. Langkah-langkah ini bertujuan untuk meningkatkan kualitas data dan memastikan model yang dibangun dapat memberikan hasil yang optimal.\nMengapa Preprocessing Data Itu Penting? Data yang tidak diproses dengan baik dapat mengarah pada model yang buruk atau bahkan tidak dapat digunakan sama sekali. Berikut beberapa alasan mengapa preprocessing sangat penting:\nMembersihkan Data: Menghapus noise, menangani nilai yang hilang, atau memperbaiki kesalahan data. Meningkatkan Akurasi Model: Dengan data yang bersih dan relevan, model machine learning dapat belajar dengan lebih baik. Menghindari Bias: Preprocessing membantu menghindari bias dalam data yang dapat memengaruhi prediksi model. Meningkatkan Kecepatan Pelatihan: Beberapa teknik preprocessing dapat mempercepat waktu pelatihan model. Langkah-langkah Preprocessing yang Umum 1. Menangani Nilai yang Hilang Data yang hilang adalah masalah umum dalam dataset dunia nyata. Ada beberapa cara untuk menangani nilai yang hilang:\nMenghapus Baris atau Kolom: Jika proporsi data yang hilang terlalu besar, bisa saja lebih baik untuk menghapusnya. Imputasi: Mengisi nilai yang hilang dengan statistik seperti rata-rata, median, atau modus. Interpolasi: Menggunakan nilai terdekat untuk mengisi data yang hilang, sering digunakan dalam data time series. 1 2 3 4 import pandas as pd # Imputasi menggunakan rata-rata df.fillna(df.mean(), inplace=True) 2. Normalisasi dan Standarisasi Proses ini penting ketika fitur dalam dataset memiliki skala yang sangat berbeda. Model machine learning seperti K-Nearest Neighbors (KNN) atau SVM sangat dipengaruhi oleh skala fitur.\nNormalisasi: Menyelaraskan data ke dalam rentang [0, 1] atau [-1, 1]. Standarisasi: Mengubah data sehingga memiliki rata-rata 0 dan deviasi standar 1. 1 2 3 4 5 6 7 8 9 from sklearn.preprocessing import StandardScaler, MinMaxScaler # Standarisasi scaler = StandardScaler() df_scaled = scaler.fit_transform(df) # Normalisasi min_max_scaler = MinMaxScaler() df_normalized = min_max_scaler.fit_transform(df) 3. Pengkodean Kategorikal Data kategorikal harus dikonversi ke format numerik agar dapat digunakan dalam model machine learning. Ada beberapa cara untuk melakukan ini:\nLabel Encoding: Mengonversi kategori menjadi angka. One-Hot Encoding: Membuat kolom biner untuk setiap kategori. 1 2 3 4 5 6 7 8 from sklearn.preprocessing import LabelEncoder, OneHotEncoder # Label Encoding encoder = LabelEncoder() df[\u0026#39;encoded_column\u0026#39;] = encoder.fit_transform(df[\u0026#39;category_column\u0026#39;]) # One-Hot Encoding df_encoded = pd.get_dummies(df[\u0026#39;category_column\u0026#39;]) 4. Deteksi dan Penanganan Outlier Outlier adalah nilai ekstrem yang bisa mempengaruhi model dan analisis. Ada beberapa cara untuk menangani outlier:\nMenghapus Outlier: Menghapus nilai yang terlalu jauh dari rata-rata. Transformasi: Menggunakan transformasi matematis seperti log atau kuadrat untuk meredam efek outlier. 1 2 3 4 5 6 7 # Deteksi outlier menggunakan IQR Q1 = df[\u0026#39;column\u0026#39;].quantile(0.25) Q3 = df[\u0026#39;column\u0026#39;].quantile(0.75) IQR = Q3 - Q1 # Menghapus outlier df_no_outliers = df[(df[\u0026#39;column\u0026#39;] \u0026gt;= (Q1 - 1.5 * IQR)) \u0026amp; (df[\u0026#39;column\u0026#39;] \u0026lt;= (Q3 + 1.5 * IQR))] 5. Pembagian Data untuk Pelatihan dan Pengujian Sebelum melatih model, penting untuk membagi data menjadi dua set: data pelatihan dan data pengujian. Pembagian yang umum adalah 80% untuk pelatihan dan 20% untuk pengujian.\n1 2 3 4 from sklearn.model_selection import train_test_split # Membagi data X_train, X_test, y_train, y_test = train_test_split(df.drop(\u0026#39;target\u0026#39;, axis=1), df[\u0026#39;target\u0026#39;], test_size=0.2, random_state=42) Kesimpulan Preprocessing data adalah langkah krusial dalam pipeline machine learning. Dengan menangani nilai yang hilang, normalisasi, pengkodean kategorikal, dan deteksi outlier, kita dapat memastikan bahwa model machine learning kita dilatih dengan data yang berkualitas tinggi. Meskipun preprocessing bisa memakan waktu, langkah ini sangat berpengaruh terhadap kinerja dan akurasi model yang dihasilkan. Pastikan untuk selalu memeriksa dataset Anda dan menerapkan teknik preprocessing yang tepat untuk mencapai hasil yang terbaik.\n","permalink":"http://localhost:1313/blogs/cara-preprocessing-data-dengan-baik/","summary":"Cara Preprocessing Data dengan Baik","title":"Cara Preprocessing Data dengan Baik"},{"content":"Demo Demo ","permalink":"http://localhost:1313/blogs/cara-menggunakan-numpy/","summary":"Exploratory Data Analysis of Amazon\u0026rsquo;s top 50 bestselling books 2009 - 2019","title":"Cara menggunakan numpy"},{"content":"Introduction The Nota Fiscal Goiana is an economic program implemented by the state of Goiás, Brazil. This program is designed to boost the collection of the Tax on Circulation of Goods and Services (ICMS) by encouraging people to ask for receipts when they buy goods and services within the state. Additionally, the program includes monthly raffles that distribute cash prizes to participating citizens.\nAs I regularly checked the Nota Fiscal Goiana portal and realized the website\u0026rsquo;s simplicity, I decided to write a scraper to collect the raffle results. Initially, I aimed to simplify the process of checking if I had won, but as I delved deeper, I realized the potential to analyze the dataset more comprehensively.\nEarly Design and Idea Phase The original idea was to scrape raffle results, store them in a database, and give people the option to access the data directly from the database or just view it through a good-looking Power BI dashboard.\nThis led me to start writing the code for scraping the website and parsing the tables. However, since there were approximately 50 published results not available on the website, I had to resort to checking the Diário Oficial do Estado de Goiás, where the official government documents of Goiás are published in Brazil.\nChallenges Encountered Not all results were completely published on the Diário Oficial, leading to some results being unavailable.\nThe project doesn\u0026rsquo;t have a budget, so automation can\u0026rsquo;t require VM and online databases. Which Power BI requires both.\nSolutions The results that I could find published on the Diário Oficial were saved in the database along with results available on the website.\nInitially, I was using free options of online databases, and the backup of the old data was on an SQLite available on the project repository. However, I realized that when running the scraper I could save the data on the SQLite and commit the changes, when I did the information persisted on the SQLite file and I could just read it.\nTo help automate the dashboard I resorted to using Streamlit instead of Power BI. Because Streamlit allows me to read the SQLite file directly from the GitHub repository and every time someone opens the Streamlit dashboard it reads the current data, so I don\u0026rsquo;t need to set up refreshes.\nCurrent State of the Project Currently, the project extracts monthly the raffle results from the Nota Fiscal Goiana and the state ICMS collection and saves the results in the SQLite file, using GitHub Actions. The Streamlit dashboard reads the SQLite file and displays the information on the Streamlit Community Cloud.\nLessons Learned While the Power BI dashboard was useful, I faced challenges with refreshing it easily. Initially using an SQLite database, I couldn\u0026rsquo;t connect to the file on the repository, necessitating manual updates on my local machine. To overcome this, I explored alternatives like MySQL and PostgreSQL, enabling automated monthly scraping with GitHub Actions and updating the dashboard through the Power BI website.\nThe SQLite database is really helpful and depending on the size and scope of your project, you should look at it as a possible option to store your data.\nGoing forward I intend to maintain the project, making future patches to the scraping script if the website changes. And improving the code as my knowledge matures.\nExplore Further GitHub Repository: Access the project\u0026rsquo;s source files and codes on our GitHub Repository. Currently, everything is in Portuguese, but I plan to translate it in the future.\nStreamlit Dashboard: Explore the live Streamlit dashboard here to interact with the project\u0026rsquo;s data visualization directly.\nAcademic Article: If you\u0026rsquo;re interested in a detailed academic analysis of this project, check out our article here. Please note that the article is in Portuguese.\n","permalink":"http://localhost:1313/blogs/bts-nota-fiscal-goiana/","summary":"A behind-the-scenes decision on the decision-making process of the project Nota Fiscal Goiana.","title":"[BTS] Nota Fiscal Goiana"},{"content":"Have you ever had a purchase blocked by your credit card issuer without you taking any action? If this happened to you, your credit card issuer probably had detected that your purchase didn\u0026rsquo;t match your normal spending habit, maybe you were at a different location or spend a lot of money suddenly. Anyway, the credit card company was using anomaly detection techniques to prevent fraudulent activities.\nWhat is Anomaly Detection? Anomaly detection refers to the problem of finding patterns in data that do not conform to expected behavior. 1\nIn statistics there is a famous distribution that is called \u0026ldquo;The Normal Distribution\u0026rdquo;, this name is not in vain. A lot of random things are normally distributed, for instance, if you select 100 random people and measure their height, and made a distribution plot, you\u0026rsquo;d end up with something looking like a symmetrical bell curve.\nEven though abnormal results are possible, it is expected that most people or things will fall within a normal distribution. When something is divergent from this normality it is said to be an outlier or an anomaly.\nWhen dealing with something like credit card fraud you can create an expectation of a person expanding habits, so if your daily routine consists of waking up and driving to your workplace and buying a coffee, then lunch in New York. Your credit card company would find it weird if you suddenly made a physical purchase in Rio de Janeiro.\nIn simple terms, that is what anomaly detection is trying to do, understand how something works so it can identify if it starts acting weird.\nAnomaly Detection x Class Imbalance in Datasets If you are aware of what class imbalance is, you may be wondering \u0026ldquo;can\u0026rsquo;t I use anomaly detection to solve this class imbalance problem?\u0026rdquo; - sometimes you can, but it\u0026rsquo;s not appropriate.\nClass imbalance occurs when one class has significantly more samples than the other class(es). This can lead to biased models that favor the majority class and perform poorly on minority classes. While anomaly detection focuses on identifying rare or abnormal data points that deviate significantly from the normal patterns in the dataset.\nEven though it has some overlap on what is trying to do, unless you have a class imbalance problem that the minority class can be considered an abnormal behavior, you won\u0026rsquo;t be able to use anomaly detection.\nI talked about class imbalance before on a churn rate problem in a fictitious telecommunication company, if you are interested you can check it here.\nChandola, V., Banerjee, A., and Kumar, V. 2009. Anomaly detection: A survey. ACM Comput. Surv. 41, 3, Article 15 (July 2009), 58 pages. DOI = 10.1145/1541880.1541882\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/blogs/anomaly-detection/","summary":"How can your credit card issuer know if a purchase is made by you or a cloned version of your credit card?","title":"What is Anomaly Detection?"},{"content":"Demo Demo ","permalink":"http://localhost:1313/blogs/cara-menggunakan-tensorflow/","summary":"Cara Menggunakan Tensorflow","title":"Cara Menggunakan Tensorflow"},{"content":"Panduan Preprocessing Data yang Baik dalam Machine Learning Preprocessing adalah langkah penting dalam alur kerja data science dan machine learning. Data yang baik adalah kunci untuk model yang akurat dan dapat diandalkan. Artikel ini akan membahas pentingnya preprocessing data dan beberapa teknik yang umum digunakan untuk mempersiapkan data sebelum digunakan dalam pelatihan model machine learning.\nApa itu Preprocessing Data? Preprocessing data adalah serangkaian langkah yang dilakukan untuk membersihkan, memformat, dan mengubah data mentah agar siap digunakan dalam analisis atau pembuatan model machine learning. Langkah-langkah ini bertujuan untuk meningkatkan kualitas data dan memastikan model yang dibangun dapat memberikan hasil yang optimal.\nMengapa Preprocessing Data Itu Penting? Data yang tidak diproses dengan baik dapat mengarah pada model yang buruk atau bahkan tidak dapat digunakan sama sekali. Berikut beberapa alasan mengapa preprocessing sangat penting:\nMembersihkan Data: Menghapus noise, menangani nilai yang hilang, atau memperbaiki kesalahan data. Meningkatkan Akurasi Model: Dengan data yang bersih dan relevan, model machine learning dapat belajar dengan lebih baik. Menghindari Bias: Preprocessing membantu menghindari bias dalam data yang dapat memengaruhi prediksi model. Meningkatkan Kecepatan Pelatihan: Beberapa teknik preprocessing dapat mempercepat waktu pelatihan model. Langkah-langkah Preprocessing yang Umum 1. Menangani Nilai yang Hilang Data yang hilang adalah masalah umum dalam dataset dunia nyata. Ada beberapa cara untuk menangani nilai yang hilang:\nMenghapus Baris atau Kolom: Jika proporsi data yang hilang terlalu besar, bisa saja lebih baik untuk menghapusnya. Imputasi: Mengisi nilai yang hilang dengan statistik seperti rata-rata, median, atau modus. Interpolasi: Menggunakan nilai terdekat untuk mengisi data yang hilang, sering digunakan dalam data time series. 1 2 3 4 import pandas as pd # Imputasi menggunakan rata-rata df.fillna(df.mean(), inplace=True) 2. Normalisasi dan Standarisasi Proses ini penting ketika fitur dalam dataset memiliki skala yang sangat berbeda. Model machine learning seperti K-Nearest Neighbors (KNN) atau SVM sangat dipengaruhi oleh skala fitur.\nNormalisasi: Menyelaraskan data ke dalam rentang [0, 1] atau [-1, 1]. Standarisasi: Mengubah data sehingga memiliki rata-rata 0 dan deviasi standar 1. 1 2 3 4 5 6 7 8 9 from sklearn.preprocessing import StandardScaler, MinMaxScaler # Standarisasi scaler = StandardScaler() df_scaled = scaler.fit_transform(df) # Normalisasi min_max_scaler = MinMaxScaler() df_normalized = min_max_scaler.fit_transform(df) 3. Pengkodean Kategorikal Data kategorikal harus dikonversi ke format numerik agar dapat digunakan dalam model machine learning. Ada beberapa cara untuk melakukan ini:\nLabel Encoding: Mengonversi kategori menjadi angka. One-Hot Encoding: Membuat kolom biner untuk setiap kategori. 1 2 3 4 5 6 7 8 from sklearn.preprocessing import LabelEncoder, OneHotEncoder # Label Encoding encoder = LabelEncoder() df[\u0026#39;encoded_column\u0026#39;] = encoder.fit_transform(df[\u0026#39;category_column\u0026#39;]) # One-Hot Encoding df_encoded = pd.get_dummies(df[\u0026#39;category_column\u0026#39;]) 4. Deteksi dan Penanganan Outlier Outlier adalah nilai ekstrem yang bisa mempengaruhi model dan analisis. Ada beberapa cara untuk menangani outlier:\nMenghapus Outlier: Menghapus nilai yang terlalu jauh dari rata-rata. Transformasi: Menggunakan transformasi matematis seperti log atau kuadrat untuk meredam efek outlier. 1 2 3 4 5 6 7 # Deteksi outlier menggunakan IQR Q1 = df[\u0026#39;column\u0026#39;].quantile(0.25) Q3 = df[\u0026#39;column\u0026#39;].quantile(0.75) IQR = Q3 - Q1 # Menghapus outlier df_no_outliers = df[(df[\u0026#39;column\u0026#39;] \u0026gt;= (Q1 - 1.5 * IQR)) \u0026amp; (df[\u0026#39;column\u0026#39;] \u0026lt;= (Q3 + 1.5 * IQR))] 5. Pembagian Data untuk Pelatihan dan Pengujian Sebelum melatih model, penting untuk membagi data menjadi dua set: data pelatihan dan data pengujian. Pembagian yang umum adalah 80% untuk pelatihan dan 20% untuk pengujian.\n1 2 3 4 from sklearn.model_selection import train_test_split # Membagi data X_train, X_test, y_train, y_test = train_test_split(df.drop(\u0026#39;target\u0026#39;, axis=1), df[\u0026#39;target\u0026#39;], test_size=0.2, random_state=42) Kesimpulan Preprocessing data adalah langkah krusial dalam pipeline machine learning. Dengan menangani nilai yang hilang, normalisasi, pengkodean kategorikal, dan deteksi outlier, kita dapat memastikan bahwa model machine learning kita dilatih dengan data yang berkualitas tinggi. Meskipun preprocessing bisa memakan waktu, langkah ini sangat berpengaruh terhadap kinerja dan akurasi model yang dihasilkan. Pastikan untuk selalu memeriksa dataset Anda dan menerapkan teknik preprocessing yang tepat untuk mencapai hasil yang terbaik.\n","permalink":"http://localhost:1313/blogs/cara-preprocessing-data-dengan-baik/","summary":"Cara Preprocessing Data dengan Baik","title":"Cara Preprocessing Data dengan Baik"},{"content":"Demo Demo ","permalink":"http://localhost:1313/blogs/cara-menggunakan-numpy/","summary":"Exploratory Data Analysis of Amazon\u0026rsquo;s top 50 bestselling books 2009 - 2019","title":"Cara menggunakan numpy"},{"content":"Introduction The Nota Fiscal Goiana is an economic program implemented by the state of Goiás, Brazil. This program is designed to boost the collection of the Tax on Circulation of Goods and Services (ICMS) by encouraging people to ask for receipts when they buy goods and services within the state. Additionally, the program includes monthly raffles that distribute cash prizes to participating citizens.\nAs I regularly checked the Nota Fiscal Goiana portal and realized the website\u0026rsquo;s simplicity, I decided to write a scraper to collect the raffle results. Initially, I aimed to simplify the process of checking if I had won, but as I delved deeper, I realized the potential to analyze the dataset more comprehensively.\nEarly Design and Idea Phase The original idea was to scrape raffle results, store them in a database, and give people the option to access the data directly from the database or just view it through a good-looking Power BI dashboard.\nThis led me to start writing the code for scraping the website and parsing the tables. However, since there were approximately 50 published results not available on the website, I had to resort to checking the Diário Oficial do Estado de Goiás, where the official government documents of Goiás are published in Brazil.\nChallenges Encountered Not all results were completely published on the Diário Oficial, leading to some results being unavailable.\nThe project doesn\u0026rsquo;t have a budget, so automation can\u0026rsquo;t require VM and online databases. Which Power BI requires both.\nSolutions The results that I could find published on the Diário Oficial were saved in the database along with results available on the website.\nInitially, I was using free options of online databases, and the backup of the old data was on an SQLite available on the project repository. However, I realized that when running the scraper I could save the data on the SQLite and commit the changes, when I did the information persisted on the SQLite file and I could just read it.\nTo help automate the dashboard I resorted to using Streamlit instead of Power BI. Because Streamlit allows me to read the SQLite file directly from the GitHub repository and every time someone opens the Streamlit dashboard it reads the current data, so I don\u0026rsquo;t need to set up refreshes.\nCurrent State of the Project Currently, the project extracts monthly the raffle results from the Nota Fiscal Goiana and the state ICMS collection and saves the results in the SQLite file, using GitHub Actions. The Streamlit dashboard reads the SQLite file and displays the information on the Streamlit Community Cloud.\nLessons Learned While the Power BI dashboard was useful, I faced challenges with refreshing it easily. Initially using an SQLite database, I couldn\u0026rsquo;t connect to the file on the repository, necessitating manual updates on my local machine. To overcome this, I explored alternatives like MySQL and PostgreSQL, enabling automated monthly scraping with GitHub Actions and updating the dashboard through the Power BI website.\nThe SQLite database is really helpful and depending on the size and scope of your project, you should look at it as a possible option to store your data.\nGoing forward I intend to maintain the project, making future patches to the scraping script if the website changes. And improving the code as my knowledge matures.\nExplore Further GitHub Repository: Access the project\u0026rsquo;s source files and codes on our GitHub Repository. Currently, everything is in Portuguese, but I plan to translate it in the future.\nStreamlit Dashboard: Explore the live Streamlit dashboard here to interact with the project\u0026rsquo;s data visualization directly.\nAcademic Article: If you\u0026rsquo;re interested in a detailed academic analysis of this project, check out our article here. Please note that the article is in Portuguese.\n","permalink":"http://localhost:1313/blogs/bts-nota-fiscal-goiana/","summary":"A behind-the-scenes decision on the decision-making process of the project Nota Fiscal Goiana.","title":"[BTS] Nota Fiscal Goiana"},{"content":"Have you ever had a purchase blocked by your credit card issuer without you taking any action? If this happened to you, your credit card issuer probably had detected that your purchase didn\u0026rsquo;t match your normal spending habit, maybe you were at a different location or spend a lot of money suddenly. Anyway, the credit card company was using anomaly detection techniques to prevent fraudulent activities.\nWhat is Anomaly Detection? Anomaly detection refers to the problem of finding patterns in data that do not conform to expected behavior. 1\nIn statistics there is a famous distribution that is called \u0026ldquo;The Normal Distribution\u0026rdquo;, this name is not in vain. A lot of random things are normally distributed, for instance, if you select 100 random people and measure their height, and made a distribution plot, you\u0026rsquo;d end up with something looking like a symmetrical bell curve.\nEven though abnormal results are possible, it is expected that most people or things will fall within a normal distribution. When something is divergent from this normality it is said to be an outlier or an anomaly.\nWhen dealing with something like credit card fraud you can create an expectation of a person expanding habits, so if your daily routine consists of waking up and driving to your workplace and buying a coffee, then lunch in New York. Your credit card company would find it weird if you suddenly made a physical purchase in Rio de Janeiro.\nIn simple terms, that is what anomaly detection is trying to do, understand how something works so it can identify if it starts acting weird.\nAnomaly Detection x Class Imbalance in Datasets If you are aware of what class imbalance is, you may be wondering \u0026ldquo;can\u0026rsquo;t I use anomaly detection to solve this class imbalance problem?\u0026rdquo; - sometimes you can, but it\u0026rsquo;s not appropriate.\nClass imbalance occurs when one class has significantly more samples than the other class(es). This can lead to biased models that favor the majority class and perform poorly on minority classes. While anomaly detection focuses on identifying rare or abnormal data points that deviate significantly from the normal patterns in the dataset.\nEven though it has some overlap on what is trying to do, unless you have a class imbalance problem that the minority class can be considered an abnormal behavior, you won\u0026rsquo;t be able to use anomaly detection.\nI talked about class imbalance before on a churn rate problem in a fictitious telecommunication company, if you are interested you can check it here.\nChandola, V., Banerjee, A., and Kumar, V. 2009. Anomaly detection: A survey. ACM Comput. Surv. 41, 3, Article 15 (July 2009), 58 pages. DOI = 10.1145/1541880.1541882\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/blogs/anomaly-detection/","summary":"How can your credit card issuer know if a purchase is made by you or a cloned version of your credit card?","title":"What is Anomaly Detection?"},{"content":"Demo Demo ","permalink":"http://localhost:1313/blogs/cara-menggunakan-tensorflow/","summary":"Cara Menggunakan Tensorflow","title":"Cara Menggunakan Tensorflow"},{"content":"Panduan Preprocessing Data yang Baik dalam Machine Learning Preprocessing adalah langkah penting dalam alur kerja data science dan machine learning. Data yang baik adalah kunci untuk model yang akurat dan dapat diandalkan. Artikel ini akan membahas pentingnya preprocessing data dan beberapa teknik yang umum digunakan untuk mempersiapkan data sebelum digunakan dalam pelatihan model machine learning.\nApa itu Preprocessing Data? Preprocessing data adalah serangkaian langkah yang dilakukan untuk membersihkan, memformat, dan mengubah data mentah agar siap digunakan dalam analisis atau pembuatan model machine learning. Langkah-langkah ini bertujuan untuk meningkatkan kualitas data dan memastikan model yang dibangun dapat memberikan hasil yang optimal.\nMengapa Preprocessing Data Itu Penting? Data yang tidak diproses dengan baik dapat mengarah pada model yang buruk atau bahkan tidak dapat digunakan sama sekali. Berikut beberapa alasan mengapa preprocessing sangat penting:\nMembersihkan Data: Menghapus noise, menangani nilai yang hilang, atau memperbaiki kesalahan data. Meningkatkan Akurasi Model: Dengan data yang bersih dan relevan, model machine learning dapat belajar dengan lebih baik. Menghindari Bias: Preprocessing membantu menghindari bias dalam data yang dapat memengaruhi prediksi model. Meningkatkan Kecepatan Pelatihan: Beberapa teknik preprocessing dapat mempercepat waktu pelatihan model. Langkah-langkah Preprocessing yang Umum 1. Menangani Nilai yang Hilang Data yang hilang adalah masalah umum dalam dataset dunia nyata. Ada beberapa cara untuk menangani nilai yang hilang:\nMenghapus Baris atau Kolom: Jika proporsi data yang hilang terlalu besar, bisa saja lebih baik untuk menghapusnya. Imputasi: Mengisi nilai yang hilang dengan statistik seperti rata-rata, median, atau modus. Interpolasi: Menggunakan nilai terdekat untuk mengisi data yang hilang, sering digunakan dalam data time series. 1 2 3 4 import pandas as pd # Imputasi menggunakan rata-rata df.fillna(df.mean(), inplace=True) 2. Normalisasi dan Standarisasi Proses ini penting ketika fitur dalam dataset memiliki skala yang sangat berbeda. Model machine learning seperti K-Nearest Neighbors (KNN) atau SVM sangat dipengaruhi oleh skala fitur.\nNormalisasi: Menyelaraskan data ke dalam rentang [0, 1] atau [-1, 1]. Standarisasi: Mengubah data sehingga memiliki rata-rata 0 dan deviasi standar 1. 1 2 3 4 5 6 7 8 9 from sklearn.preprocessing import StandardScaler, MinMaxScaler # Standarisasi scaler = StandardScaler() df_scaled = scaler.fit_transform(df) # Normalisasi min_max_scaler = MinMaxScaler() df_normalized = min_max_scaler.fit_transform(df) 3. Pengkodean Kategorikal Data kategorikal harus dikonversi ke format numerik agar dapat digunakan dalam model machine learning. Ada beberapa cara untuk melakukan ini:\nLabel Encoding: Mengonversi kategori menjadi angka. One-Hot Encoding: Membuat kolom biner untuk setiap kategori. 1 2 3 4 5 6 7 8 from sklearn.preprocessing import LabelEncoder, OneHotEncoder # Label Encoding encoder = LabelEncoder() df[\u0026#39;encoded_column\u0026#39;] = encoder.fit_transform(df[\u0026#39;category_column\u0026#39;]) # One-Hot Encoding df_encoded = pd.get_dummies(df[\u0026#39;category_column\u0026#39;]) 4. Deteksi dan Penanganan Outlier Outlier adalah nilai ekstrem yang bisa mempengaruhi model dan analisis. Ada beberapa cara untuk menangani outlier:\nMenghapus Outlier: Menghapus nilai yang terlalu jauh dari rata-rata. Transformasi: Menggunakan transformasi matematis seperti log atau kuadrat untuk meredam efek outlier. 1 2 3 4 5 6 7 # Deteksi outlier menggunakan IQR Q1 = df[\u0026#39;column\u0026#39;].quantile(0.25) Q3 = df[\u0026#39;column\u0026#39;].quantile(0.75) IQR = Q3 - Q1 # Menghapus outlier df_no_outliers = df[(df[\u0026#39;column\u0026#39;] \u0026gt;= (Q1 - 1.5 * IQR)) \u0026amp; (df[\u0026#39;column\u0026#39;] \u0026lt;= (Q3 + 1.5 * IQR))] 5. Pembagian Data untuk Pelatihan dan Pengujian Sebelum melatih model, penting untuk membagi data menjadi dua set: data pelatihan dan data pengujian. Pembagian yang umum adalah 80% untuk pelatihan dan 20% untuk pengujian.\n1 2 3 4 from sklearn.model_selection import train_test_split # Membagi data X_train, X_test, y_train, y_test = train_test_split(df.drop(\u0026#39;target\u0026#39;, axis=1), df[\u0026#39;target\u0026#39;], test_size=0.2, random_state=42) Kesimpulan Preprocessing data adalah langkah krusial dalam pipeline machine learning. Dengan menangani nilai yang hilang, normalisasi, pengkodean kategorikal, dan deteksi outlier, kita dapat memastikan bahwa model machine learning kita dilatih dengan data yang berkualitas tinggi. Meskipun preprocessing bisa memakan waktu, langkah ini sangat berpengaruh terhadap kinerja dan akurasi model yang dihasilkan. Pastikan untuk selalu memeriksa dataset Anda dan menerapkan teknik preprocessing yang tepat untuk mencapai hasil yang terbaik.\n","permalink":"http://localhost:1313/blogs/cara-preprocessing-data-dengan-baik/","summary":"Cara Preprocessing Data dengan Baik","title":"Cara Preprocessing Data dengan Baik"},{"content":"Demo Demo ","permalink":"http://localhost:1313/blogs/cara-menggunakan-numpy/","summary":"Exploratory Data Analysis of Amazon\u0026rsquo;s top 50 bestselling books 2009 - 2019","title":"Cara menggunakan numpy"},{"content":"Introduction The Nota Fiscal Goiana is an economic program implemented by the state of Goiás, Brazil. This program is designed to boost the collection of the Tax on Circulation of Goods and Services (ICMS) by encouraging people to ask for receipts when they buy goods and services within the state. Additionally, the program includes monthly raffles that distribute cash prizes to participating citizens.\nAs I regularly checked the Nota Fiscal Goiana portal and realized the website\u0026rsquo;s simplicity, I decided to write a scraper to collect the raffle results. Initially, I aimed to simplify the process of checking if I had won, but as I delved deeper, I realized the potential to analyze the dataset more comprehensively.\nEarly Design and Idea Phase The original idea was to scrape raffle results, store them in a database, and give people the option to access the data directly from the database or just view it through a good-looking Power BI dashboard.\nThis led me to start writing the code for scraping the website and parsing the tables. However, since there were approximately 50 published results not available on the website, I had to resort to checking the Diário Oficial do Estado de Goiás, where the official government documents of Goiás are published in Brazil.\nChallenges Encountered Not all results were completely published on the Diário Oficial, leading to some results being unavailable.\nThe project doesn\u0026rsquo;t have a budget, so automation can\u0026rsquo;t require VM and online databases. Which Power BI requires both.\nSolutions The results that I could find published on the Diário Oficial were saved in the database along with results available on the website.\nInitially, I was using free options of online databases, and the backup of the old data was on an SQLite available on the project repository. However, I realized that when running the scraper I could save the data on the SQLite and commit the changes, when I did the information persisted on the SQLite file and I could just read it.\nTo help automate the dashboard I resorted to using Streamlit instead of Power BI. Because Streamlit allows me to read the SQLite file directly from the GitHub repository and every time someone opens the Streamlit dashboard it reads the current data, so I don\u0026rsquo;t need to set up refreshes.\nCurrent State of the Project Currently, the project extracts monthly the raffle results from the Nota Fiscal Goiana and the state ICMS collection and saves the results in the SQLite file, using GitHub Actions. The Streamlit dashboard reads the SQLite file and displays the information on the Streamlit Community Cloud.\nLessons Learned While the Power BI dashboard was useful, I faced challenges with refreshing it easily. Initially using an SQLite database, I couldn\u0026rsquo;t connect to the file on the repository, necessitating manual updates on my local machine. To overcome this, I explored alternatives like MySQL and PostgreSQL, enabling automated monthly scraping with GitHub Actions and updating the dashboard through the Power BI website.\nThe SQLite database is really helpful and depending on the size and scope of your project, you should look at it as a possible option to store your data.\nGoing forward I intend to maintain the project, making future patches to the scraping script if the website changes. And improving the code as my knowledge matures.\nExplore Further GitHub Repository: Access the project\u0026rsquo;s source files and codes on our GitHub Repository. Currently, everything is in Portuguese, but I plan to translate it in the future.\nStreamlit Dashboard: Explore the live Streamlit dashboard here to interact with the project\u0026rsquo;s data visualization directly.\nAcademic Article: If you\u0026rsquo;re interested in a detailed academic analysis of this project, check out our article here. Please note that the article is in Portuguese.\n","permalink":"http://localhost:1313/blogs/bts-nota-fiscal-goiana/","summary":"A behind-the-scenes decision on the decision-making process of the project Nota Fiscal Goiana.","title":"[BTS] Nota Fiscal Goiana"},{"content":"Have you ever had a purchase blocked by your credit card issuer without you taking any action? If this happened to you, your credit card issuer probably had detected that your purchase didn\u0026rsquo;t match your normal spending habit, maybe you were at a different location or spend a lot of money suddenly. Anyway, the credit card company was using anomaly detection techniques to prevent fraudulent activities.\nWhat is Anomaly Detection? Anomaly detection refers to the problem of finding patterns in data that do not conform to expected behavior. 1\nIn statistics there is a famous distribution that is called \u0026ldquo;The Normal Distribution\u0026rdquo;, this name is not in vain. A lot of random things are normally distributed, for instance, if you select 100 random people and measure their height, and made a distribution plot, you\u0026rsquo;d end up with something looking like a symmetrical bell curve.\nEven though abnormal results are possible, it is expected that most people or things will fall within a normal distribution. When something is divergent from this normality it is said to be an outlier or an anomaly.\nWhen dealing with something like credit card fraud you can create an expectation of a person expanding habits, so if your daily routine consists of waking up and driving to your workplace and buying a coffee, then lunch in New York. Your credit card company would find it weird if you suddenly made a physical purchase in Rio de Janeiro.\nIn simple terms, that is what anomaly detection is trying to do, understand how something works so it can identify if it starts acting weird.\nAnomaly Detection x Class Imbalance in Datasets If you are aware of what class imbalance is, you may be wondering \u0026ldquo;can\u0026rsquo;t I use anomaly detection to solve this class imbalance problem?\u0026rdquo; - sometimes you can, but it\u0026rsquo;s not appropriate.\nClass imbalance occurs when one class has significantly more samples than the other class(es). This can lead to biased models that favor the majority class and perform poorly on minority classes. While anomaly detection focuses on identifying rare or abnormal data points that deviate significantly from the normal patterns in the dataset.\nEven though it has some overlap on what is trying to do, unless you have a class imbalance problem that the minority class can be considered an abnormal behavior, you won\u0026rsquo;t be able to use anomaly detection.\nI talked about class imbalance before on a churn rate problem in a fictitious telecommunication company, if you are interested you can check it here.\nChandola, V., Banerjee, A., and Kumar, V. 2009. Anomaly detection: A survey. ACM Comput. Surv. 41, 3, Article 15 (July 2009), 58 pages. DOI = 10.1145/1541880.1541882\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/blogs/anomaly-detection/","summary":"How can your credit card issuer know if a purchase is made by you or a cloned version of your credit card?","title":"What is Anomaly Detection?"},{"content":"Demo Demo ","permalink":"http://localhost:1313/blogs/cara-menggunakan-tensorflow/","summary":"Cara Menggunakan Tensorflow","title":"Cara Menggunakan Tensorflow"},{"content":"Panduan Preprocessing Data yang Baik dalam Machine Learning Preprocessing adalah langkah penting dalam alur kerja data science dan machine learning. Data yang baik adalah kunci untuk model yang akurat dan dapat diandalkan. Artikel ini akan membahas pentingnya preprocessing data dan beberapa teknik yang umum digunakan untuk mempersiapkan data sebelum digunakan dalam pelatihan model machine learning.\nApa itu Preprocessing Data? Preprocessing data adalah serangkaian langkah yang dilakukan untuk membersihkan, memformat, dan mengubah data mentah agar siap digunakan dalam analisis atau pembuatan model machine learning. Langkah-langkah ini bertujuan untuk meningkatkan kualitas data dan memastikan model yang dibangun dapat memberikan hasil yang optimal.\nMengapa Preprocessing Data Itu Penting? Data yang tidak diproses dengan baik dapat mengarah pada model yang buruk atau bahkan tidak dapat digunakan sama sekali. Berikut beberapa alasan mengapa preprocessing sangat penting:\nMembersihkan Data: Menghapus noise, menangani nilai yang hilang, atau memperbaiki kesalahan data. Meningkatkan Akurasi Model: Dengan data yang bersih dan relevan, model machine learning dapat belajar dengan lebih baik. Menghindari Bias: Preprocessing membantu menghindari bias dalam data yang dapat memengaruhi prediksi model. Meningkatkan Kecepatan Pelatihan: Beberapa teknik preprocessing dapat mempercepat waktu pelatihan model. Langkah-langkah Preprocessing yang Umum 1. Menangani Nilai yang Hilang Data yang hilang adalah masalah umum dalam dataset dunia nyata. Ada beberapa cara untuk menangani nilai yang hilang:\nMenghapus Baris atau Kolom: Jika proporsi data yang hilang terlalu besar, bisa saja lebih baik untuk menghapusnya. Imputasi: Mengisi nilai yang hilang dengan statistik seperti rata-rata, median, atau modus. Interpolasi: Menggunakan nilai terdekat untuk mengisi data yang hilang, sering digunakan dalam data time series. 1 2 3 4 import pandas as pd # Imputasi menggunakan rata-rata df.fillna(df.mean(), inplace=True) 2. Normalisasi dan Standarisasi Proses ini penting ketika fitur dalam dataset memiliki skala yang sangat berbeda. Model machine learning seperti K-Nearest Neighbors (KNN) atau SVM sangat dipengaruhi oleh skala fitur.\nNormalisasi: Menyelaraskan data ke dalam rentang [0, 1] atau [-1, 1]. Standarisasi: Mengubah data sehingga memiliki rata-rata 0 dan deviasi standar 1. 1 2 3 4 5 6 7 8 9 from sklearn.preprocessing import StandardScaler, MinMaxScaler # Standarisasi scaler = StandardScaler() df_scaled = scaler.fit_transform(df) # Normalisasi min_max_scaler = MinMaxScaler() df_normalized = min_max_scaler.fit_transform(df) 3. Pengkodean Kategorikal Data kategorikal harus dikonversi ke format numerik agar dapat digunakan dalam model machine learning. Ada beberapa cara untuk melakukan ini:\nLabel Encoding: Mengonversi kategori menjadi angka. One-Hot Encoding: Membuat kolom biner untuk setiap kategori. 1 2 3 4 5 6 7 8 from sklearn.preprocessing import LabelEncoder, OneHotEncoder # Label Encoding encoder = LabelEncoder() df[\u0026#39;encoded_column\u0026#39;] = encoder.fit_transform(df[\u0026#39;category_column\u0026#39;]) # One-Hot Encoding df_encoded = pd.get_dummies(df[\u0026#39;category_column\u0026#39;]) 4. Deteksi dan Penanganan Outlier Outlier adalah nilai ekstrem yang bisa mempengaruhi model dan analisis. Ada beberapa cara untuk menangani outlier:\nMenghapus Outlier: Menghapus nilai yang terlalu jauh dari rata-rata. Transformasi: Menggunakan transformasi matematis seperti log atau kuadrat untuk meredam efek outlier. 1 2 3 4 5 6 7 # Deteksi outlier menggunakan IQR Q1 = df[\u0026#39;column\u0026#39;].quantile(0.25) Q3 = df[\u0026#39;column\u0026#39;].quantile(0.75) IQR = Q3 - Q1 # Menghapus outlier df_no_outliers = df[(df[\u0026#39;column\u0026#39;] \u0026gt;= (Q1 - 1.5 * IQR)) \u0026amp; (df[\u0026#39;column\u0026#39;] \u0026lt;= (Q3 + 1.5 * IQR))] 5. Pembagian Data untuk Pelatihan dan Pengujian Sebelum melatih model, penting untuk membagi data menjadi dua set: data pelatihan dan data pengujian. Pembagian yang umum adalah 80% untuk pelatihan dan 20% untuk pengujian.\n1 2 3 4 from sklearn.model_selection import train_test_split # Membagi data X_train, X_test, y_train, y_test = train_test_split(df.drop(\u0026#39;target\u0026#39;, axis=1), df[\u0026#39;target\u0026#39;], test_size=0.2, random_state=42) Kesimpulan Preprocessing data adalah langkah krusial dalam pipeline machine learning. Dengan menangani nilai yang hilang, normalisasi, pengkodean kategorikal, dan deteksi outlier, kita dapat memastikan bahwa model machine learning kita dilatih dengan data yang berkualitas tinggi. Meskipun preprocessing bisa memakan waktu, langkah ini sangat berpengaruh terhadap kinerja dan akurasi model yang dihasilkan. Pastikan untuk selalu memeriksa dataset Anda dan menerapkan teknik preprocessing yang tepat untuk mencapai hasil yang terbaik.\n","permalink":"http://localhost:1313/blogs/cara-preprocessing-data-dengan-baik/","summary":"Cara Preprocessing Data dengan Baik","title":"Cara Preprocessing Data dengan Baik"},{"content":"Demo Demo ","permalink":"http://localhost:1313/blogs/cara-menggunakan-numpy/","summary":"Exploratory Data Analysis of Amazon\u0026rsquo;s top 50 bestselling books 2009 - 2019","title":"Cara menggunakan numpy"},{"content":"Introduction The Nota Fiscal Goiana is an economic program implemented by the state of Goiás, Brazil. This program is designed to boost the collection of the Tax on Circulation of Goods and Services (ICMS) by encouraging people to ask for receipts when they buy goods and services within the state. Additionally, the program includes monthly raffles that distribute cash prizes to participating citizens.\nAs I regularly checked the Nota Fiscal Goiana portal and realized the website\u0026rsquo;s simplicity, I decided to write a scraper to collect the raffle results. Initially, I aimed to simplify the process of checking if I had won, but as I delved deeper, I realized the potential to analyze the dataset more comprehensively.\nEarly Design and Idea Phase The original idea was to scrape raffle results, store them in a database, and give people the option to access the data directly from the database or just view it through a good-looking Power BI dashboard.\nThis led me to start writing the code for scraping the website and parsing the tables. However, since there were approximately 50 published results not available on the website, I had to resort to checking the Diário Oficial do Estado de Goiás, where the official government documents of Goiás are published in Brazil.\nChallenges Encountered Not all results were completely published on the Diário Oficial, leading to some results being unavailable.\nThe project doesn\u0026rsquo;t have a budget, so automation can\u0026rsquo;t require VM and online databases. Which Power BI requires both.\nSolutions The results that I could find published on the Diário Oficial were saved in the database along with results available on the website.\nInitially, I was using free options of online databases, and the backup of the old data was on an SQLite available on the project repository. However, I realized that when running the scraper I could save the data on the SQLite and commit the changes, when I did the information persisted on the SQLite file and I could just read it.\nTo help automate the dashboard I resorted to using Streamlit instead of Power BI. Because Streamlit allows me to read the SQLite file directly from the GitHub repository and every time someone opens the Streamlit dashboard it reads the current data, so I don\u0026rsquo;t need to set up refreshes.\nCurrent State of the Project Currently, the project extracts monthly the raffle results from the Nota Fiscal Goiana and the state ICMS collection and saves the results in the SQLite file, using GitHub Actions. The Streamlit dashboard reads the SQLite file and displays the information on the Streamlit Community Cloud.\nLessons Learned While the Power BI dashboard was useful, I faced challenges with refreshing it easily. Initially using an SQLite database, I couldn\u0026rsquo;t connect to the file on the repository, necessitating manual updates on my local machine. To overcome this, I explored alternatives like MySQL and PostgreSQL, enabling automated monthly scraping with GitHub Actions and updating the dashboard through the Power BI website.\nThe SQLite database is really helpful and depending on the size and scope of your project, you should look at it as a possible option to store your data.\nGoing forward I intend to maintain the project, making future patches to the scraping script if the website changes. And improving the code as my knowledge matures.\nExplore Further GitHub Repository: Access the project\u0026rsquo;s source files and codes on our GitHub Repository. Currently, everything is in Portuguese, but I plan to translate it in the future.\nStreamlit Dashboard: Explore the live Streamlit dashboard here to interact with the project\u0026rsquo;s data visualization directly.\nAcademic Article: If you\u0026rsquo;re interested in a detailed academic analysis of this project, check out our article here. Please note that the article is in Portuguese.\n","permalink":"http://localhost:1313/blogs/bts-nota-fiscal-goiana/","summary":"A behind-the-scenes decision on the decision-making process of the project Nota Fiscal Goiana.","title":"[BTS] Nota Fiscal Goiana"},{"content":"Have you ever had a purchase blocked by your credit card issuer without you taking any action? If this happened to you, your credit card issuer probably had detected that your purchase didn\u0026rsquo;t match your normal spending habit, maybe you were at a different location or spend a lot of money suddenly. Anyway, the credit card company was using anomaly detection techniques to prevent fraudulent activities.\nWhat is Anomaly Detection? Anomaly detection refers to the problem of finding patterns in data that do not conform to expected behavior. 1\nIn statistics there is a famous distribution that is called \u0026ldquo;The Normal Distribution\u0026rdquo;, this name is not in vain. A lot of random things are normally distributed, for instance, if you select 100 random people and measure their height, and made a distribution plot, you\u0026rsquo;d end up with something looking like a symmetrical bell curve.\nEven though abnormal results are possible, it is expected that most people or things will fall within a normal distribution. When something is divergent from this normality it is said to be an outlier or an anomaly.\nWhen dealing with something like credit card fraud you can create an expectation of a person expanding habits, so if your daily routine consists of waking up and driving to your workplace and buying a coffee, then lunch in New York. Your credit card company would find it weird if you suddenly made a physical purchase in Rio de Janeiro.\nIn simple terms, that is what anomaly detection is trying to do, understand how something works so it can identify if it starts acting weird.\nAnomaly Detection x Class Imbalance in Datasets If you are aware of what class imbalance is, you may be wondering \u0026ldquo;can\u0026rsquo;t I use anomaly detection to solve this class imbalance problem?\u0026rdquo; - sometimes you can, but it\u0026rsquo;s not appropriate.\nClass imbalance occurs when one class has significantly more samples than the other class(es). This can lead to biased models that favor the majority class and perform poorly on minority classes. While anomaly detection focuses on identifying rare or abnormal data points that deviate significantly from the normal patterns in the dataset.\nEven though it has some overlap on what is trying to do, unless you have a class imbalance problem that the minority class can be considered an abnormal behavior, you won\u0026rsquo;t be able to use anomaly detection.\nI talked about class imbalance before on a churn rate problem in a fictitious telecommunication company, if you are interested you can check it here.\nChandola, V., Banerjee, A., and Kumar, V. 2009. Anomaly detection: A survey. ACM Comput. Surv. 41, 3, Article 15 (July 2009), 58 pages. DOI = 10.1145/1541880.1541882\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/blogs/anomaly-detection/","summary":"How can your credit card issuer know if a purchase is made by you or a cloned version of your credit card?","title":"What is Anomaly Detection?"},{"content":"Demo Demo ","permalink":"http://localhost:1313/blogs/cara-menggunakan-tensorflow/","summary":"Cara Menggunakan Tensorflow","title":"Cara Menggunakan Tensorflow"},{"content":"Panduan Preprocessing Data yang Baik dalam Machine Learning Preprocessing adalah langkah penting dalam alur kerja data science dan machine learning. Data yang baik adalah kunci untuk model yang akurat dan dapat diandalkan. Artikel ini akan membahas pentingnya preprocessing data dan beberapa teknik yang umum digunakan untuk mempersiapkan data sebelum digunakan dalam pelatihan model machine learning.\nApa itu Preprocessing Data? Preprocessing data adalah serangkaian langkah yang dilakukan untuk membersihkan, memformat, dan mengubah data mentah agar siap digunakan dalam analisis atau pembuatan model machine learning. Langkah-langkah ini bertujuan untuk meningkatkan kualitas data dan memastikan model yang dibangun dapat memberikan hasil yang optimal.\nMengapa Preprocessing Data Itu Penting? Data yang tidak diproses dengan baik dapat mengarah pada model yang buruk atau bahkan tidak dapat digunakan sama sekali. Berikut beberapa alasan mengapa preprocessing sangat penting:\nMembersihkan Data: Menghapus noise, menangani nilai yang hilang, atau memperbaiki kesalahan data. Meningkatkan Akurasi Model: Dengan data yang bersih dan relevan, model machine learning dapat belajar dengan lebih baik. Menghindari Bias: Preprocessing membantu menghindari bias dalam data yang dapat memengaruhi prediksi model. Meningkatkan Kecepatan Pelatihan: Beberapa teknik preprocessing dapat mempercepat waktu pelatihan model. Langkah-langkah Preprocessing yang Umum 1. Menangani Nilai yang Hilang Data yang hilang adalah masalah umum dalam dataset dunia nyata. Ada beberapa cara untuk menangani nilai yang hilang:\nMenghapus Baris atau Kolom: Jika proporsi data yang hilang terlalu besar, bisa saja lebih baik untuk menghapusnya. Imputasi: Mengisi nilai yang hilang dengan statistik seperti rata-rata, median, atau modus. Interpolasi: Menggunakan nilai terdekat untuk mengisi data yang hilang, sering digunakan dalam data time series. 1 2 3 4 import pandas as pd # Imputasi menggunakan rata-rata df.fillna(df.mean(), inplace=True) 2. Normalisasi dan Standarisasi Proses ini penting ketika fitur dalam dataset memiliki skala yang sangat berbeda. Model machine learning seperti K-Nearest Neighbors (KNN) atau SVM sangat dipengaruhi oleh skala fitur.\nNormalisasi: Menyelaraskan data ke dalam rentang [0, 1] atau [-1, 1]. Standarisasi: Mengubah data sehingga memiliki rata-rata 0 dan deviasi standar 1. 1 2 3 4 5 6 7 8 9 from sklearn.preprocessing import StandardScaler, MinMaxScaler # Standarisasi scaler = StandardScaler() df_scaled = scaler.fit_transform(df) # Normalisasi min_max_scaler = MinMaxScaler() df_normalized = min_max_scaler.fit_transform(df) 3. Pengkodean Kategorikal Data kategorikal harus dikonversi ke format numerik agar dapat digunakan dalam model machine learning. Ada beberapa cara untuk melakukan ini:\nLabel Encoding: Mengonversi kategori menjadi angka. One-Hot Encoding: Membuat kolom biner untuk setiap kategori. 1 2 3 4 5 6 7 8 from sklearn.preprocessing import LabelEncoder, OneHotEncoder # Label Encoding encoder = LabelEncoder() df[\u0026#39;encoded_column\u0026#39;] = encoder.fit_transform(df[\u0026#39;category_column\u0026#39;]) # One-Hot Encoding df_encoded = pd.get_dummies(df[\u0026#39;category_column\u0026#39;]) 4. Deteksi dan Penanganan Outlier Outlier adalah nilai ekstrem yang bisa mempengaruhi model dan analisis. Ada beberapa cara untuk menangani outlier:\nMenghapus Outlier: Menghapus nilai yang terlalu jauh dari rata-rata. Transformasi: Menggunakan transformasi matematis seperti log atau kuadrat untuk meredam efek outlier. 1 2 3 4 5 6 7 # Deteksi outlier menggunakan IQR Q1 = df[\u0026#39;column\u0026#39;].quantile(0.25) Q3 = df[\u0026#39;column\u0026#39;].quantile(0.75) IQR = Q3 - Q1 # Menghapus outlier df_no_outliers = df[(df[\u0026#39;column\u0026#39;] \u0026gt;= (Q1 - 1.5 * IQR)) \u0026amp; (df[\u0026#39;column\u0026#39;] \u0026lt;= (Q3 + 1.5 * IQR))] 5. Pembagian Data untuk Pelatihan dan Pengujian Sebelum melatih model, penting untuk membagi data menjadi dua set: data pelatihan dan data pengujian. Pembagian yang umum adalah 80% untuk pelatihan dan 20% untuk pengujian.\n1 2 3 4 from sklearn.model_selection import train_test_split # Membagi data X_train, X_test, y_train, y_test = train_test_split(df.drop(\u0026#39;target\u0026#39;, axis=1), df[\u0026#39;target\u0026#39;], test_size=0.2, random_state=42) Kesimpulan Preprocessing data adalah langkah krusial dalam pipeline machine learning. Dengan menangani nilai yang hilang, normalisasi, pengkodean kategorikal, dan deteksi outlier, kita dapat memastikan bahwa model machine learning kita dilatih dengan data yang berkualitas tinggi. Meskipun preprocessing bisa memakan waktu, langkah ini sangat berpengaruh terhadap kinerja dan akurasi model yang dihasilkan. Pastikan untuk selalu memeriksa dataset Anda dan menerapkan teknik preprocessing yang tepat untuk mencapai hasil yang terbaik.\n","permalink":"http://localhost:1313/blogs/cara-preprocessing-data-dengan-baik/","summary":"Cara Preprocessing Data dengan Baik","title":"Cara Preprocessing Data dengan Baik"},{"content":"Demo Demo ","permalink":"http://localhost:1313/blogs/cara-menggunakan-numpy/","summary":"Exploratory Data Analysis of Amazon\u0026rsquo;s top 50 bestselling books 2009 - 2019","title":"Cara menggunakan numpy"},{"content":"Introduction The Nota Fiscal Goiana is an economic program implemented by the state of Goiás, Brazil. This program is designed to boost the collection of the Tax on Circulation of Goods and Services (ICMS) by encouraging people to ask for receipts when they buy goods and services within the state. Additionally, the program includes monthly raffles that distribute cash prizes to participating citizens.\nAs I regularly checked the Nota Fiscal Goiana portal and realized the website\u0026rsquo;s simplicity, I decided to write a scraper to collect the raffle results. Initially, I aimed to simplify the process of checking if I had won, but as I delved deeper, I realized the potential to analyze the dataset more comprehensively.\nEarly Design and Idea Phase The original idea was to scrape raffle results, store them in a database, and give people the option to access the data directly from the database or just view it through a good-looking Power BI dashboard.\nThis led me to start writing the code for scraping the website and parsing the tables. However, since there were approximately 50 published results not available on the website, I had to resort to checking the Diário Oficial do Estado de Goiás, where the official government documents of Goiás are published in Brazil.\nChallenges Encountered Not all results were completely published on the Diário Oficial, leading to some results being unavailable.\nThe project doesn\u0026rsquo;t have a budget, so automation can\u0026rsquo;t require VM and online databases. Which Power BI requires both.\nSolutions The results that I could find published on the Diário Oficial were saved in the database along with results available on the website.\nInitially, I was using free options of online databases, and the backup of the old data was on an SQLite available on the project repository. However, I realized that when running the scraper I could save the data on the SQLite and commit the changes, when I did the information persisted on the SQLite file and I could just read it.\nTo help automate the dashboard I resorted to using Streamlit instead of Power BI. Because Streamlit allows me to read the SQLite file directly from the GitHub repository and every time someone opens the Streamlit dashboard it reads the current data, so I don\u0026rsquo;t need to set up refreshes.\nCurrent State of the Project Currently, the project extracts monthly the raffle results from the Nota Fiscal Goiana and the state ICMS collection and saves the results in the SQLite file, using GitHub Actions. The Streamlit dashboard reads the SQLite file and displays the information on the Streamlit Community Cloud.\nLessons Learned While the Power BI dashboard was useful, I faced challenges with refreshing it easily. Initially using an SQLite database, I couldn\u0026rsquo;t connect to the file on the repository, necessitating manual updates on my local machine. To overcome this, I explored alternatives like MySQL and PostgreSQL, enabling automated monthly scraping with GitHub Actions and updating the dashboard through the Power BI website.\nThe SQLite database is really helpful and depending on the size and scope of your project, you should look at it as a possible option to store your data.\nGoing forward I intend to maintain the project, making future patches to the scraping script if the website changes. And improving the code as my knowledge matures.\nExplore Further GitHub Repository: Access the project\u0026rsquo;s source files and codes on our GitHub Repository. Currently, everything is in Portuguese, but I plan to translate it in the future.\nStreamlit Dashboard: Explore the live Streamlit dashboard here to interact with the project\u0026rsquo;s data visualization directly.\nAcademic Article: If you\u0026rsquo;re interested in a detailed academic analysis of this project, check out our article here. Please note that the article is in Portuguese.\n","permalink":"http://localhost:1313/blogs/bts-nota-fiscal-goiana/","summary":"A behind-the-scenes decision on the decision-making process of the project Nota Fiscal Goiana.","title":"[BTS] Nota Fiscal Goiana"},{"content":"Have you ever had a purchase blocked by your credit card issuer without you taking any action? If this happened to you, your credit card issuer probably had detected that your purchase didn\u0026rsquo;t match your normal spending habit, maybe you were at a different location or spend a lot of money suddenly. Anyway, the credit card company was using anomaly detection techniques to prevent fraudulent activities.\nWhat is Anomaly Detection? Anomaly detection refers to the problem of finding patterns in data that do not conform to expected behavior. 1\nIn statistics there is a famous distribution that is called \u0026ldquo;The Normal Distribution\u0026rdquo;, this name is not in vain. A lot of random things are normally distributed, for instance, if you select 100 random people and measure their height, and made a distribution plot, you\u0026rsquo;d end up with something looking like a symmetrical bell curve.\nEven though abnormal results are possible, it is expected that most people or things will fall within a normal distribution. When something is divergent from this normality it is said to be an outlier or an anomaly.\nWhen dealing with something like credit card fraud you can create an expectation of a person expanding habits, so if your daily routine consists of waking up and driving to your workplace and buying a coffee, then lunch in New York. Your credit card company would find it weird if you suddenly made a physical purchase in Rio de Janeiro.\nIn simple terms, that is what anomaly detection is trying to do, understand how something works so it can identify if it starts acting weird.\nAnomaly Detection x Class Imbalance in Datasets If you are aware of what class imbalance is, you may be wondering \u0026ldquo;can\u0026rsquo;t I use anomaly detection to solve this class imbalance problem?\u0026rdquo; - sometimes you can, but it\u0026rsquo;s not appropriate.\nClass imbalance occurs when one class has significantly more samples than the other class(es). This can lead to biased models that favor the majority class and perform poorly on minority classes. While anomaly detection focuses on identifying rare or abnormal data points that deviate significantly from the normal patterns in the dataset.\nEven though it has some overlap on what is trying to do, unless you have a class imbalance problem that the minority class can be considered an abnormal behavior, you won\u0026rsquo;t be able to use anomaly detection.\nI talked about class imbalance before on a churn rate problem in a fictitious telecommunication company, if you are interested you can check it here.\nChandola, V., Banerjee, A., and Kumar, V. 2009. Anomaly detection: A survey. ACM Comput. Surv. 41, 3, Article 15 (July 2009), 58 pages. DOI = 10.1145/1541880.1541882\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/blogs/anomaly-detection/","summary":"How can your credit card issuer know if a purchase is made by you or a cloned version of your credit card?","title":"What is Anomaly Detection?"},{"content":"Demo Demo ","permalink":"http://localhost:1313/blogs/cara-menggunakan-tensorflow/","summary":"Cara Menggunakan Tensorflow","title":"Cara Menggunakan Tensorflow"},{"content":"Panduan Preprocessing Data yang Baik dalam Machine Learning Preprocessing adalah langkah penting dalam alur kerja data science dan machine learning. Data yang baik adalah kunci untuk model yang akurat dan dapat diandalkan. Artikel ini akan membahas pentingnya preprocessing data dan beberapa teknik yang umum digunakan untuk mempersiapkan data sebelum digunakan dalam pelatihan model machine learning.\nApa itu Preprocessing Data? Preprocessing data adalah serangkaian langkah yang dilakukan untuk membersihkan, memformat, dan mengubah data mentah agar siap digunakan dalam analisis atau pembuatan model machine learning. Langkah-langkah ini bertujuan untuk meningkatkan kualitas data dan memastikan model yang dibangun dapat memberikan hasil yang optimal.\nMengapa Preprocessing Data Itu Penting? Data yang tidak diproses dengan baik dapat mengarah pada model yang buruk atau bahkan tidak dapat digunakan sama sekali. Berikut beberapa alasan mengapa preprocessing sangat penting:\nMembersihkan Data: Menghapus noise, menangani nilai yang hilang, atau memperbaiki kesalahan data. Meningkatkan Akurasi Model: Dengan data yang bersih dan relevan, model machine learning dapat belajar dengan lebih baik. Menghindari Bias: Preprocessing membantu menghindari bias dalam data yang dapat memengaruhi prediksi model. Meningkatkan Kecepatan Pelatihan: Beberapa teknik preprocessing dapat mempercepat waktu pelatihan model. Langkah-langkah Preprocessing yang Umum 1. Menangani Nilai yang Hilang Data yang hilang adalah masalah umum dalam dataset dunia nyata. Ada beberapa cara untuk menangani nilai yang hilang:\nMenghapus Baris atau Kolom: Jika proporsi data yang hilang terlalu besar, bisa saja lebih baik untuk menghapusnya. Imputasi: Mengisi nilai yang hilang dengan statistik seperti rata-rata, median, atau modus. Interpolasi: Menggunakan nilai terdekat untuk mengisi data yang hilang, sering digunakan dalam data time series. 1 2 3 4 import pandas as pd # Imputasi menggunakan rata-rata df.fillna(df.mean(), inplace=True) 2. Normalisasi dan Standarisasi Proses ini penting ketika fitur dalam dataset memiliki skala yang sangat berbeda. Model machine learning seperti K-Nearest Neighbors (KNN) atau SVM sangat dipengaruhi oleh skala fitur.\nNormalisasi: Menyelaraskan data ke dalam rentang [0, 1] atau [-1, 1]. Standarisasi: Mengubah data sehingga memiliki rata-rata 0 dan deviasi standar 1. 1 2 3 4 5 6 7 8 9 from sklearn.preprocessing import StandardScaler, MinMaxScaler # Standarisasi scaler = StandardScaler() df_scaled = scaler.fit_transform(df) # Normalisasi min_max_scaler = MinMaxScaler() df_normalized = min_max_scaler.fit_transform(df) 3. Pengkodean Kategorikal Data kategorikal harus dikonversi ke format numerik agar dapat digunakan dalam model machine learning. Ada beberapa cara untuk melakukan ini:\nLabel Encoding: Mengonversi kategori menjadi angka. One-Hot Encoding: Membuat kolom biner untuk setiap kategori. 1 2 3 4 5 6 7 8 from sklearn.preprocessing import LabelEncoder, OneHotEncoder # Label Encoding encoder = LabelEncoder() df[\u0026#39;encoded_column\u0026#39;] = encoder.fit_transform(df[\u0026#39;category_column\u0026#39;]) # One-Hot Encoding df_encoded = pd.get_dummies(df[\u0026#39;category_column\u0026#39;]) 4. Deteksi dan Penanganan Outlier Outlier adalah nilai ekstrem yang bisa mempengaruhi model dan analisis. Ada beberapa cara untuk menangani outlier:\nMenghapus Outlier: Menghapus nilai yang terlalu jauh dari rata-rata. Transformasi: Menggunakan transformasi matematis seperti log atau kuadrat untuk meredam efek outlier. 1 2 3 4 5 6 7 # Deteksi outlier menggunakan IQR Q1 = df[\u0026#39;column\u0026#39;].quantile(0.25) Q3 = df[\u0026#39;column\u0026#39;].quantile(0.75) IQR = Q3 - Q1 # Menghapus outlier df_no_outliers = df[(df[\u0026#39;column\u0026#39;] \u0026gt;= (Q1 - 1.5 * IQR)) \u0026amp; (df[\u0026#39;column\u0026#39;] \u0026lt;= (Q3 + 1.5 * IQR))] 5. Pembagian Data untuk Pelatihan dan Pengujian Sebelum melatih model, penting untuk membagi data menjadi dua set: data pelatihan dan data pengujian. Pembagian yang umum adalah 80% untuk pelatihan dan 20% untuk pengujian.\n1 2 3 4 from sklearn.model_selection import train_test_split # Membagi data X_train, X_test, y_train, y_test = train_test_split(df.drop(\u0026#39;target\u0026#39;, axis=1), df[\u0026#39;target\u0026#39;], test_size=0.2, random_state=42) Kesimpulan Preprocessing data adalah langkah krusial dalam pipeline machine learning. Dengan menangani nilai yang hilang, normalisasi, pengkodean kategorikal, dan deteksi outlier, kita dapat memastikan bahwa model machine learning kita dilatih dengan data yang berkualitas tinggi. Meskipun preprocessing bisa memakan waktu, langkah ini sangat berpengaruh terhadap kinerja dan akurasi model yang dihasilkan. Pastikan untuk selalu memeriksa dataset Anda dan menerapkan teknik preprocessing yang tepat untuk mencapai hasil yang terbaik.\n","permalink":"http://localhost:1313/blogs/cara-preprocessing-data-dengan-baik/","summary":"Cara Preprocessing Data dengan Baik","title":"Cara Preprocessing Data dengan Baik"},{"content":"Demo Demo ","permalink":"http://localhost:1313/blogs/cara-menggunakan-numpy/","summary":"Exploratory Data Analysis of Amazon\u0026rsquo;s top 50 bestselling books 2009 - 2019","title":"Cara menggunakan numpy"},{"content":"Introduction The Nota Fiscal Goiana is an economic program implemented by the state of Goiás, Brazil. This program is designed to boost the collection of the Tax on Circulation of Goods and Services (ICMS) by encouraging people to ask for receipts when they buy goods and services within the state. Additionally, the program includes monthly raffles that distribute cash prizes to participating citizens.\nAs I regularly checked the Nota Fiscal Goiana portal and realized the website\u0026rsquo;s simplicity, I decided to write a scraper to collect the raffle results. Initially, I aimed to simplify the process of checking if I had won, but as I delved deeper, I realized the potential to analyze the dataset more comprehensively.\nEarly Design and Idea Phase The original idea was to scrape raffle results, store them in a database, and give people the option to access the data directly from the database or just view it through a good-looking Power BI dashboard.\nThis led me to start writing the code for scraping the website and parsing the tables. However, since there were approximately 50 published results not available on the website, I had to resort to checking the Diário Oficial do Estado de Goiás, where the official government documents of Goiás are published in Brazil.\nChallenges Encountered Not all results were completely published on the Diário Oficial, leading to some results being unavailable.\nThe project doesn\u0026rsquo;t have a budget, so automation can\u0026rsquo;t require VM and online databases. Which Power BI requires both.\nSolutions The results that I could find published on the Diário Oficial were saved in the database along with results available on the website.\nInitially, I was using free options of online databases, and the backup of the old data was on an SQLite available on the project repository. However, I realized that when running the scraper I could save the data on the SQLite and commit the changes, when I did the information persisted on the SQLite file and I could just read it.\nTo help automate the dashboard I resorted to using Streamlit instead of Power BI. Because Streamlit allows me to read the SQLite file directly from the GitHub repository and every time someone opens the Streamlit dashboard it reads the current data, so I don\u0026rsquo;t need to set up refreshes.\nCurrent State of the Project Currently, the project extracts monthly the raffle results from the Nota Fiscal Goiana and the state ICMS collection and saves the results in the SQLite file, using GitHub Actions. The Streamlit dashboard reads the SQLite file and displays the information on the Streamlit Community Cloud.\nLessons Learned While the Power BI dashboard was useful, I faced challenges with refreshing it easily. Initially using an SQLite database, I couldn\u0026rsquo;t connect to the file on the repository, necessitating manual updates on my local machine. To overcome this, I explored alternatives like MySQL and PostgreSQL, enabling automated monthly scraping with GitHub Actions and updating the dashboard through the Power BI website.\nThe SQLite database is really helpful and depending on the size and scope of your project, you should look at it as a possible option to store your data.\nGoing forward I intend to maintain the project, making future patches to the scraping script if the website changes. And improving the code as my knowledge matures.\nExplore Further GitHub Repository: Access the project\u0026rsquo;s source files and codes on our GitHub Repository. Currently, everything is in Portuguese, but I plan to translate it in the future.\nStreamlit Dashboard: Explore the live Streamlit dashboard here to interact with the project\u0026rsquo;s data visualization directly.\nAcademic Article: If you\u0026rsquo;re interested in a detailed academic analysis of this project, check out our article here. Please note that the article is in Portuguese.\n","permalink":"http://localhost:1313/blogs/bts-nota-fiscal-goiana/","summary":"A behind-the-scenes decision on the decision-making process of the project Nota Fiscal Goiana.","title":"[BTS] Nota Fiscal Goiana"},{"content":"Have you ever had a purchase blocked by your credit card issuer without you taking any action? If this happened to you, your credit card issuer probably had detected that your purchase didn\u0026rsquo;t match your normal spending habit, maybe you were at a different location or spend a lot of money suddenly. Anyway, the credit card company was using anomaly detection techniques to prevent fraudulent activities.\nWhat is Anomaly Detection? Anomaly detection refers to the problem of finding patterns in data that do not conform to expected behavior. 1\nIn statistics there is a famous distribution that is called \u0026ldquo;The Normal Distribution\u0026rdquo;, this name is not in vain. A lot of random things are normally distributed, for instance, if you select 100 random people and measure their height, and made a distribution plot, you\u0026rsquo;d end up with something looking like a symmetrical bell curve.\nEven though abnormal results are possible, it is expected that most people or things will fall within a normal distribution. When something is divergent from this normality it is said to be an outlier or an anomaly.\nWhen dealing with something like credit card fraud you can create an expectation of a person expanding habits, so if your daily routine consists of waking up and driving to your workplace and buying a coffee, then lunch in New York. Your credit card company would find it weird if you suddenly made a physical purchase in Rio de Janeiro.\nIn simple terms, that is what anomaly detection is trying to do, understand how something works so it can identify if it starts acting weird.\nAnomaly Detection x Class Imbalance in Datasets If you are aware of what class imbalance is, you may be wondering \u0026ldquo;can\u0026rsquo;t I use anomaly detection to solve this class imbalance problem?\u0026rdquo; - sometimes you can, but it\u0026rsquo;s not appropriate.\nClass imbalance occurs when one class has significantly more samples than the other class(es). This can lead to biased models that favor the majority class and perform poorly on minority classes. While anomaly detection focuses on identifying rare or abnormal data points that deviate significantly from the normal patterns in the dataset.\nEven though it has some overlap on what is trying to do, unless you have a class imbalance problem that the minority class can be considered an abnormal behavior, you won\u0026rsquo;t be able to use anomaly detection.\nI talked about class imbalance before on a churn rate problem in a fictitious telecommunication company, if you are interested you can check it here.\nChandola, V., Banerjee, A., and Kumar, V. 2009. Anomaly detection: A survey. ACM Comput. Surv. 41, 3, Article 15 (July 2009), 58 pages. DOI = 10.1145/1541880.1541882\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/blogs/anomaly-detection/","summary":"How can your credit card issuer know if a purchase is made by you or a cloned version of your credit card?","title":"What is Anomaly Detection?"},{"content":"Demo Demo ","permalink":"http://localhost:1313/blogs/cara-menggunakan-tensorflow/","summary":"Cara Menggunakan Tensorflow","title":"Cara Menggunakan Tensorflow"},{"content":"Panduan Preprocessing Data yang Baik dalam Machine Learning Preprocessing adalah langkah penting dalam alur kerja data science dan machine learning. Data yang baik adalah kunci untuk model yang akurat dan dapat diandalkan. Artikel ini akan membahas pentingnya preprocessing data dan beberapa teknik yang umum digunakan untuk mempersiapkan data sebelum digunakan dalam pelatihan model machine learning.\nApa itu Preprocessing Data? Preprocessing data adalah serangkaian langkah yang dilakukan untuk membersihkan, memformat, dan mengubah data mentah agar siap digunakan dalam analisis atau pembuatan model machine learning. Langkah-langkah ini bertujuan untuk meningkatkan kualitas data dan memastikan model yang dibangun dapat memberikan hasil yang optimal.\nMengapa Preprocessing Data Itu Penting? Data yang tidak diproses dengan baik dapat mengarah pada model yang buruk atau bahkan tidak dapat digunakan sama sekali. Berikut beberapa alasan mengapa preprocessing sangat penting:\nMembersihkan Data: Menghapus noise, menangani nilai yang hilang, atau memperbaiki kesalahan data. Meningkatkan Akurasi Model: Dengan data yang bersih dan relevan, model machine learning dapat belajar dengan lebih baik. Menghindari Bias: Preprocessing membantu menghindari bias dalam data yang dapat memengaruhi prediksi model. Meningkatkan Kecepatan Pelatihan: Beberapa teknik preprocessing dapat mempercepat waktu pelatihan model. Langkah-langkah Preprocessing yang Umum 1. Menangani Nilai yang Hilang Data yang hilang adalah masalah umum dalam dataset dunia nyata. Ada beberapa cara untuk menangani nilai yang hilang:\nMenghapus Baris atau Kolom: Jika proporsi data yang hilang terlalu besar, bisa saja lebih baik untuk menghapusnya. Imputasi: Mengisi nilai yang hilang dengan statistik seperti rata-rata, median, atau modus. Interpolasi: Menggunakan nilai terdekat untuk mengisi data yang hilang, sering digunakan dalam data time series. 1 2 3 4 import pandas as pd # Imputasi menggunakan rata-rata df.fillna(df.mean(), inplace=True) 2. Normalisasi dan Standarisasi Proses ini penting ketika fitur dalam dataset memiliki skala yang sangat berbeda. Model machine learning seperti K-Nearest Neighbors (KNN) atau SVM sangat dipengaruhi oleh skala fitur.\nNormalisasi: Menyelaraskan data ke dalam rentang [0, 1] atau [-1, 1]. Standarisasi: Mengubah data sehingga memiliki rata-rata 0 dan deviasi standar 1. 1 2 3 4 5 6 7 8 9 from sklearn.preprocessing import StandardScaler, MinMaxScaler # Standarisasi scaler = StandardScaler() df_scaled = scaler.fit_transform(df) # Normalisasi min_max_scaler = MinMaxScaler() df_normalized = min_max_scaler.fit_transform(df) 3. Pengkodean Kategorikal Data kategorikal harus dikonversi ke format numerik agar dapat digunakan dalam model machine learning. Ada beberapa cara untuk melakukan ini:\nLabel Encoding: Mengonversi kategori menjadi angka. One-Hot Encoding: Membuat kolom biner untuk setiap kategori. 1 2 3 4 5 6 7 8 from sklearn.preprocessing import LabelEncoder, OneHotEncoder # Label Encoding encoder = LabelEncoder() df[\u0026#39;encoded_column\u0026#39;] = encoder.fit_transform(df[\u0026#39;category_column\u0026#39;]) # One-Hot Encoding df_encoded = pd.get_dummies(df[\u0026#39;category_column\u0026#39;]) 4. Deteksi dan Penanganan Outlier Outlier adalah nilai ekstrem yang bisa mempengaruhi model dan analisis. Ada beberapa cara untuk menangani outlier:\nMenghapus Outlier: Menghapus nilai yang terlalu jauh dari rata-rata. Transformasi: Menggunakan transformasi matematis seperti log atau kuadrat untuk meredam efek outlier. 1 2 3 4 5 6 7 # Deteksi outlier menggunakan IQR Q1 = df[\u0026#39;column\u0026#39;].quantile(0.25) Q3 = df[\u0026#39;column\u0026#39;].quantile(0.75) IQR = Q3 - Q1 # Menghapus outlier df_no_outliers = df[(df[\u0026#39;column\u0026#39;] \u0026gt;= (Q1 - 1.5 * IQR)) \u0026amp; (df[\u0026#39;column\u0026#39;] \u0026lt;= (Q3 + 1.5 * IQR))] 5. Pembagian Data untuk Pelatihan dan Pengujian Sebelum melatih model, penting untuk membagi data menjadi dua set: data pelatihan dan data pengujian. Pembagian yang umum adalah 80% untuk pelatihan dan 20% untuk pengujian.\n1 2 3 4 from sklearn.model_selection import train_test_split # Membagi data X_train, X_test, y_train, y_test = train_test_split(df.drop(\u0026#39;target\u0026#39;, axis=1), df[\u0026#39;target\u0026#39;], test_size=0.2, random_state=42) Kesimpulan Preprocessing data adalah langkah krusial dalam pipeline machine learning. Dengan menangani nilai yang hilang, normalisasi, pengkodean kategorikal, dan deteksi outlier, kita dapat memastikan bahwa model machine learning kita dilatih dengan data yang berkualitas tinggi. Meskipun preprocessing bisa memakan waktu, langkah ini sangat berpengaruh terhadap kinerja dan akurasi model yang dihasilkan. Pastikan untuk selalu memeriksa dataset Anda dan menerapkan teknik preprocessing yang tepat untuk mencapai hasil yang terbaik.\n","permalink":"http://localhost:1313/blogs/cara-preprocessing-data-dengan-baik/","summary":"Cara Preprocessing Data dengan Baik","title":"Cara Preprocessing Data dengan Baik"},{"content":"Demo Demo ","permalink":"http://localhost:1313/blogs/cara-menggunakan-numpy/","summary":"Exploratory Data Analysis of Amazon\u0026rsquo;s top 50 bestselling books 2009 - 2019","title":"Cara menggunakan numpy"},{"content":"Introduction The Nota Fiscal Goiana is an economic program implemented by the state of Goiás, Brazil. This program is designed to boost the collection of the Tax on Circulation of Goods and Services (ICMS) by encouraging people to ask for receipts when they buy goods and services within the state. Additionally, the program includes monthly raffles that distribute cash prizes to participating citizens.\nAs I regularly checked the Nota Fiscal Goiana portal and realized the website\u0026rsquo;s simplicity, I decided to write a scraper to collect the raffle results. Initially, I aimed to simplify the process of checking if I had won, but as I delved deeper, I realized the potential to analyze the dataset more comprehensively.\nEarly Design and Idea Phase The original idea was to scrape raffle results, store them in a database, and give people the option to access the data directly from the database or just view it through a good-looking Power BI dashboard.\nThis led me to start writing the code for scraping the website and parsing the tables. However, since there were approximately 50 published results not available on the website, I had to resort to checking the Diário Oficial do Estado de Goiás, where the official government documents of Goiás are published in Brazil.\nChallenges Encountered Not all results were completely published on the Diário Oficial, leading to some results being unavailable.\nThe project doesn\u0026rsquo;t have a budget, so automation can\u0026rsquo;t require VM and online databases. Which Power BI requires both.\nSolutions The results that I could find published on the Diário Oficial were saved in the database along with results available on the website.\nInitially, I was using free options of online databases, and the backup of the old data was on an SQLite available on the project repository. However, I realized that when running the scraper I could save the data on the SQLite and commit the changes, when I did the information persisted on the SQLite file and I could just read it.\nTo help automate the dashboard I resorted to using Streamlit instead of Power BI. Because Streamlit allows me to read the SQLite file directly from the GitHub repository and every time someone opens the Streamlit dashboard it reads the current data, so I don\u0026rsquo;t need to set up refreshes.\nCurrent State of the Project Currently, the project extracts monthly the raffle results from the Nota Fiscal Goiana and the state ICMS collection and saves the results in the SQLite file, using GitHub Actions. The Streamlit dashboard reads the SQLite file and displays the information on the Streamlit Community Cloud.\nLessons Learned While the Power BI dashboard was useful, I faced challenges with refreshing it easily. Initially using an SQLite database, I couldn\u0026rsquo;t connect to the file on the repository, necessitating manual updates on my local machine. To overcome this, I explored alternatives like MySQL and PostgreSQL, enabling automated monthly scraping with GitHub Actions and updating the dashboard through the Power BI website.\nThe SQLite database is really helpful and depending on the size and scope of your project, you should look at it as a possible option to store your data.\nGoing forward I intend to maintain the project, making future patches to the scraping script if the website changes. And improving the code as my knowledge matures.\nExplore Further GitHub Repository: Access the project\u0026rsquo;s source files and codes on our GitHub Repository. Currently, everything is in Portuguese, but I plan to translate it in the future.\nStreamlit Dashboard: Explore the live Streamlit dashboard here to interact with the project\u0026rsquo;s data visualization directly.\nAcademic Article: If you\u0026rsquo;re interested in a detailed academic analysis of this project, check out our article here. Please note that the article is in Portuguese.\n","permalink":"http://localhost:1313/blogs/bts-nota-fiscal-goiana/","summary":"A behind-the-scenes decision on the decision-making process of the project Nota Fiscal Goiana.","title":"[BTS] Nota Fiscal Goiana"},{"content":"Have you ever had a purchase blocked by your credit card issuer without you taking any action? If this happened to you, your credit card issuer probably had detected that your purchase didn\u0026rsquo;t match your normal spending habit, maybe you were at a different location or spend a lot of money suddenly. Anyway, the credit card company was using anomaly detection techniques to prevent fraudulent activities.\nWhat is Anomaly Detection? Anomaly detection refers to the problem of finding patterns in data that do not conform to expected behavior. 1\nIn statistics there is a famous distribution that is called \u0026ldquo;The Normal Distribution\u0026rdquo;, this name is not in vain. A lot of random things are normally distributed, for instance, if you select 100 random people and measure their height, and made a distribution plot, you\u0026rsquo;d end up with something looking like a symmetrical bell curve.\nEven though abnormal results are possible, it is expected that most people or things will fall within a normal distribution. When something is divergent from this normality it is said to be an outlier or an anomaly.\nWhen dealing with something like credit card fraud you can create an expectation of a person expanding habits, so if your daily routine consists of waking up and driving to your workplace and buying a coffee, then lunch in New York. Your credit card company would find it weird if you suddenly made a physical purchase in Rio de Janeiro.\nIn simple terms, that is what anomaly detection is trying to do, understand how something works so it can identify if it starts acting weird.\nAnomaly Detection x Class Imbalance in Datasets If you are aware of what class imbalance is, you may be wondering \u0026ldquo;can\u0026rsquo;t I use anomaly detection to solve this class imbalance problem?\u0026rdquo; - sometimes you can, but it\u0026rsquo;s not appropriate.\nClass imbalance occurs when one class has significantly more samples than the other class(es). This can lead to biased models that favor the majority class and perform poorly on minority classes. While anomaly detection focuses on identifying rare or abnormal data points that deviate significantly from the normal patterns in the dataset.\nEven though it has some overlap on what is trying to do, unless you have a class imbalance problem that the minority class can be considered an abnormal behavior, you won\u0026rsquo;t be able to use anomaly detection.\nI talked about class imbalance before on a churn rate problem in a fictitious telecommunication company, if you are interested you can check it here.\nChandola, V., Banerjee, A., and Kumar, V. 2009. Anomaly detection: A survey. ACM Comput. Surv. 41, 3, Article 15 (July 2009), 58 pages. DOI = 10.1145/1541880.1541882\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/blogs/anomaly-detection/","summary":"How can your credit card issuer know if a purchase is made by you or a cloned version of your credit card?","title":"What is Anomaly Detection?"},{"content":"Demo Demo ","permalink":"http://localhost:1313/blogs/cara-menggunakan-tensorflow/","summary":"Cara Menggunakan Tensorflow","title":"Cara Menggunakan Tensorflow"},{"content":"Panduan Preprocessing Data yang Baik dalam Machine Learning Preprocessing adalah langkah penting dalam alur kerja data science dan machine learning. Data yang baik adalah kunci untuk model yang akurat dan dapat diandalkan. Artikel ini akan membahas pentingnya preprocessing data dan beberapa teknik yang umum digunakan untuk mempersiapkan data sebelum digunakan dalam pelatihan model machine learning.\nApa itu Preprocessing Data? Preprocessing data adalah serangkaian langkah yang dilakukan untuk membersihkan, memformat, dan mengubah data mentah agar siap digunakan dalam analisis atau pembuatan model machine learning. Langkah-langkah ini bertujuan untuk meningkatkan kualitas data dan memastikan model yang dibangun dapat memberikan hasil yang optimal.\nMengapa Preprocessing Data Itu Penting? Data yang tidak diproses dengan baik dapat mengarah pada model yang buruk atau bahkan tidak dapat digunakan sama sekali. Berikut beberapa alasan mengapa preprocessing sangat penting:\nMembersihkan Data: Menghapus noise, menangani nilai yang hilang, atau memperbaiki kesalahan data. Meningkatkan Akurasi Model: Dengan data yang bersih dan relevan, model machine learning dapat belajar dengan lebih baik. Menghindari Bias: Preprocessing membantu menghindari bias dalam data yang dapat memengaruhi prediksi model. Meningkatkan Kecepatan Pelatihan: Beberapa teknik preprocessing dapat mempercepat waktu pelatihan model. Langkah-langkah Preprocessing yang Umum 1. Menangani Nilai yang Hilang Data yang hilang adalah masalah umum dalam dataset dunia nyata. Ada beberapa cara untuk menangani nilai yang hilang:\nMenghapus Baris atau Kolom: Jika proporsi data yang hilang terlalu besar, bisa saja lebih baik untuk menghapusnya. Imputasi: Mengisi nilai yang hilang dengan statistik seperti rata-rata, median, atau modus. Interpolasi: Menggunakan nilai terdekat untuk mengisi data yang hilang, sering digunakan dalam data time series. 1 2 3 4 import pandas as pd # Imputasi menggunakan rata-rata df.fillna(df.mean(), inplace=True) 2. Normalisasi dan Standarisasi Proses ini penting ketika fitur dalam dataset memiliki skala yang sangat berbeda. Model machine learning seperti K-Nearest Neighbors (KNN) atau SVM sangat dipengaruhi oleh skala fitur.\nNormalisasi: Menyelaraskan data ke dalam rentang [0, 1] atau [-1, 1]. Standarisasi: Mengubah data sehingga memiliki rata-rata 0 dan deviasi standar 1. 1 2 3 4 5 6 7 8 9 from sklearn.preprocessing import StandardScaler, MinMaxScaler # Standarisasi scaler = StandardScaler() df_scaled = scaler.fit_transform(df) # Normalisasi min_max_scaler = MinMaxScaler() df_normalized = min_max_scaler.fit_transform(df) 3. Pengkodean Kategorikal Data kategorikal harus dikonversi ke format numerik agar dapat digunakan dalam model machine learning. Ada beberapa cara untuk melakukan ini:\nLabel Encoding: Mengonversi kategori menjadi angka. One-Hot Encoding: Membuat kolom biner untuk setiap kategori. 1 2 3 4 5 6 7 8 from sklearn.preprocessing import LabelEncoder, OneHotEncoder # Label Encoding encoder = LabelEncoder() df[\u0026#39;encoded_column\u0026#39;] = encoder.fit_transform(df[\u0026#39;category_column\u0026#39;]) # One-Hot Encoding df_encoded = pd.get_dummies(df[\u0026#39;category_column\u0026#39;]) 4. Deteksi dan Penanganan Outlier Outlier adalah nilai ekstrem yang bisa mempengaruhi model dan analisis. Ada beberapa cara untuk menangani outlier:\nMenghapus Outlier: Menghapus nilai yang terlalu jauh dari rata-rata. Transformasi: Menggunakan transformasi matematis seperti log atau kuadrat untuk meredam efek outlier. 1 2 3 4 5 6 7 # Deteksi outlier menggunakan IQR Q1 = df[\u0026#39;column\u0026#39;].quantile(0.25) Q3 = df[\u0026#39;column\u0026#39;].quantile(0.75) IQR = Q3 - Q1 # Menghapus outlier df_no_outliers = df[(df[\u0026#39;column\u0026#39;] \u0026gt;= (Q1 - 1.5 * IQR)) \u0026amp; (df[\u0026#39;column\u0026#39;] \u0026lt;= (Q3 + 1.5 * IQR))] 5. Pembagian Data untuk Pelatihan dan Pengujian Sebelum melatih model, penting untuk membagi data menjadi dua set: data pelatihan dan data pengujian. Pembagian yang umum adalah 80% untuk pelatihan dan 20% untuk pengujian.\n1 2 3 4 from sklearn.model_selection import train_test_split # Membagi data X_train, X_test, y_train, y_test = train_test_split(df.drop(\u0026#39;target\u0026#39;, axis=1), df[\u0026#39;target\u0026#39;], test_size=0.2, random_state=42) Kesimpulan Preprocessing data adalah langkah krusial dalam pipeline machine learning. Dengan menangani nilai yang hilang, normalisasi, pengkodean kategorikal, dan deteksi outlier, kita dapat memastikan bahwa model machine learning kita dilatih dengan data yang berkualitas tinggi. Meskipun preprocessing bisa memakan waktu, langkah ini sangat berpengaruh terhadap kinerja dan akurasi model yang dihasilkan. Pastikan untuk selalu memeriksa dataset Anda dan menerapkan teknik preprocessing yang tepat untuk mencapai hasil yang terbaik.\n","permalink":"http://localhost:1313/blogs/cara-preprocessing-data-dengan-baik/","summary":"Cara Preprocessing Data dengan Baik","title":"Cara Preprocessing Data dengan Baik"},{"content":"Demo Demo ","permalink":"http://localhost:1313/blogs/cara-menggunakan-numpy/","summary":"Exploratory Data Analysis of Amazon\u0026rsquo;s top 50 bestselling books 2009 - 2019","title":"Cara menggunakan numpy"},{"content":"Introduction The Nota Fiscal Goiana is an economic program implemented by the state of Goiás, Brazil. This program is designed to boost the collection of the Tax on Circulation of Goods and Services (ICMS) by encouraging people to ask for receipts when they buy goods and services within the state. Additionally, the program includes monthly raffles that distribute cash prizes to participating citizens.\nAs I regularly checked the Nota Fiscal Goiana portal and realized the website\u0026rsquo;s simplicity, I decided to write a scraper to collect the raffle results. Initially, I aimed to simplify the process of checking if I had won, but as I delved deeper, I realized the potential to analyze the dataset more comprehensively.\nEarly Design and Idea Phase The original idea was to scrape raffle results, store them in a database, and give people the option to access the data directly from the database or just view it through a good-looking Power BI dashboard.\nThis led me to start writing the code for scraping the website and parsing the tables. However, since there were approximately 50 published results not available on the website, I had to resort to checking the Diário Oficial do Estado de Goiás, where the official government documents of Goiás are published in Brazil.\nChallenges Encountered Not all results were completely published on the Diário Oficial, leading to some results being unavailable.\nThe project doesn\u0026rsquo;t have a budget, so automation can\u0026rsquo;t require VM and online databases. Which Power BI requires both.\nSolutions The results that I could find published on the Diário Oficial were saved in the database along with results available on the website.\nInitially, I was using free options of online databases, and the backup of the old data was on an SQLite available on the project repository. However, I realized that when running the scraper I could save the data on the SQLite and commit the changes, when I did the information persisted on the SQLite file and I could just read it.\nTo help automate the dashboard I resorted to using Streamlit instead of Power BI. Because Streamlit allows me to read the SQLite file directly from the GitHub repository and every time someone opens the Streamlit dashboard it reads the current data, so I don\u0026rsquo;t need to set up refreshes.\nCurrent State of the Project Currently, the project extracts monthly the raffle results from the Nota Fiscal Goiana and the state ICMS collection and saves the results in the SQLite file, using GitHub Actions. The Streamlit dashboard reads the SQLite file and displays the information on the Streamlit Community Cloud.\nLessons Learned While the Power BI dashboard was useful, I faced challenges with refreshing it easily. Initially using an SQLite database, I couldn\u0026rsquo;t connect to the file on the repository, necessitating manual updates on my local machine. To overcome this, I explored alternatives like MySQL and PostgreSQL, enabling automated monthly scraping with GitHub Actions and updating the dashboard through the Power BI website.\nThe SQLite database is really helpful and depending on the size and scope of your project, you should look at it as a possible option to store your data.\nGoing forward I intend to maintain the project, making future patches to the scraping script if the website changes. And improving the code as my knowledge matures.\nExplore Further GitHub Repository: Access the project\u0026rsquo;s source files and codes on our GitHub Repository. Currently, everything is in Portuguese, but I plan to translate it in the future.\nStreamlit Dashboard: Explore the live Streamlit dashboard here to interact with the project\u0026rsquo;s data visualization directly.\nAcademic Article: If you\u0026rsquo;re interested in a detailed academic analysis of this project, check out our article here. Please note that the article is in Portuguese.\n","permalink":"http://localhost:1313/blogs/bts-nota-fiscal-goiana/","summary":"A behind-the-scenes decision on the decision-making process of the project Nota Fiscal Goiana.","title":"[BTS] Nota Fiscal Goiana"},{"content":"Have you ever had a purchase blocked by your credit card issuer without you taking any action? If this happened to you, your credit card issuer probably had detected that your purchase didn\u0026rsquo;t match your normal spending habit, maybe you were at a different location or spend a lot of money suddenly. Anyway, the credit card company was using anomaly detection techniques to prevent fraudulent activities.\nWhat is Anomaly Detection? Anomaly detection refers to the problem of finding patterns in data that do not conform to expected behavior. 1\nIn statistics there is a famous distribution that is called \u0026ldquo;The Normal Distribution\u0026rdquo;, this name is not in vain. A lot of random things are normally distributed, for instance, if you select 100 random people and measure their height, and made a distribution plot, you\u0026rsquo;d end up with something looking like a symmetrical bell curve.\nEven though abnormal results are possible, it is expected that most people or things will fall within a normal distribution. When something is divergent from this normality it is said to be an outlier or an anomaly.\nWhen dealing with something like credit card fraud you can create an expectation of a person expanding habits, so if your daily routine consists of waking up and driving to your workplace and buying a coffee, then lunch in New York. Your credit card company would find it weird if you suddenly made a physical purchase in Rio de Janeiro.\nIn simple terms, that is what anomaly detection is trying to do, understand how something works so it can identify if it starts acting weird.\nAnomaly Detection x Class Imbalance in Datasets If you are aware of what class imbalance is, you may be wondering \u0026ldquo;can\u0026rsquo;t I use anomaly detection to solve this class imbalance problem?\u0026rdquo; - sometimes you can, but it\u0026rsquo;s not appropriate.\nClass imbalance occurs when one class has significantly more samples than the other class(es). This can lead to biased models that favor the majority class and perform poorly on minority classes. While anomaly detection focuses on identifying rare or abnormal data points that deviate significantly from the normal patterns in the dataset.\nEven though it has some overlap on what is trying to do, unless you have a class imbalance problem that the minority class can be considered an abnormal behavior, you won\u0026rsquo;t be able to use anomaly detection.\nI talked about class imbalance before on a churn rate problem in a fictitious telecommunication company, if you are interested you can check it here.\nChandola, V., Banerjee, A., and Kumar, V. 2009. Anomaly detection: A survey. ACM Comput. Surv. 41, 3, Article 15 (July 2009), 58 pages. DOI = 10.1145/1541880.1541882\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/blogs/anomaly-detection/","summary":"How can your credit card issuer know if a purchase is made by you or a cloned version of your credit card?","title":"What is Anomaly Detection?"},{"content":"Demo Demo ","permalink":"http://localhost:1313/blogs/cara-menggunakan-tensorflow/","summary":"Cara Menggunakan Tensorflow","title":"Cara Menggunakan Tensorflow"},{"content":"Panduan Preprocessing Data yang Baik dalam Machine Learning Preprocessing adalah langkah penting dalam alur kerja data science dan machine learning. Data yang baik adalah kunci untuk model yang akurat dan dapat diandalkan. Artikel ini akan membahas pentingnya preprocessing data dan beberapa teknik yang umum digunakan untuk mempersiapkan data sebelum digunakan dalam pelatihan model machine learning.\nApa itu Preprocessing Data? Preprocessing data adalah serangkaian langkah yang dilakukan untuk membersihkan, memformat, dan mengubah data mentah agar siap digunakan dalam analisis atau pembuatan model machine learning. Langkah-langkah ini bertujuan untuk meningkatkan kualitas data dan memastikan model yang dibangun dapat memberikan hasil yang optimal.\nMengapa Preprocessing Data Itu Penting? Data yang tidak diproses dengan baik dapat mengarah pada model yang buruk atau bahkan tidak dapat digunakan sama sekali. Berikut beberapa alasan mengapa preprocessing sangat penting:\nMembersihkan Data: Menghapus noise, menangani nilai yang hilang, atau memperbaiki kesalahan data. Meningkatkan Akurasi Model: Dengan data yang bersih dan relevan, model machine learning dapat belajar dengan lebih baik. Menghindari Bias: Preprocessing membantu menghindari bias dalam data yang dapat memengaruhi prediksi model. Meningkatkan Kecepatan Pelatihan: Beberapa teknik preprocessing dapat mempercepat waktu pelatihan model. Langkah-langkah Preprocessing yang Umum 1. Menangani Nilai yang Hilang Data yang hilang adalah masalah umum dalam dataset dunia nyata. Ada beberapa cara untuk menangani nilai yang hilang:\nMenghapus Baris atau Kolom: Jika proporsi data yang hilang terlalu besar, bisa saja lebih baik untuk menghapusnya. Imputasi: Mengisi nilai yang hilang dengan statistik seperti rata-rata, median, atau modus. Interpolasi: Menggunakan nilai terdekat untuk mengisi data yang hilang, sering digunakan dalam data time series. 1 2 3 4 import pandas as pd # Imputasi menggunakan rata-rata df.fillna(df.mean(), inplace=True) 2. Normalisasi dan Standarisasi Proses ini penting ketika fitur dalam dataset memiliki skala yang sangat berbeda. Model machine learning seperti K-Nearest Neighbors (KNN) atau SVM sangat dipengaruhi oleh skala fitur.\nNormalisasi: Menyelaraskan data ke dalam rentang [0, 1] atau [-1, 1]. Standarisasi: Mengubah data sehingga memiliki rata-rata 0 dan deviasi standar 1. 1 2 3 4 5 6 7 8 9 from sklearn.preprocessing import StandardScaler, MinMaxScaler # Standarisasi scaler = StandardScaler() df_scaled = scaler.fit_transform(df) # Normalisasi min_max_scaler = MinMaxScaler() df_normalized = min_max_scaler.fit_transform(df) 3. Pengkodean Kategorikal Data kategorikal harus dikonversi ke format numerik agar dapat digunakan dalam model machine learning. Ada beberapa cara untuk melakukan ini:\nLabel Encoding: Mengonversi kategori menjadi angka. One-Hot Encoding: Membuat kolom biner untuk setiap kategori. 1 2 3 4 5 6 7 8 from sklearn.preprocessing import LabelEncoder, OneHotEncoder # Label Encoding encoder = LabelEncoder() df[\u0026#39;encoded_column\u0026#39;] = encoder.fit_transform(df[\u0026#39;category_column\u0026#39;]) # One-Hot Encoding df_encoded = pd.get_dummies(df[\u0026#39;category_column\u0026#39;]) 4. Deteksi dan Penanganan Outlier Outlier adalah nilai ekstrem yang bisa mempengaruhi model dan analisis. Ada beberapa cara untuk menangani outlier:\nMenghapus Outlier: Menghapus nilai yang terlalu jauh dari rata-rata. Transformasi: Menggunakan transformasi matematis seperti log atau kuadrat untuk meredam efek outlier. 1 2 3 4 5 6 7 # Deteksi outlier menggunakan IQR Q1 = df[\u0026#39;column\u0026#39;].quantile(0.25) Q3 = df[\u0026#39;column\u0026#39;].quantile(0.75) IQR = Q3 - Q1 # Menghapus outlier df_no_outliers = df[(df[\u0026#39;column\u0026#39;] \u0026gt;= (Q1 - 1.5 * IQR)) \u0026amp; (df[\u0026#39;column\u0026#39;] \u0026lt;= (Q3 + 1.5 * IQR))] 5. Pembagian Data untuk Pelatihan dan Pengujian Sebelum melatih model, penting untuk membagi data menjadi dua set: data pelatihan dan data pengujian. Pembagian yang umum adalah 80% untuk pelatihan dan 20% untuk pengujian.\n1 2 3 4 from sklearn.model_selection import train_test_split # Membagi data X_train, X_test, y_train, y_test = train_test_split(df.drop(\u0026#39;target\u0026#39;, axis=1), df[\u0026#39;target\u0026#39;], test_size=0.2, random_state=42) Kesimpulan Preprocessing data adalah langkah krusial dalam pipeline machine learning. Dengan menangani nilai yang hilang, normalisasi, pengkodean kategorikal, dan deteksi outlier, kita dapat memastikan bahwa model machine learning kita dilatih dengan data yang berkualitas tinggi. Meskipun preprocessing bisa memakan waktu, langkah ini sangat berpengaruh terhadap kinerja dan akurasi model yang dihasilkan. Pastikan untuk selalu memeriksa dataset Anda dan menerapkan teknik preprocessing yang tepat untuk mencapai hasil yang terbaik.\n","permalink":"http://localhost:1313/blogs/cara-preprocessing-data-dengan-baik/","summary":"Cara Preprocessing Data dengan Baik","title":"Cara Preprocessing Data dengan Baik"},{"content":"Demo Demo ","permalink":"http://localhost:1313/blogs/cara-menggunakan-numpy/","summary":"Exploratory Data Analysis of Amazon\u0026rsquo;s top 50 bestselling books 2009 - 2019","title":"Cara menggunakan numpy"},{"content":"Introduction The Nota Fiscal Goiana is an economic program implemented by the state of Goiás, Brazil. This program is designed to boost the collection of the Tax on Circulation of Goods and Services (ICMS) by encouraging people to ask for receipts when they buy goods and services within the state. Additionally, the program includes monthly raffles that distribute cash prizes to participating citizens.\nAs I regularly checked the Nota Fiscal Goiana portal and realized the website\u0026rsquo;s simplicity, I decided to write a scraper to collect the raffle results. Initially, I aimed to simplify the process of checking if I had won, but as I delved deeper, I realized the potential to analyze the dataset more comprehensively.\nEarly Design and Idea Phase The original idea was to scrape raffle results, store them in a database, and give people the option to access the data directly from the database or just view it through a good-looking Power BI dashboard.\nThis led me to start writing the code for scraping the website and parsing the tables. However, since there were approximately 50 published results not available on the website, I had to resort to checking the Diário Oficial do Estado de Goiás, where the official government documents of Goiás are published in Brazil.\nChallenges Encountered Not all results were completely published on the Diário Oficial, leading to some results being unavailable.\nThe project doesn\u0026rsquo;t have a budget, so automation can\u0026rsquo;t require VM and online databases. Which Power BI requires both.\nSolutions The results that I could find published on the Diário Oficial were saved in the database along with results available on the website.\nInitially, I was using free options of online databases, and the backup of the old data was on an SQLite available on the project repository. However, I realized that when running the scraper I could save the data on the SQLite and commit the changes, when I did the information persisted on the SQLite file and I could just read it.\nTo help automate the dashboard I resorted to using Streamlit instead of Power BI. Because Streamlit allows me to read the SQLite file directly from the GitHub repository and every time someone opens the Streamlit dashboard it reads the current data, so I don\u0026rsquo;t need to set up refreshes.\nCurrent State of the Project Currently, the project extracts monthly the raffle results from the Nota Fiscal Goiana and the state ICMS collection and saves the results in the SQLite file, using GitHub Actions. The Streamlit dashboard reads the SQLite file and displays the information on the Streamlit Community Cloud.\nLessons Learned While the Power BI dashboard was useful, I faced challenges with refreshing it easily. Initially using an SQLite database, I couldn\u0026rsquo;t connect to the file on the repository, necessitating manual updates on my local machine. To overcome this, I explored alternatives like MySQL and PostgreSQL, enabling automated monthly scraping with GitHub Actions and updating the dashboard through the Power BI website.\nThe SQLite database is really helpful and depending on the size and scope of your project, you should look at it as a possible option to store your data.\nGoing forward I intend to maintain the project, making future patches to the scraping script if the website changes. And improving the code as my knowledge matures.\nExplore Further GitHub Repository: Access the project\u0026rsquo;s source files and codes on our GitHub Repository. Currently, everything is in Portuguese, but I plan to translate it in the future.\nStreamlit Dashboard: Explore the live Streamlit dashboard here to interact with the project\u0026rsquo;s data visualization directly.\nAcademic Article: If you\u0026rsquo;re interested in a detailed academic analysis of this project, check out our article here. Please note that the article is in Portuguese.\n","permalink":"http://localhost:1313/blogs/bts-nota-fiscal-goiana/","summary":"A behind-the-scenes decision on the decision-making process of the project Nota Fiscal Goiana.","title":"[BTS] Nota Fiscal Goiana"},{"content":"Have you ever had a purchase blocked by your credit card issuer without you taking any action? If this happened to you, your credit card issuer probably had detected that your purchase didn\u0026rsquo;t match your normal spending habit, maybe you were at a different location or spend a lot of money suddenly. Anyway, the credit card company was using anomaly detection techniques to prevent fraudulent activities.\nWhat is Anomaly Detection? Anomaly detection refers to the problem of finding patterns in data that do not conform to expected behavior. 1\nIn statistics there is a famous distribution that is called \u0026ldquo;The Normal Distribution\u0026rdquo;, this name is not in vain. A lot of random things are normally distributed, for instance, if you select 100 random people and measure their height, and made a distribution plot, you\u0026rsquo;d end up with something looking like a symmetrical bell curve.\nEven though abnormal results are possible, it is expected that most people or things will fall within a normal distribution. When something is divergent from this normality it is said to be an outlier or an anomaly.\nWhen dealing with something like credit card fraud you can create an expectation of a person expanding habits, so if your daily routine consists of waking up and driving to your workplace and buying a coffee, then lunch in New York. Your credit card company would find it weird if you suddenly made a physical purchase in Rio de Janeiro.\nIn simple terms, that is what anomaly detection is trying to do, understand how something works so it can identify if it starts acting weird.\nAnomaly Detection x Class Imbalance in Datasets If you are aware of what class imbalance is, you may be wondering \u0026ldquo;can\u0026rsquo;t I use anomaly detection to solve this class imbalance problem?\u0026rdquo; - sometimes you can, but it\u0026rsquo;s not appropriate.\nClass imbalance occurs when one class has significantly more samples than the other class(es). This can lead to biased models that favor the majority class and perform poorly on minority classes. While anomaly detection focuses on identifying rare or abnormal data points that deviate significantly from the normal patterns in the dataset.\nEven though it has some overlap on what is trying to do, unless you have a class imbalance problem that the minority class can be considered an abnormal behavior, you won\u0026rsquo;t be able to use anomaly detection.\nI talked about class imbalance before on a churn rate problem in a fictitious telecommunication company, if you are interested you can check it here.\nChandola, V., Banerjee, A., and Kumar, V. 2009. Anomaly detection: A survey. ACM Comput. Surv. 41, 3, Article 15 (July 2009), 58 pages. DOI = 10.1145/1541880.1541882\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/blogs/anomaly-detection/","summary":"How can your credit card issuer know if a purchase is made by you or a cloned version of your credit card?","title":"What is Anomaly Detection?"},{"content":"Demo Demo ","permalink":"http://localhost:1313/blogs/cara-menggunakan-tensorflow/","summary":"Cara Menggunakan Tensorflow","title":"Cara Menggunakan Tensorflow"},{"content":"Panduan Preprocessing Data yang Baik dalam Machine Learning Preprocessing adalah langkah penting dalam alur kerja data science dan machine learning. Data yang baik adalah kunci untuk model yang akurat dan dapat diandalkan. Artikel ini akan membahas pentingnya preprocessing data dan beberapa teknik yang umum digunakan untuk mempersiapkan data sebelum digunakan dalam pelatihan model machine learning.\nApa itu Preprocessing Data? Preprocessing data adalah serangkaian langkah yang dilakukan untuk membersihkan, memformat, dan mengubah data mentah agar siap digunakan dalam analisis atau pembuatan model machine learning. Langkah-langkah ini bertujuan untuk meningkatkan kualitas data dan memastikan model yang dibangun dapat memberikan hasil yang optimal.\nMengapa Preprocessing Data Itu Penting? Data yang tidak diproses dengan baik dapat mengarah pada model yang buruk atau bahkan tidak dapat digunakan sama sekali. Berikut beberapa alasan mengapa preprocessing sangat penting:\nMembersihkan Data: Menghapus noise, menangani nilai yang hilang, atau memperbaiki kesalahan data. Meningkatkan Akurasi Model: Dengan data yang bersih dan relevan, model machine learning dapat belajar dengan lebih baik. Menghindari Bias: Preprocessing membantu menghindari bias dalam data yang dapat memengaruhi prediksi model. Meningkatkan Kecepatan Pelatihan: Beberapa teknik preprocessing dapat mempercepat waktu pelatihan model. Langkah-langkah Preprocessing yang Umum 1. Menangani Nilai yang Hilang Data yang hilang adalah masalah umum dalam dataset dunia nyata. Ada beberapa cara untuk menangani nilai yang hilang:\nMenghapus Baris atau Kolom: Jika proporsi data yang hilang terlalu besar, bisa saja lebih baik untuk menghapusnya. Imputasi: Mengisi nilai yang hilang dengan statistik seperti rata-rata, median, atau modus. Interpolasi: Menggunakan nilai terdekat untuk mengisi data yang hilang, sering digunakan dalam data time series. 1 2 3 4 import pandas as pd # Imputasi menggunakan rata-rata df.fillna(df.mean(), inplace=True) 2. Normalisasi dan Standarisasi Proses ini penting ketika fitur dalam dataset memiliki skala yang sangat berbeda. Model machine learning seperti K-Nearest Neighbors (KNN) atau SVM sangat dipengaruhi oleh skala fitur.\nNormalisasi: Menyelaraskan data ke dalam rentang [0, 1] atau [-1, 1]. Standarisasi: Mengubah data sehingga memiliki rata-rata 0 dan deviasi standar 1. 1 2 3 4 5 6 7 8 9 from sklearn.preprocessing import StandardScaler, MinMaxScaler # Standarisasi scaler = StandardScaler() df_scaled = scaler.fit_transform(df) # Normalisasi min_max_scaler = MinMaxScaler() df_normalized = min_max_scaler.fit_transform(df) 3. Pengkodean Kategorikal Data kategorikal harus dikonversi ke format numerik agar dapat digunakan dalam model machine learning. Ada beberapa cara untuk melakukan ini:\nLabel Encoding: Mengonversi kategori menjadi angka. One-Hot Encoding: Membuat kolom biner untuk setiap kategori. 1 2 3 4 5 6 7 8 from sklearn.preprocessing import LabelEncoder, OneHotEncoder # Label Encoding encoder = LabelEncoder() df[\u0026#39;encoded_column\u0026#39;] = encoder.fit_transform(df[\u0026#39;category_column\u0026#39;]) # One-Hot Encoding df_encoded = pd.get_dummies(df[\u0026#39;category_column\u0026#39;]) 4. Deteksi dan Penanganan Outlier Outlier adalah nilai ekstrem yang bisa mempengaruhi model dan analisis. Ada beberapa cara untuk menangani outlier:\nMenghapus Outlier: Menghapus nilai yang terlalu jauh dari rata-rata. Transformasi: Menggunakan transformasi matematis seperti log atau kuadrat untuk meredam efek outlier. 1 2 3 4 5 6 7 # Deteksi outlier menggunakan IQR Q1 = df[\u0026#39;column\u0026#39;].quantile(0.25) Q3 = df[\u0026#39;column\u0026#39;].quantile(0.75) IQR = Q3 - Q1 # Menghapus outlier df_no_outliers = df[(df[\u0026#39;column\u0026#39;] \u0026gt;= (Q1 - 1.5 * IQR)) \u0026amp; (df[\u0026#39;column\u0026#39;] \u0026lt;= (Q3 + 1.5 * IQR))] 5. Pembagian Data untuk Pelatihan dan Pengujian Sebelum melatih model, penting untuk membagi data menjadi dua set: data pelatihan dan data pengujian. Pembagian yang umum adalah 80% untuk pelatihan dan 20% untuk pengujian.\n1 2 3 4 from sklearn.model_selection import train_test_split # Membagi data X_train, X_test, y_train, y_test = train_test_split(df.drop(\u0026#39;target\u0026#39;, axis=1), df[\u0026#39;target\u0026#39;], test_size=0.2, random_state=42) Kesimpulan Preprocessing data adalah langkah krusial dalam pipeline machine learning. Dengan menangani nilai yang hilang, normalisasi, pengkodean kategorikal, dan deteksi outlier, kita dapat memastikan bahwa model machine learning kita dilatih dengan data yang berkualitas tinggi. Meskipun preprocessing bisa memakan waktu, langkah ini sangat berpengaruh terhadap kinerja dan akurasi model yang dihasilkan. Pastikan untuk selalu memeriksa dataset Anda dan menerapkan teknik preprocessing yang tepat untuk mencapai hasil yang terbaik.\n","permalink":"http://localhost:1313/blogs/cara-preprocessing-data-dengan-baik/","summary":"Cara Preprocessing Data dengan Baik","title":"Cara Preprocessing Data dengan Baik"},{"content":"Demo Demo ","permalink":"http://localhost:1313/blogs/cara-menggunakan-numpy/","summary":"Exploratory Data Analysis of Amazon\u0026rsquo;s top 50 bestselling books 2009 - 2019","title":"Cara menggunakan numpy"},{"content":"Introduction The Nota Fiscal Goiana is an economic program implemented by the state of Goiás, Brazil. This program is designed to boost the collection of the Tax on Circulation of Goods and Services (ICMS) by encouraging people to ask for receipts when they buy goods and services within the state. Additionally, the program includes monthly raffles that distribute cash prizes to participating citizens.\nAs I regularly checked the Nota Fiscal Goiana portal and realized the website\u0026rsquo;s simplicity, I decided to write a scraper to collect the raffle results. Initially, I aimed to simplify the process of checking if I had won, but as I delved deeper, I realized the potential to analyze the dataset more comprehensively.\nEarly Design and Idea Phase The original idea was to scrape raffle results, store them in a database, and give people the option to access the data directly from the database or just view it through a good-looking Power BI dashboard.\nThis led me to start writing the code for scraping the website and parsing the tables. However, since there were approximately 50 published results not available on the website, I had to resort to checking the Diário Oficial do Estado de Goiás, where the official government documents of Goiás are published in Brazil.\nChallenges Encountered Not all results were completely published on the Diário Oficial, leading to some results being unavailable.\nThe project doesn\u0026rsquo;t have a budget, so automation can\u0026rsquo;t require VM and online databases. Which Power BI requires both.\nSolutions The results that I could find published on the Diário Oficial were saved in the database along with results available on the website.\nInitially, I was using free options of online databases, and the backup of the old data was on an SQLite available on the project repository. However, I realized that when running the scraper I could save the data on the SQLite and commit the changes, when I did the information persisted on the SQLite file and I could just read it.\nTo help automate the dashboard I resorted to using Streamlit instead of Power BI. Because Streamlit allows me to read the SQLite file directly from the GitHub repository and every time someone opens the Streamlit dashboard it reads the current data, so I don\u0026rsquo;t need to set up refreshes.\nCurrent State of the Project Currently, the project extracts monthly the raffle results from the Nota Fiscal Goiana and the state ICMS collection and saves the results in the SQLite file, using GitHub Actions. The Streamlit dashboard reads the SQLite file and displays the information on the Streamlit Community Cloud.\nLessons Learned While the Power BI dashboard was useful, I faced challenges with refreshing it easily. Initially using an SQLite database, I couldn\u0026rsquo;t connect to the file on the repository, necessitating manual updates on my local machine. To overcome this, I explored alternatives like MySQL and PostgreSQL, enabling automated monthly scraping with GitHub Actions and updating the dashboard through the Power BI website.\nThe SQLite database is really helpful and depending on the size and scope of your project, you should look at it as a possible option to store your data.\nGoing forward I intend to maintain the project, making future patches to the scraping script if the website changes. And improving the code as my knowledge matures.\nExplore Further GitHub Repository: Access the project\u0026rsquo;s source files and codes on our GitHub Repository. Currently, everything is in Portuguese, but I plan to translate it in the future.\nStreamlit Dashboard: Explore the live Streamlit dashboard here to interact with the project\u0026rsquo;s data visualization directly.\nAcademic Article: If you\u0026rsquo;re interested in a detailed academic analysis of this project, check out our article here. Please note that the article is in Portuguese.\n","permalink":"http://localhost:1313/blogs/bts-nota-fiscal-goiana/","summary":"A behind-the-scenes decision on the decision-making process of the project Nota Fiscal Goiana.","title":"[BTS] Nota Fiscal Goiana"},{"content":"Have you ever had a purchase blocked by your credit card issuer without you taking any action? If this happened to you, your credit card issuer probably had detected that your purchase didn\u0026rsquo;t match your normal spending habit, maybe you were at a different location or spend a lot of money suddenly. Anyway, the credit card company was using anomaly detection techniques to prevent fraudulent activities.\nWhat is Anomaly Detection? Anomaly detection refers to the problem of finding patterns in data that do not conform to expected behavior. 1\nIn statistics there is a famous distribution that is called \u0026ldquo;The Normal Distribution\u0026rdquo;, this name is not in vain. A lot of random things are normally distributed, for instance, if you select 100 random people and measure their height, and made a distribution plot, you\u0026rsquo;d end up with something looking like a symmetrical bell curve.\nEven though abnormal results are possible, it is expected that most people or things will fall within a normal distribution. When something is divergent from this normality it is said to be an outlier or an anomaly.\nWhen dealing with something like credit card fraud you can create an expectation of a person expanding habits, so if your daily routine consists of waking up and driving to your workplace and buying a coffee, then lunch in New York. Your credit card company would find it weird if you suddenly made a physical purchase in Rio de Janeiro.\nIn simple terms, that is what anomaly detection is trying to do, understand how something works so it can identify if it starts acting weird.\nAnomaly Detection x Class Imbalance in Datasets If you are aware of what class imbalance is, you may be wondering \u0026ldquo;can\u0026rsquo;t I use anomaly detection to solve this class imbalance problem?\u0026rdquo; - sometimes you can, but it\u0026rsquo;s not appropriate.\nClass imbalance occurs when one class has significantly more samples than the other class(es). This can lead to biased models that favor the majority class and perform poorly on minority classes. While anomaly detection focuses on identifying rare or abnormal data points that deviate significantly from the normal patterns in the dataset.\nEven though it has some overlap on what is trying to do, unless you have a class imbalance problem that the minority class can be considered an abnormal behavior, you won\u0026rsquo;t be able to use anomaly detection.\nI talked about class imbalance before on a churn rate problem in a fictitious telecommunication company, if you are interested you can check it here.\nChandola, V., Banerjee, A., and Kumar, V. 2009. Anomaly detection: A survey. ACM Comput. Surv. 41, 3, Article 15 (July 2009), 58 pages. DOI = 10.1145/1541880.1541882\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/blogs/anomaly-detection/","summary":"How can your credit card issuer know if a purchase is made by you or a cloned version of your credit card?","title":"What is Anomaly Detection?"},{"content":"Demo Demo ","permalink":"http://localhost:1313/blogs/cara-menggunakan-tensorflow/","summary":"Cara Menggunakan Tensorflow","title":"Cara Menggunakan Tensorflow"},{"content":"Panduan Preprocessing Data yang Baik dalam Machine Learning Preprocessing adalah langkah penting dalam alur kerja data science dan machine learning. Data yang baik adalah kunci untuk model yang akurat dan dapat diandalkan. Artikel ini akan membahas pentingnya preprocessing data dan beberapa teknik yang umum digunakan untuk mempersiapkan data sebelum digunakan dalam pelatihan model machine learning.\nApa itu Preprocessing Data? Preprocessing data adalah serangkaian langkah yang dilakukan untuk membersihkan, memformat, dan mengubah data mentah agar siap digunakan dalam analisis atau pembuatan model machine learning. Langkah-langkah ini bertujuan untuk meningkatkan kualitas data dan memastikan model yang dibangun dapat memberikan hasil yang optimal.\nMengapa Preprocessing Data Itu Penting? Data yang tidak diproses dengan baik dapat mengarah pada model yang buruk atau bahkan tidak dapat digunakan sama sekali. Berikut beberapa alasan mengapa preprocessing sangat penting:\nMembersihkan Data: Menghapus noise, menangani nilai yang hilang, atau memperbaiki kesalahan data. Meningkatkan Akurasi Model: Dengan data yang bersih dan relevan, model machine learning dapat belajar dengan lebih baik. Menghindari Bias: Preprocessing membantu menghindari bias dalam data yang dapat memengaruhi prediksi model. Meningkatkan Kecepatan Pelatihan: Beberapa teknik preprocessing dapat mempercepat waktu pelatihan model. Langkah-langkah Preprocessing yang Umum 1. Menangani Nilai yang Hilang Data yang hilang adalah masalah umum dalam dataset dunia nyata. Ada beberapa cara untuk menangani nilai yang hilang:\nMenghapus Baris atau Kolom: Jika proporsi data yang hilang terlalu besar, bisa saja lebih baik untuk menghapusnya. Imputasi: Mengisi nilai yang hilang dengan statistik seperti rata-rata, median, atau modus. Interpolasi: Menggunakan nilai terdekat untuk mengisi data yang hilang, sering digunakan dalam data time series. 1 2 3 4 import pandas as pd # Imputasi menggunakan rata-rata df.fillna(df.mean(), inplace=True) 2. Normalisasi dan Standarisasi Proses ini penting ketika fitur dalam dataset memiliki skala yang sangat berbeda. Model machine learning seperti K-Nearest Neighbors (KNN) atau SVM sangat dipengaruhi oleh skala fitur.\nNormalisasi: Menyelaraskan data ke dalam rentang [0, 1] atau [-1, 1]. Standarisasi: Mengubah data sehingga memiliki rata-rata 0 dan deviasi standar 1. 1 2 3 4 5 6 7 8 9 from sklearn.preprocessing import StandardScaler, MinMaxScaler # Standarisasi scaler = StandardScaler() df_scaled = scaler.fit_transform(df) # Normalisasi min_max_scaler = MinMaxScaler() df_normalized = min_max_scaler.fit_transform(df) 3. Pengkodean Kategorikal Data kategorikal harus dikonversi ke format numerik agar dapat digunakan dalam model machine learning. Ada beberapa cara untuk melakukan ini:\nLabel Encoding: Mengonversi kategori menjadi angka. One-Hot Encoding: Membuat kolom biner untuk setiap kategori. 1 2 3 4 5 6 7 8 from sklearn.preprocessing import LabelEncoder, OneHotEncoder # Label Encoding encoder = LabelEncoder() df[\u0026#39;encoded_column\u0026#39;] = encoder.fit_transform(df[\u0026#39;category_column\u0026#39;]) # One-Hot Encoding df_encoded = pd.get_dummies(df[\u0026#39;category_column\u0026#39;]) 4. Deteksi dan Penanganan Outlier Outlier adalah nilai ekstrem yang bisa mempengaruhi model dan analisis. Ada beberapa cara untuk menangani outlier:\nMenghapus Outlier: Menghapus nilai yang terlalu jauh dari rata-rata. Transformasi: Menggunakan transformasi matematis seperti log atau kuadrat untuk meredam efek outlier. 1 2 3 4 5 6 7 # Deteksi outlier menggunakan IQR Q1 = df[\u0026#39;column\u0026#39;].quantile(0.25) Q3 = df[\u0026#39;column\u0026#39;].quantile(0.75) IQR = Q3 - Q1 # Menghapus outlier df_no_outliers = df[(df[\u0026#39;column\u0026#39;] \u0026gt;= (Q1 - 1.5 * IQR)) \u0026amp; (df[\u0026#39;column\u0026#39;] \u0026lt;= (Q3 + 1.5 * IQR))] 5. Pembagian Data untuk Pelatihan dan Pengujian Sebelum melatih model, penting untuk membagi data menjadi dua set: data pelatihan dan data pengujian. Pembagian yang umum adalah 80% untuk pelatihan dan 20% untuk pengujian.\n1 2 3 4 from sklearn.model_selection import train_test_split # Membagi data X_train, X_test, y_train, y_test = train_test_split(df.drop(\u0026#39;target\u0026#39;, axis=1), df[\u0026#39;target\u0026#39;], test_size=0.2, random_state=42) Kesimpulan Preprocessing data adalah langkah krusial dalam pipeline machine learning. Dengan menangani nilai yang hilang, normalisasi, pengkodean kategorikal, dan deteksi outlier, kita dapat memastikan bahwa model machine learning kita dilatih dengan data yang berkualitas tinggi. Meskipun preprocessing bisa memakan waktu, langkah ini sangat berpengaruh terhadap kinerja dan akurasi model yang dihasilkan. Pastikan untuk selalu memeriksa dataset Anda dan menerapkan teknik preprocessing yang tepat untuk mencapai hasil yang terbaik.\n","permalink":"http://localhost:1313/blogs/cara-preprocessing-data-dengan-baik/","summary":"Cara Preprocessing Data dengan Baik","title":"Cara Preprocessing Data dengan Baik"},{"content":"Demo Demo ","permalink":"http://localhost:1313/blogs/cara-menggunakan-numpy/","summary":"Exploratory Data Analysis of Amazon\u0026rsquo;s top 50 bestselling books 2009 - 2019","title":"Cara menggunakan numpy"},{"content":"Introduction The Nota Fiscal Goiana is an economic program implemented by the state of Goiás, Brazil. This program is designed to boost the collection of the Tax on Circulation of Goods and Services (ICMS) by encouraging people to ask for receipts when they buy goods and services within the state. Additionally, the program includes monthly raffles that distribute cash prizes to participating citizens.\nAs I regularly checked the Nota Fiscal Goiana portal and realized the website\u0026rsquo;s simplicity, I decided to write a scraper to collect the raffle results. Initially, I aimed to simplify the process of checking if I had won, but as I delved deeper, I realized the potential to analyze the dataset more comprehensively.\nEarly Design and Idea Phase The original idea was to scrape raffle results, store them in a database, and give people the option to access the data directly from the database or just view it through a good-looking Power BI dashboard.\nThis led me to start writing the code for scraping the website and parsing the tables. However, since there were approximately 50 published results not available on the website, I had to resort to checking the Diário Oficial do Estado de Goiás, where the official government documents of Goiás are published in Brazil.\nChallenges Encountered Not all results were completely published on the Diário Oficial, leading to some results being unavailable.\nThe project doesn\u0026rsquo;t have a budget, so automation can\u0026rsquo;t require VM and online databases. Which Power BI requires both.\nSolutions The results that I could find published on the Diário Oficial were saved in the database along with results available on the website.\nInitially, I was using free options of online databases, and the backup of the old data was on an SQLite available on the project repository. However, I realized that when running the scraper I could save the data on the SQLite and commit the changes, when I did the information persisted on the SQLite file and I could just read it.\nTo help automate the dashboard I resorted to using Streamlit instead of Power BI. Because Streamlit allows me to read the SQLite file directly from the GitHub repository and every time someone opens the Streamlit dashboard it reads the current data, so I don\u0026rsquo;t need to set up refreshes.\nCurrent State of the Project Currently, the project extracts monthly the raffle results from the Nota Fiscal Goiana and the state ICMS collection and saves the results in the SQLite file, using GitHub Actions. The Streamlit dashboard reads the SQLite file and displays the information on the Streamlit Community Cloud.\nLessons Learned While the Power BI dashboard was useful, I faced challenges with refreshing it easily. Initially using an SQLite database, I couldn\u0026rsquo;t connect to the file on the repository, necessitating manual updates on my local machine. To overcome this, I explored alternatives like MySQL and PostgreSQL, enabling automated monthly scraping with GitHub Actions and updating the dashboard through the Power BI website.\nThe SQLite database is really helpful and depending on the size and scope of your project, you should look at it as a possible option to store your data.\nGoing forward I intend to maintain the project, making future patches to the scraping script if the website changes. And improving the code as my knowledge matures.\nExplore Further GitHub Repository: Access the project\u0026rsquo;s source files and codes on our GitHub Repository. Currently, everything is in Portuguese, but I plan to translate it in the future.\nStreamlit Dashboard: Explore the live Streamlit dashboard here to interact with the project\u0026rsquo;s data visualization directly.\nAcademic Article: If you\u0026rsquo;re interested in a detailed academic analysis of this project, check out our article here. Please note that the article is in Portuguese.\n","permalink":"http://localhost:1313/blogs/bts-nota-fiscal-goiana/","summary":"A behind-the-scenes decision on the decision-making process of the project Nota Fiscal Goiana.","title":"[BTS] Nota Fiscal Goiana"},{"content":"Have you ever had a purchase blocked by your credit card issuer without you taking any action? If this happened to you, your credit card issuer probably had detected that your purchase didn\u0026rsquo;t match your normal spending habit, maybe you were at a different location or spend a lot of money suddenly. Anyway, the credit card company was using anomaly detection techniques to prevent fraudulent activities.\nWhat is Anomaly Detection? Anomaly detection refers to the problem of finding patterns in data that do not conform to expected behavior. 1\nIn statistics there is a famous distribution that is called \u0026ldquo;The Normal Distribution\u0026rdquo;, this name is not in vain. A lot of random things are normally distributed, for instance, if you select 100 random people and measure their height, and made a distribution plot, you\u0026rsquo;d end up with something looking like a symmetrical bell curve.\nEven though abnormal results are possible, it is expected that most people or things will fall within a normal distribution. When something is divergent from this normality it is said to be an outlier or an anomaly.\nWhen dealing with something like credit card fraud you can create an expectation of a person expanding habits, so if your daily routine consists of waking up and driving to your workplace and buying a coffee, then lunch in New York. Your credit card company would find it weird if you suddenly made a physical purchase in Rio de Janeiro.\nIn simple terms, that is what anomaly detection is trying to do, understand how something works so it can identify if it starts acting weird.\nAnomaly Detection x Class Imbalance in Datasets If you are aware of what class imbalance is, you may be wondering \u0026ldquo;can\u0026rsquo;t I use anomaly detection to solve this class imbalance problem?\u0026rdquo; - sometimes you can, but it\u0026rsquo;s not appropriate.\nClass imbalance occurs when one class has significantly more samples than the other class(es). This can lead to biased models that favor the majority class and perform poorly on minority classes. While anomaly detection focuses on identifying rare or abnormal data points that deviate significantly from the normal patterns in the dataset.\nEven though it has some overlap on what is trying to do, unless you have a class imbalance problem that the minority class can be considered an abnormal behavior, you won\u0026rsquo;t be able to use anomaly detection.\nI talked about class imbalance before on a churn rate problem in a fictitious telecommunication company, if you are interested you can check it here.\nChandola, V., Banerjee, A., and Kumar, V. 2009. Anomaly detection: A survey. ACM Comput. Surv. 41, 3, Article 15 (July 2009), 58 pages. DOI = 10.1145/1541880.1541882\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/blogs/anomaly-detection/","summary":"How can your credit card issuer know if a purchase is made by you or a cloned version of your credit card?","title":"What is Anomaly Detection?"},{"content":"Demo Demo ","permalink":"http://localhost:1313/blogs/cara-menggunakan-tensorflow/","summary":"Cara Menggunakan Tensorflow","title":"Cara Menggunakan Tensorflow"},{"content":"Panduan Preprocessing Data yang Baik dalam Machine Learning Preprocessing adalah langkah penting dalam alur kerja data science dan machine learning. Data yang baik adalah kunci untuk model yang akurat dan dapat diandalkan. Artikel ini akan membahas pentingnya preprocessing data dan beberapa teknik yang umum digunakan untuk mempersiapkan data sebelum digunakan dalam pelatihan model machine learning.\nApa itu Preprocessing Data? Preprocessing data adalah serangkaian langkah yang dilakukan untuk membersihkan, memformat, dan mengubah data mentah agar siap digunakan dalam analisis atau pembuatan model machine learning. Langkah-langkah ini bertujuan untuk meningkatkan kualitas data dan memastikan model yang dibangun dapat memberikan hasil yang optimal.\nMengapa Preprocessing Data Itu Penting? Data yang tidak diproses dengan baik dapat mengarah pada model yang buruk atau bahkan tidak dapat digunakan sama sekali. Berikut beberapa alasan mengapa preprocessing sangat penting:\nMembersihkan Data: Menghapus noise, menangani nilai yang hilang, atau memperbaiki kesalahan data. Meningkatkan Akurasi Model: Dengan data yang bersih dan relevan, model machine learning dapat belajar dengan lebih baik. Menghindari Bias: Preprocessing membantu menghindari bias dalam data yang dapat memengaruhi prediksi model. Meningkatkan Kecepatan Pelatihan: Beberapa teknik preprocessing dapat mempercepat waktu pelatihan model. Langkah-langkah Preprocessing yang Umum 1. Menangani Nilai yang Hilang Data yang hilang adalah masalah umum dalam dataset dunia nyata. Ada beberapa cara untuk menangani nilai yang hilang:\nMenghapus Baris atau Kolom: Jika proporsi data yang hilang terlalu besar, bisa saja lebih baik untuk menghapusnya. Imputasi: Mengisi nilai yang hilang dengan statistik seperti rata-rata, median, atau modus. Interpolasi: Menggunakan nilai terdekat untuk mengisi data yang hilang, sering digunakan dalam data time series. 1 2 3 4 import pandas as pd # Imputasi menggunakan rata-rata df.fillna(df.mean(), inplace=True) 2. Normalisasi dan Standarisasi Proses ini penting ketika fitur dalam dataset memiliki skala yang sangat berbeda. Model machine learning seperti K-Nearest Neighbors (KNN) atau SVM sangat dipengaruhi oleh skala fitur.\nNormalisasi: Menyelaraskan data ke dalam rentang [0, 1] atau [-1, 1]. Standarisasi: Mengubah data sehingga memiliki rata-rata 0 dan deviasi standar 1. 1 2 3 4 5 6 7 8 9 from sklearn.preprocessing import StandardScaler, MinMaxScaler # Standarisasi scaler = StandardScaler() df_scaled = scaler.fit_transform(df) # Normalisasi min_max_scaler = MinMaxScaler() df_normalized = min_max_scaler.fit_transform(df) 3. Pengkodean Kategorikal Data kategorikal harus dikonversi ke format numerik agar dapat digunakan dalam model machine learning. Ada beberapa cara untuk melakukan ini:\nLabel Encoding: Mengonversi kategori menjadi angka. One-Hot Encoding: Membuat kolom biner untuk setiap kategori. 1 2 3 4 5 6 7 8 from sklearn.preprocessing import LabelEncoder, OneHotEncoder # Label Encoding encoder = LabelEncoder() df[\u0026#39;encoded_column\u0026#39;] = encoder.fit_transform(df[\u0026#39;category_column\u0026#39;]) # One-Hot Encoding df_encoded = pd.get_dummies(df[\u0026#39;category_column\u0026#39;]) 4. Deteksi dan Penanganan Outlier Outlier adalah nilai ekstrem yang bisa mempengaruhi model dan analisis. Ada beberapa cara untuk menangani outlier:\nMenghapus Outlier: Menghapus nilai yang terlalu jauh dari rata-rata. Transformasi: Menggunakan transformasi matematis seperti log atau kuadrat untuk meredam efek outlier. 1 2 3 4 5 6 7 # Deteksi outlier menggunakan IQR Q1 = df[\u0026#39;column\u0026#39;].quantile(0.25) Q3 = df[\u0026#39;column\u0026#39;].quantile(0.75) IQR = Q3 - Q1 # Menghapus outlier df_no_outliers = df[(df[\u0026#39;column\u0026#39;] \u0026gt;= (Q1 - 1.5 * IQR)) \u0026amp; (df[\u0026#39;column\u0026#39;] \u0026lt;= (Q3 + 1.5 * IQR))] 5. Pembagian Data untuk Pelatihan dan Pengujian Sebelum melatih model, penting untuk membagi data menjadi dua set: data pelatihan dan data pengujian. Pembagian yang umum adalah 80% untuk pelatihan dan 20% untuk pengujian.\n1 2 3 4 from sklearn.model_selection import train_test_split # Membagi data X_train, X_test, y_train, y_test = train_test_split(df.drop(\u0026#39;target\u0026#39;, axis=1), df[\u0026#39;target\u0026#39;], test_size=0.2, random_state=42) Kesimpulan Preprocessing data adalah langkah krusial dalam pipeline machine learning. Dengan menangani nilai yang hilang, normalisasi, pengkodean kategorikal, dan deteksi outlier, kita dapat memastikan bahwa model machine learning kita dilatih dengan data yang berkualitas tinggi. Meskipun preprocessing bisa memakan waktu, langkah ini sangat berpengaruh terhadap kinerja dan akurasi model yang dihasilkan. Pastikan untuk selalu memeriksa dataset Anda dan menerapkan teknik preprocessing yang tepat untuk mencapai hasil yang terbaik.\n","permalink":"http://localhost:1313/blogs/cara-preprocessing-data-dengan-baik/","summary":"Cara Preprocessing Data dengan Baik","title":"Cara Preprocessing Data dengan Baik"},{"content":"Panduan Preprocessing Data yang Baik dalam Machine Learning Preprocessing adalah langkah penting dalam alur kerja data science dan machine learning. Data yang baik adalah kunci untuk model yang akurat dan dapat diandalkan. Artikel ini akan membahas pentingnya preprocessing data dan beberapa teknik yang umum digunakan untuk mempersiapkan data sebelum digunakan dalam pelatihan model machine learning.\nApa itu Preprocessing Data? Preprocessing data adalah serangkaian langkah yang dilakukan untuk membersihkan, memformat, dan mengubah data mentah agar siap digunakan dalam analisis atau pembuatan model machine learning. Langkah-langkah ini bertujuan untuk meningkatkan kualitas data dan memastikan model yang dibangun dapat memberikan hasil yang optimal.\nMengapa Preprocessing Data Itu Penting? Data yang tidak diproses dengan baik dapat mengarah pada model yang buruk atau bahkan tidak dapat digunakan sama sekali. Berikut beberapa alasan mengapa preprocessing sangat penting:\nMembersihkan Data: Menghapus noise, menangani nilai yang hilang, atau memperbaiki kesalahan data. Meningkatkan Akurasi Model: Dengan data yang bersih dan relevan, model machine learning dapat belajar dengan lebih baik. Menghindari Bias: Preprocessing membantu menghindari bias dalam data yang dapat memengaruhi prediksi model. Meningkatkan Kecepatan Pelatihan: Beberapa teknik preprocessing dapat mempercepat waktu pelatihan model. Langkah-langkah Preprocessing yang Umum 1. Menangani Nilai yang Hilang Data yang hilang adalah masalah umum dalam dataset dunia nyata. Ada beberapa cara untuk menangani nilai yang hilang:\nMenghapus Baris atau Kolom: Jika proporsi data yang hilang terlalu besar, bisa saja lebih baik untuk menghapusnya. Imputasi: Mengisi nilai yang hilang dengan statistik seperti rata-rata, median, atau modus. Interpolasi: Menggunakan nilai terdekat untuk mengisi data yang hilang, sering digunakan dalam data time series. 1 2 3 4 import pandas as pd # Imputasi menggunakan rata-rata df.fillna(df.mean(), inplace=True) 2. Normalisasi dan Standarisasi Proses ini penting ketika fitur dalam dataset memiliki skala yang sangat berbeda. Model machine learning seperti K-Nearest Neighbors (KNN) atau SVM sangat dipengaruhi oleh skala fitur.\nNormalisasi: Menyelaraskan data ke dalam rentang [0, 1] atau [-1, 1]. Standarisasi: Mengubah data sehingga memiliki rata-rata 0 dan deviasi standar 1. 1 2 3 4 5 6 7 8 9 from sklearn.preprocessing import StandardScaler, MinMaxScaler # Standarisasi scaler = StandardScaler() df_scaled = scaler.fit_transform(df) # Normalisasi min_max_scaler = MinMaxScaler() df_normalized = min_max_scaler.fit_transform(df) 3. Pengkodean Kategorikal Data kategorikal harus dikonversi ke format numerik agar dapat digunakan dalam model machine learning. Ada beberapa cara untuk melakukan ini:\nLabel Encoding: Mengonversi kategori menjadi angka. One-Hot Encoding: Membuat kolom biner untuk setiap kategori. 1 2 3 4 5 6 7 8 from sklearn.preprocessing import LabelEncoder, OneHotEncoder # Label Encoding encoder = LabelEncoder() df[\u0026#39;encoded_column\u0026#39;] = encoder.fit_transform(df[\u0026#39;category_column\u0026#39;]) # One-Hot Encoding df_encoded = pd.get_dummies(df[\u0026#39;category_column\u0026#39;]) 4. Deteksi dan Penanganan Outlier Outlier adalah nilai ekstrem yang bisa mempengaruhi model dan analisis. Ada beberapa cara untuk menangani outlier:\nMenghapus Outlier: Menghapus nilai yang terlalu jauh dari rata-rata. Transformasi: Menggunakan transformasi matematis seperti log atau kuadrat untuk meredam efek outlier. 1 2 3 4 5 6 7 # Deteksi outlier menggunakan IQR Q1 = df[\u0026#39;column\u0026#39;].quantile(0.25) Q3 = df[\u0026#39;column\u0026#39;].quantile(0.75) IQR = Q3 - Q1 # Menghapus outlier df_no_outliers = df[(df[\u0026#39;column\u0026#39;] \u0026gt;= (Q1 - 1.5 * IQR)) \u0026amp; (df[\u0026#39;column\u0026#39;] \u0026lt;= (Q3 + 1.5 * IQR))] 5. Pembagian Data untuk Pelatihan dan Pengujian Sebelum melatih model, penting untuk membagi data menjadi dua set: data pelatihan dan data pengujian. Pembagian yang umum adalah 80% untuk pelatihan dan 20% untuk pengujian.\n1 2 3 4 from sklearn.model_selection import train_test_split # Membagi data X_train, X_test, y_train, y_test = train_test_split(df.drop(\u0026#39;target\u0026#39;, axis=1), df[\u0026#39;target\u0026#39;], test_size=0.2, random_state=42) Kesimpulan Preprocessing data adalah langkah krusial dalam pipeline machine learning. Dengan menangani nilai yang hilang, normalisasi, pengkodean kategorikal, dan deteksi outlier, kita dapat memastikan bahwa model machine learning kita dilatih dengan data yang berkualitas tinggi. Meskipun preprocessing bisa memakan waktu, langkah ini sangat berpengaruh terhadap kinerja dan akurasi model yang dihasilkan. Pastikan untuk selalu memeriksa dataset Anda dan menerapkan teknik preprocessing yang tepat untuk mencapai hasil yang terbaik.\n","permalink":"http://localhost:1313/blogs/cara-menggunakan-numpy/","summary":"Exploratory Data Analysis of Amazon\u0026rsquo;s top 50 bestselling books 2009 - 2019","title":"Cara menggunakan numpy"},{"content":"Introduction The Nota Fiscal Goiana is an economic program implemented by the state of Goiás, Brazil. This program is designed to boost the collection of the Tax on Circulation of Goods and Services (ICMS) by encouraging people to ask for receipts when they buy goods and services within the state. Additionally, the program includes monthly raffles that distribute cash prizes to participating citizens.\nAs I regularly checked the Nota Fiscal Goiana portal and realized the website\u0026rsquo;s simplicity, I decided to write a scraper to collect the raffle results. Initially, I aimed to simplify the process of checking if I had won, but as I delved deeper, I realized the potential to analyze the dataset more comprehensively.\nEarly Design and Idea Phase The original idea was to scrape raffle results, store them in a database, and give people the option to access the data directly from the database or just view it through a good-looking Power BI dashboard.\nThis led me to start writing the code for scraping the website and parsing the tables. However, since there were approximately 50 published results not available on the website, I had to resort to checking the Diário Oficial do Estado de Goiás, where the official government documents of Goiás are published in Brazil.\nChallenges Encountered Not all results were completely published on the Diário Oficial, leading to some results being unavailable.\nThe project doesn\u0026rsquo;t have a budget, so automation can\u0026rsquo;t require VM and online databases. Which Power BI requires both.\nSolutions The results that I could find published on the Diário Oficial were saved in the database along with results available on the website.\nInitially, I was using free options of online databases, and the backup of the old data was on an SQLite available on the project repository. However, I realized that when running the scraper I could save the data on the SQLite and commit the changes, when I did the information persisted on the SQLite file and I could just read it.\nTo help automate the dashboard I resorted to using Streamlit instead of Power BI. Because Streamlit allows me to read the SQLite file directly from the GitHub repository and every time someone opens the Streamlit dashboard it reads the current data, so I don\u0026rsquo;t need to set up refreshes.\nCurrent State of the Project Currently, the project extracts monthly the raffle results from the Nota Fiscal Goiana and the state ICMS collection and saves the results in the SQLite file, using GitHub Actions. The Streamlit dashboard reads the SQLite file and displays the information on the Streamlit Community Cloud.\nLessons Learned While the Power BI dashboard was useful, I faced challenges with refreshing it easily. Initially using an SQLite database, I couldn\u0026rsquo;t connect to the file on the repository, necessitating manual updates on my local machine. To overcome this, I explored alternatives like MySQL and PostgreSQL, enabling automated monthly scraping with GitHub Actions and updating the dashboard through the Power BI website.\nThe SQLite database is really helpful and depending on the size and scope of your project, you should look at it as a possible option to store your data.\nGoing forward I intend to maintain the project, making future patches to the scraping script if the website changes. And improving the code as my knowledge matures.\nExplore Further GitHub Repository: Access the project\u0026rsquo;s source files and codes on our GitHub Repository. Currently, everything is in Portuguese, but I plan to translate it in the future.\nStreamlit Dashboard: Explore the live Streamlit dashboard here to interact with the project\u0026rsquo;s data visualization directly.\nAcademic Article: If you\u0026rsquo;re interested in a detailed academic analysis of this project, check out our article here. Please note that the article is in Portuguese.\n","permalink":"http://localhost:1313/blogs/bts-nota-fiscal-goiana/","summary":"A behind-the-scenes decision on the decision-making process of the project Nota Fiscal Goiana.","title":"[BTS] Nota Fiscal Goiana"},{"content":"Have you ever had a purchase blocked by your credit card issuer without you taking any action? If this happened to you, your credit card issuer probably had detected that your purchase didn\u0026rsquo;t match your normal spending habit, maybe you were at a different location or spend a lot of money suddenly. Anyway, the credit card company was using anomaly detection techniques to prevent fraudulent activities.\nWhat is Anomaly Detection? Anomaly detection refers to the problem of finding patterns in data that do not conform to expected behavior. 1\nIn statistics there is a famous distribution that is called \u0026ldquo;The Normal Distribution\u0026rdquo;, this name is not in vain. A lot of random things are normally distributed, for instance, if you select 100 random people and measure their height, and made a distribution plot, you\u0026rsquo;d end up with something looking like a symmetrical bell curve.\nEven though abnormal results are possible, it is expected that most people or things will fall within a normal distribution. When something is divergent from this normality it is said to be an outlier or an anomaly.\nWhen dealing with something like credit card fraud you can create an expectation of a person expanding habits, so if your daily routine consists of waking up and driving to your workplace and buying a coffee, then lunch in New York. Your credit card company would find it weird if you suddenly made a physical purchase in Rio de Janeiro.\nIn simple terms, that is what anomaly detection is trying to do, understand how something works so it can identify if it starts acting weird.\nAnomaly Detection x Class Imbalance in Datasets If you are aware of what class imbalance is, you may be wondering \u0026ldquo;can\u0026rsquo;t I use anomaly detection to solve this class imbalance problem?\u0026rdquo; - sometimes you can, but it\u0026rsquo;s not appropriate.\nClass imbalance occurs when one class has significantly more samples than the other class(es). This can lead to biased models that favor the majority class and perform poorly on minority classes. While anomaly detection focuses on identifying rare or abnormal data points that deviate significantly from the normal patterns in the dataset.\nEven though it has some overlap on what is trying to do, unless you have a class imbalance problem that the minority class can be considered an abnormal behavior, you won\u0026rsquo;t be able to use anomaly detection.\nI talked about class imbalance before on a churn rate problem in a fictitious telecommunication company, if you are interested you can check it here.\nChandola, V., Banerjee, A., and Kumar, V. 2009. Anomaly detection: A survey. ACM Comput. Surv. 41, 3, Article 15 (July 2009), 58 pages. DOI = 10.1145/1541880.1541882\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/blogs/anomaly-detection/","summary":"How can your credit card issuer know if a purchase is made by you or a cloned version of your credit card?","title":"What is Anomaly Detection?"},{"content":"Demo Demo ","permalink":"http://localhost:1313/blogs/cara-menggunakan-tensorflow/","summary":"Cara Menggunakan Tensorflow","title":"Cara Menggunakan Tensorflow"},{"content":"Panduan Preprocessing Data yang Baik dalam Machine Learning Preprocessing adalah langkah penting dalam alur kerja data science dan machine learning. Data yang baik adalah kunci untuk model yang akurat dan dapat diandalkan. Artikel ini akan membahas pentingnya preprocessing data dan beberapa teknik yang umum digunakan untuk mempersiapkan data sebelum digunakan dalam pelatihan model machine learning.\nApa itu Preprocessing Data? Preprocessing data adalah serangkaian langkah yang dilakukan untuk membersihkan, memformat, dan mengubah data mentah agar siap digunakan dalam analisis atau pembuatan model machine learning. Langkah-langkah ini bertujuan untuk meningkatkan kualitas data dan memastikan model yang dibangun dapat memberikan hasil yang optimal.\nMengapa Preprocessing Data Itu Penting? Data yang tidak diproses dengan baik dapat mengarah pada model yang buruk atau bahkan tidak dapat digunakan sama sekali. Berikut beberapa alasan mengapa preprocessing sangat penting:\nMembersihkan Data: Menghapus noise, menangani nilai yang hilang, atau memperbaiki kesalahan data. Meningkatkan Akurasi Model: Dengan data yang bersih dan relevan, model machine learning dapat belajar dengan lebih baik. Menghindari Bias: Preprocessing membantu menghindari bias dalam data yang dapat memengaruhi prediksi model. Meningkatkan Kecepatan Pelatihan: Beberapa teknik preprocessing dapat mempercepat waktu pelatihan model. Langkah-langkah Preprocessing yang Umum 1. Menangani Nilai yang Hilang Data yang hilang adalah masalah umum dalam dataset dunia nyata. Ada beberapa cara untuk menangani nilai yang hilang:\nMenghapus Baris atau Kolom: Jika proporsi data yang hilang terlalu besar, bisa saja lebih baik untuk menghapusnya. Imputasi: Mengisi nilai yang hilang dengan statistik seperti rata-rata, median, atau modus. Interpolasi: Menggunakan nilai terdekat untuk mengisi data yang hilang, sering digunakan dalam data time series. 1 2 3 4 import pandas as pd # Imputasi menggunakan rata-rata df.fillna(df.mean(), inplace=True) 2. Normalisasi dan Standarisasi Proses ini penting ketika fitur dalam dataset memiliki skala yang sangat berbeda. Model machine learning seperti K-Nearest Neighbors (KNN) atau SVM sangat dipengaruhi oleh skala fitur.\nNormalisasi: Menyelaraskan data ke dalam rentang [0, 1] atau [-1, 1]. Standarisasi: Mengubah data sehingga memiliki rata-rata 0 dan deviasi standar 1. 1 2 3 4 5 6 7 8 9 from sklearn.preprocessing import StandardScaler, MinMaxScaler # Standarisasi scaler = StandardScaler() df_scaled = scaler.fit_transform(df) # Normalisasi min_max_scaler = MinMaxScaler() df_normalized = min_max_scaler.fit_transform(df) 3. Pengkodean Kategorikal Data kategorikal harus dikonversi ke format numerik agar dapat digunakan dalam model machine learning. Ada beberapa cara untuk melakukan ini:\nLabel Encoding: Mengonversi kategori menjadi angka. One-Hot Encoding: Membuat kolom biner untuk setiap kategori. 1 2 3 4 5 6 7 8 from sklearn.preprocessing import LabelEncoder, OneHotEncoder # Label Encoding encoder = LabelEncoder() df[\u0026#39;encoded_column\u0026#39;] = encoder.fit_transform(df[\u0026#39;category_column\u0026#39;]) # One-Hot Encoding df_encoded = pd.get_dummies(df[\u0026#39;category_column\u0026#39;]) 4. Deteksi dan Penanganan Outlier Outlier adalah nilai ekstrem yang bisa mempengaruhi model dan analisis. Ada beberapa cara untuk menangani outlier:\nMenghapus Outlier: Menghapus nilai yang terlalu jauh dari rata-rata. Transformasi: Menggunakan transformasi matematis seperti log atau kuadrat untuk meredam efek outlier. 1 2 3 4 5 6 7 # Deteksi outlier menggunakan IQR Q1 = df[\u0026#39;column\u0026#39;].quantile(0.25) Q3 = df[\u0026#39;column\u0026#39;].quantile(0.75) IQR = Q3 - Q1 # Menghapus outlier df_no_outliers = df[(df[\u0026#39;column\u0026#39;] \u0026gt;= (Q1 - 1.5 * IQR)) \u0026amp; (df[\u0026#39;column\u0026#39;] \u0026lt;= (Q3 + 1.5 * IQR))] 5. Pembagian Data untuk Pelatihan dan Pengujian Sebelum melatih model, penting untuk membagi data menjadi dua set: data pelatihan dan data pengujian. Pembagian yang umum adalah 80% untuk pelatihan dan 20% untuk pengujian.\n1 2 3 4 from sklearn.model_selection import train_test_split # Membagi data X_train, X_test, y_train, y_test = train_test_split(df.drop(\u0026#39;target\u0026#39;, axis=1), df[\u0026#39;target\u0026#39;], test_size=0.2, random_state=42) Kesimpulan Preprocessing data adalah langkah krusial dalam pipeline machine learning. Dengan menangani nilai yang hilang, normalisasi, pengkodean kategorikal, dan deteksi outlier, kita dapat memastikan bahwa model machine learning kita dilatih dengan data yang berkualitas tinggi. Meskipun preprocessing bisa memakan waktu, langkah ini sangat berpengaruh terhadap kinerja dan akurasi model yang dihasilkan. Pastikan untuk selalu memeriksa dataset Anda dan menerapkan teknik preprocessing yang tepat untuk mencapai hasil yang terbaik.\n","permalink":"http://localhost:1313/blogs/cara-preprocessing-data-dengan-baik/","summary":"Cara Preprocessing Data dengan Baik","title":"Cara Preprocessing Data dengan Baik"},{"content":"Panduan Preprocessing Data yang Baik dalam Machine Learning Preprocessing adalah langkah penting dalam alur kerja data science dan machine learning. Data yang baik adalah kunci untuk model yang akurat dan dapat diandalkan. Artikel ini akan membahas pentingnya preprocessing data dan beberapa teknik yang umum digunakan untuk mempersiapkan data sebelum digunakan dalam pelatihan model machine learning.\nApa itu Preprocessing Data? Preprocessing data adalah serangkaian langkah yang dilakukan untuk membersihkan, memformat, dan mengubah data mentah agar siap digunakan dalam analisis atau pembuatan model machine learning. Langkah-langkah ini bertujuan untuk meningkatkan kualitas data dan memastikan model yang dibangun dapat memberikan hasil yang optimal.\nMengapa Preprocessing Data Itu Penting? Data yang tidak diproses dengan baik dapat mengarah pada model yang buruk atau bahkan tidak dapat digunakan sama sekali. Berikut beberapa alasan mengapa preprocessing sangat penting:\nMembersihkan Data: Menghapus noise, menangani nilai yang hilang, atau memperbaiki kesalahan data. Meningkatkan Akurasi Model: Dengan data yang bersih dan relevan, model machine learning dapat belajar dengan lebih baik. Menghindari Bias: Preprocessing membantu menghindari bias dalam data yang dapat memengaruhi prediksi model. Meningkatkan Kecepatan Pelatihan: Beberapa teknik preprocessing dapat mempercepat waktu pelatihan model. Langkah-langkah Preprocessing yang Umum 1. Menangani Nilai yang Hilang Data yang hilang adalah masalah umum dalam dataset dunia nyata. Ada beberapa cara untuk menangani nilai yang hilang:\nMenghapus Baris atau Kolom: Jika proporsi data yang hilang terlalu besar, bisa saja lebih baik untuk menghapusnya. Imputasi: Mengisi nilai yang hilang dengan statistik seperti rata-rata, median, atau modus. Interpolasi: Menggunakan nilai terdekat untuk mengisi data yang hilang, sering digunakan dalam data time series. 1 2 3 4 import pandas as pd # Imputasi menggunakan rata-rata df.fillna(df.mean(), inplace=True) 2. Normalisasi dan Standarisasi Proses ini penting ketika fitur dalam dataset memiliki skala yang sangat berbeda. Model machine learning seperti K-Nearest Neighbors (KNN) atau SVM sangat dipengaruhi oleh skala fitur.\nNormalisasi: Menyelaraskan data ke dalam rentang [0, 1] atau [-1, 1]. Standarisasi: Mengubah data sehingga memiliki rata-rata 0 dan deviasi standar 1. 1 2 3 4 5 6 7 8 9 from sklearn.preprocessing import StandardScaler, MinMaxScaler # Standarisasi scaler = StandardScaler() df_scaled = scaler.fit_transform(df) # Normalisasi min_max_scaler = MinMaxScaler() df_normalized = min_max_scaler.fit_transform(df) 3. Pengkodean Kategorikal Data kategorikal harus dikonversi ke format numerik agar dapat digunakan dalam model machine learning. Ada beberapa cara untuk melakukan ini:\nLabel Encoding: Mengonversi kategori menjadi angka. One-Hot Encoding: Membuat kolom biner untuk setiap kategori. 1 2 3 4 5 6 7 8 from sklearn.preprocessing import LabelEncoder, OneHotEncoder # Label Encoding encoder = LabelEncoder() df[\u0026#39;encoded_column\u0026#39;] = encoder.fit_transform(df[\u0026#39;category_column\u0026#39;]) # One-Hot Encoding df_encoded = pd.get_dummies(df[\u0026#39;category_column\u0026#39;]) 4. Deteksi dan Penanganan Outlier Outlier adalah nilai ekstrem yang bisa mempengaruhi model dan analisis. Ada beberapa cara untuk menangani outlier:\nMenghapus Outlier: Menghapus nilai yang terlalu jauh dari rata-rata. Transformasi: Menggunakan transformasi matematis seperti log atau kuadrat untuk meredam efek outlier. 1 2 3 4 5 6 7 # Deteksi outlier menggunakan IQR Q1 = df[\u0026#39;column\u0026#39;].quantile(0.25) Q3 = df[\u0026#39;column\u0026#39;].quantile(0.75) IQR = Q3 - Q1 # Menghapus outlier df_no_outliers = df[(df[\u0026#39;column\u0026#39;] \u0026gt;= (Q1 - 1.5 * IQR)) \u0026amp; (df[\u0026#39;column\u0026#39;] \u0026lt;= (Q3 + 1.5 * IQR))] 5. Pembagian Data untuk Pelatihan dan Pengujian Sebelum melatih model, penting untuk membagi data menjadi dua set: data pelatihan dan data pengujian. Pembagian yang umum adalah 80% untuk pelatihan dan 20% untuk pengujian.\n1 2 3 4 from sklearn.model_selection import train_test_split # Membagi data X_train, X_test, y_train, y_test = train_test_split(df.drop(\u0026#39;target\u0026#39;, axis=1), df[\u0026#39;target\u0026#39;], test_size=0.2, random_state=42) Kesimpulan Preprocessing data adalah langkah krusial dalam pipeline machine learning. Dengan menangani nilai yang hilang, normalisasi, pengkodean kategorikal, dan deteksi outlier, kita dapat memastikan bahwa model machine learning kita dilatih dengan data yang berkualitas tinggi. Meskipun preprocessing bisa memakan waktu, langkah ini sangat berpengaruh terhadap kinerja dan akurasi model yang dihasilkan. Pastikan untuk selalu memeriksa dataset Anda dan menerapkan teknik preprocessing yang tepat untuk mencapai hasil yang terbaik.\n","permalink":"http://localhost:1313/blogs/cara-menggunakan-numpy/","summary":"Exploratory Data Analysis of Amazon\u0026rsquo;s top 50 bestselling books 2009 - 2019","title":"Cara Preprocessing Data dengan Baik"},{"content":"Introduction The Nota Fiscal Goiana is an economic program implemented by the state of Goiás, Brazil. This program is designed to boost the collection of the Tax on Circulation of Goods and Services (ICMS) by encouraging people to ask for receipts when they buy goods and services within the state. Additionally, the program includes monthly raffles that distribute cash prizes to participating citizens.\nAs I regularly checked the Nota Fiscal Goiana portal and realized the website\u0026rsquo;s simplicity, I decided to write a scraper to collect the raffle results. Initially, I aimed to simplify the process of checking if I had won, but as I delved deeper, I realized the potential to analyze the dataset more comprehensively.\nEarly Design and Idea Phase The original idea was to scrape raffle results, store them in a database, and give people the option to access the data directly from the database or just view it through a good-looking Power BI dashboard.\nThis led me to start writing the code for scraping the website and parsing the tables. However, since there were approximately 50 published results not available on the website, I had to resort to checking the Diário Oficial do Estado de Goiás, where the official government documents of Goiás are published in Brazil.\nChallenges Encountered Not all results were completely published on the Diário Oficial, leading to some results being unavailable.\nThe project doesn\u0026rsquo;t have a budget, so automation can\u0026rsquo;t require VM and online databases. Which Power BI requires both.\nSolutions The results that I could find published on the Diário Oficial were saved in the database along with results available on the website.\nInitially, I was using free options of online databases, and the backup of the old data was on an SQLite available on the project repository. However, I realized that when running the scraper I could save the data on the SQLite and commit the changes, when I did the information persisted on the SQLite file and I could just read it.\nTo help automate the dashboard I resorted to using Streamlit instead of Power BI. Because Streamlit allows me to read the SQLite file directly from the GitHub repository and every time someone opens the Streamlit dashboard it reads the current data, so I don\u0026rsquo;t need to set up refreshes.\nCurrent State of the Project Currently, the project extracts monthly the raffle results from the Nota Fiscal Goiana and the state ICMS collection and saves the results in the SQLite file, using GitHub Actions. The Streamlit dashboard reads the SQLite file and displays the information on the Streamlit Community Cloud.\nLessons Learned While the Power BI dashboard was useful, I faced challenges with refreshing it easily. Initially using an SQLite database, I couldn\u0026rsquo;t connect to the file on the repository, necessitating manual updates on my local machine. To overcome this, I explored alternatives like MySQL and PostgreSQL, enabling automated monthly scraping with GitHub Actions and updating the dashboard through the Power BI website.\nThe SQLite database is really helpful and depending on the size and scope of your project, you should look at it as a possible option to store your data.\nGoing forward I intend to maintain the project, making future patches to the scraping script if the website changes. And improving the code as my knowledge matures.\nExplore Further GitHub Repository: Access the project\u0026rsquo;s source files and codes on our GitHub Repository. Currently, everything is in Portuguese, but I plan to translate it in the future.\nStreamlit Dashboard: Explore the live Streamlit dashboard here to interact with the project\u0026rsquo;s data visualization directly.\nAcademic Article: If you\u0026rsquo;re interested in a detailed academic analysis of this project, check out our article here. Please note that the article is in Portuguese.\n","permalink":"http://localhost:1313/blogs/bts-nota-fiscal-goiana/","summary":"A behind-the-scenes decision on the decision-making process of the project Nota Fiscal Goiana.","title":"[BTS] Nota Fiscal Goiana"},{"content":"Have you ever had a purchase blocked by your credit card issuer without you taking any action? If this happened to you, your credit card issuer probably had detected that your purchase didn\u0026rsquo;t match your normal spending habit, maybe you were at a different location or spend a lot of money suddenly. Anyway, the credit card company was using anomaly detection techniques to prevent fraudulent activities.\nWhat is Anomaly Detection? Anomaly detection refers to the problem of finding patterns in data that do not conform to expected behavior. 1\nIn statistics there is a famous distribution that is called \u0026ldquo;The Normal Distribution\u0026rdquo;, this name is not in vain. A lot of random things are normally distributed, for instance, if you select 100 random people and measure their height, and made a distribution plot, you\u0026rsquo;d end up with something looking like a symmetrical bell curve.\nEven though abnormal results are possible, it is expected that most people or things will fall within a normal distribution. When something is divergent from this normality it is said to be an outlier or an anomaly.\nWhen dealing with something like credit card fraud you can create an expectation of a person expanding habits, so if your daily routine consists of waking up and driving to your workplace and buying a coffee, then lunch in New York. Your credit card company would find it weird if you suddenly made a physical purchase in Rio de Janeiro.\nIn simple terms, that is what anomaly detection is trying to do, understand how something works so it can identify if it starts acting weird.\nAnomaly Detection x Class Imbalance in Datasets If you are aware of what class imbalance is, you may be wondering \u0026ldquo;can\u0026rsquo;t I use anomaly detection to solve this class imbalance problem?\u0026rdquo; - sometimes you can, but it\u0026rsquo;s not appropriate.\nClass imbalance occurs when one class has significantly more samples than the other class(es). This can lead to biased models that favor the majority class and perform poorly on minority classes. While anomaly detection focuses on identifying rare or abnormal data points that deviate significantly from the normal patterns in the dataset.\nEven though it has some overlap on what is trying to do, unless you have a class imbalance problem that the minority class can be considered an abnormal behavior, you won\u0026rsquo;t be able to use anomaly detection.\nI talked about class imbalance before on a churn rate problem in a fictitious telecommunication company, if you are interested you can check it here.\nChandola, V., Banerjee, A., and Kumar, V. 2009. Anomaly detection: A survey. ACM Comput. Surv. 41, 3, Article 15 (July 2009), 58 pages. DOI = 10.1145/1541880.1541882\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/blogs/anomaly-detection/","summary":"How can your credit card issuer know if a purchase is made by you or a cloned version of your credit card?","title":"What is Anomaly Detection?"},{"content":"Demo Demo ","permalink":"http://localhost:1313/blogs/cara-menggunakan-tensorflow/","summary":"Cara Menggunakan Tensorflow","title":"Cara Menggunakan Tensorflow"},{"content":"Panduan Preprocessing Data yang Baik dalam Machine Learning Preprocessing adalah langkah penting dalam alur kerja data science dan machine learning. Data yang baik adalah kunci untuk model yang akurat dan dapat diandalkan. Artikel ini akan membahas pentingnya preprocessing data dan beberapa teknik yang umum digunakan untuk mempersiapkan data sebelum digunakan dalam pelatihan model machine learning.\nApa itu Preprocessing Data? Preprocessing data adalah serangkaian langkah yang dilakukan untuk membersihkan, memformat, dan mengubah data mentah agar siap digunakan dalam analisis atau pembuatan model machine learning. Langkah-langkah ini bertujuan untuk meningkatkan kualitas data dan memastikan model yang dibangun dapat memberikan hasil yang optimal.\nMengapa Preprocessing Data Itu Penting? Data yang tidak diproses dengan baik dapat mengarah pada model yang buruk atau bahkan tidak dapat digunakan sama sekali. Berikut beberapa alasan mengapa preprocessing sangat penting:\nMembersihkan Data: Menghapus noise, menangani nilai yang hilang, atau memperbaiki kesalahan data. Meningkatkan Akurasi Model: Dengan data yang bersih dan relevan, model machine learning dapat belajar dengan lebih baik. Menghindari Bias: Preprocessing membantu menghindari bias dalam data yang dapat memengaruhi prediksi model. Meningkatkan Kecepatan Pelatihan: Beberapa teknik preprocessing dapat mempercepat waktu pelatihan model. Langkah-langkah Preprocessing yang Umum 1. Menangani Nilai yang Hilang Data yang hilang adalah masalah umum dalam dataset dunia nyata. Ada beberapa cara untuk menangani nilai yang hilang:\nMenghapus Baris atau Kolom: Jika proporsi data yang hilang terlalu besar, bisa saja lebih baik untuk menghapusnya. Imputasi: Mengisi nilai yang hilang dengan statistik seperti rata-rata, median, atau modus. Interpolasi: Menggunakan nilai terdekat untuk mengisi data yang hilang, sering digunakan dalam data time series. 1 2 3 4 import pandas as pd # Imputasi menggunakan rata-rata df.fillna(df.mean(), inplace=True) 2. Normalisasi dan Standarisasi Proses ini penting ketika fitur dalam dataset memiliki skala yang sangat berbeda. Model machine learning seperti K-Nearest Neighbors (KNN) atau SVM sangat dipengaruhi oleh skala fitur.\nNormalisasi: Menyelaraskan data ke dalam rentang [0, 1] atau [-1, 1]. Standarisasi: Mengubah data sehingga memiliki rata-rata 0 dan deviasi standar 1. 1 2 3 4 5 6 7 8 9 from sklearn.preprocessing import StandardScaler, MinMaxScaler # Standarisasi scaler = StandardScaler() df_scaled = scaler.fit_transform(df) # Normalisasi min_max_scaler = MinMaxScaler() df_normalized = min_max_scaler.fit_transform(df) 3. Pengkodean Kategorikal Data kategorikal harus dikonversi ke format numerik agar dapat digunakan dalam model machine learning. Ada beberapa cara untuk melakukan ini:\nLabel Encoding: Mengonversi kategori menjadi angka. One-Hot Encoding: Membuat kolom biner untuk setiap kategori. 1 2 3 4 5 6 7 8 from sklearn.preprocessing import LabelEncoder, OneHotEncoder # Label Encoding encoder = LabelEncoder() df[\u0026#39;encoded_column\u0026#39;] = encoder.fit_transform(df[\u0026#39;category_column\u0026#39;]) # One-Hot Encoding df_encoded = pd.get_dummies(df[\u0026#39;category_column\u0026#39;]) 4. Deteksi dan Penanganan Outlier Outlier adalah nilai ekstrem yang bisa mempengaruhi model dan analisis. Ada beberapa cara untuk menangani outlier:\nMenghapus Outlier: Menghapus nilai yang terlalu jauh dari rata-rata. Transformasi: Menggunakan transformasi matematis seperti log atau kuadrat untuk meredam efek outlier. 1 2 3 4 5 6 7 # Deteksi outlier menggunakan IQR Q1 = df[\u0026#39;column\u0026#39;].quantile(0.25) Q3 = df[\u0026#39;column\u0026#39;].quantile(0.75) IQR = Q3 - Q1 # Menghapus outlier df_no_outliers = df[(df[\u0026#39;column\u0026#39;] \u0026gt;= (Q1 - 1.5 * IQR)) \u0026amp; (df[\u0026#39;column\u0026#39;] \u0026lt;= (Q3 + 1.5 * IQR))] 5. Pembagian Data untuk Pelatihan dan Pengujian Sebelum melatih model, penting untuk membagi data menjadi dua set: data pelatihan dan data pengujian. Pembagian yang umum adalah 80% untuk pelatihan dan 20% untuk pengujian.\n1 2 3 4 from sklearn.model_selection import train_test_split # Membagi data X_train, X_test, y_train, y_test = train_test_split(df.drop(\u0026#39;target\u0026#39;, axis=1), df[\u0026#39;target\u0026#39;], test_size=0.2, random_state=42) Kesimpulan Preprocessing data adalah langkah krusial dalam pipeline machine learning. Dengan menangani nilai yang hilang, normalisasi, pengkodean kategorikal, dan deteksi outlier, kita dapat memastikan bahwa model machine learning kita dilatih dengan data yang berkualitas tinggi. Meskipun preprocessing bisa memakan waktu, langkah ini sangat berpengaruh terhadap kinerja dan akurasi model yang dihasilkan. Pastikan untuk selalu memeriksa dataset Anda dan menerapkan teknik preprocessing yang tepat untuk mencapai hasil yang terbaik.\n","permalink":"http://localhost:1313/blogs/cara-preprocessing-data-dengan-baik/","summary":"Cara Preprocessing Data dengan Baik","title":"Cara Preprocessing Data dengan Baik"},{"content":"Panduan Preprocessing Data yang Baik dalam Machine Learning Preprocessing adalah langkah penting dalam alur kerja data science dan machine learning. Data yang baik adalah kunci untuk model yang akurat dan dapat diandalkan. Artikel ini akan membahas pentingnya preprocessing data dan beberapa teknik yang umum digunakan untuk mempersiapkan data sebelum digunakan dalam pelatihan model machine learning.\nApa itu Preprocessing Data? Preprocessing data adalah serangkaian langkah yang dilakukan untuk membersihkan, memformat, dan mengubah data mentah agar siap digunakan dalam analisis atau pembuatan model machine learning. Langkah-langkah ini bertujuan untuk meningkatkan kualitas data dan memastikan model yang dibangun dapat memberikan hasil yang optimal.\nMengapa Preprocessing Data Itu Penting? Data yang tidak diproses dengan baik dapat mengarah pada model yang buruk atau bahkan tidak dapat digunakan sama sekali. Berikut beberapa alasan mengapa preprocessing sangat penting:\nMembersihkan Data: Menghapus noise, menangani nilai yang hilang, atau memperbaiki kesalahan data. Meningkatkan Akurasi Model: Dengan data yang bersih dan relevan, model machine learning dapat belajar dengan lebih baik. Menghindari Bias: Preprocessing membantu menghindari bias dalam data yang dapat memengaruhi prediksi model. Meningkatkan Kecepatan Pelatihan: Beberapa teknik preprocessing dapat mempercepat waktu pelatihan model. Langkah-langkah Preprocessing yang Umum 1. Menangani Nilai yang Hilang Data yang hilang adalah masalah umum dalam dataset dunia nyata. Ada beberapa cara untuk menangani nilai yang hilang:\nMenghapus Baris atau Kolom: Jika proporsi data yang hilang terlalu besar, bisa saja lebih baik untuk menghapusnya. Imputasi: Mengisi nilai yang hilang dengan statistik seperti rata-rata, median, atau modus. Interpolasi: Menggunakan nilai terdekat untuk mengisi data yang hilang, sering digunakan dalam data time series. 1 2 3 4 import pandas as pd # Imputasi menggunakan rata-rata df.fillna(df.mean(), inplace=True) 2. Normalisasi dan Standarisasi Proses ini penting ketika fitur dalam dataset memiliki skala yang sangat berbeda. Model machine learning seperti K-Nearest Neighbors (KNN) atau SVM sangat dipengaruhi oleh skala fitur.\nNormalisasi: Menyelaraskan data ke dalam rentang [0, 1] atau [-1, 1]. Standarisasi: Mengubah data sehingga memiliki rata-rata 0 dan deviasi standar 1. 1 2 3 4 5 6 7 8 9 from sklearn.preprocessing import StandardScaler, MinMaxScaler # Standarisasi scaler = StandardScaler() df_scaled = scaler.fit_transform(df) # Normalisasi min_max_scaler = MinMaxScaler() df_normalized = min_max_scaler.fit_transform(df) 3. Pengkodean Kategorikal Data kategorikal harus dikonversi ke format numerik agar dapat digunakan dalam model machine learning. Ada beberapa cara untuk melakukan ini:\nLabel Encoding: Mengonversi kategori menjadi angka. One-Hot Encoding: Membuat kolom biner untuk setiap kategori. 1 2 3 4 5 6 7 8 from sklearn.preprocessing import LabelEncoder, OneHotEncoder # Label Encoding encoder = LabelEncoder() df[\u0026#39;encoded_column\u0026#39;] = encoder.fit_transform(df[\u0026#39;category_column\u0026#39;]) # One-Hot Encoding df_encoded = pd.get_dummies(df[\u0026#39;category_column\u0026#39;]) 4. Deteksi dan Penanganan Outlier Outlier adalah nilai ekstrem yang bisa mempengaruhi model dan analisis. Ada beberapa cara untuk menangani outlier:\nMenghapus Outlier: Menghapus nilai yang terlalu jauh dari rata-rata. Transformasi: Menggunakan transformasi matematis seperti log atau kuadrat untuk meredam efek outlier. 1 2 3 4 5 6 7 # Deteksi outlier menggunakan IQR Q1 = df[\u0026#39;column\u0026#39;].quantile(0.25) Q3 = df[\u0026#39;column\u0026#39;].quantile(0.75) IQR = Q3 - Q1 # Menghapus outlier df_no_outliers = df[(df[\u0026#39;column\u0026#39;] \u0026gt;= (Q1 - 1.5 * IQR)) \u0026amp; (df[\u0026#39;column\u0026#39;] \u0026lt;= (Q3 + 1.5 * IQR))] 5. Pembagian Data untuk Pelatihan dan Pengujian Sebelum melatih model, penting untuk membagi data menjadi dua set: data pelatihan dan data pengujian. Pembagian yang umum adalah 80% untuk pelatihan dan 20% untuk pengujian.\n1 2 3 4 from sklearn.model_selection import train_test_split # Membagi data X_train, X_test, y_train, y_test = train_test_split(df.drop(\u0026#39;target\u0026#39;, axis=1), df[\u0026#39;target\u0026#39;], test_size=0.2, random_state=42) Kesimpulan Preprocessing data adalah langkah krusial dalam pipeline machine learning. Dengan menangani nilai yang hilang, normalisasi, pengkodean kategorikal, dan deteksi outlier, kita dapat memastikan bahwa model machine learning kita dilatih dengan data yang berkualitas tinggi. Meskipun preprocessing bisa memakan waktu, langkah ini sangat berpengaruh terhadap kinerja dan akurasi model yang dihasilkan. Pastikan untuk selalu memeriksa dataset Anda dan menerapkan teknik preprocessing yang tepat untuk mencapai hasil yang terbaik.\n","permalink":"http://localhost:1313/blogs/cara-preprocessing-data-dengan-baikk/","summary":"Exploratory Data Analysis of Amazon\u0026rsquo;s top 50 bestselling books 2009 - 2019","title":"Cara Preprocessing Data dengan Baik"},{"content":"Introduction The Nota Fiscal Goiana is an economic program implemented by the state of Goiás, Brazil. This program is designed to boost the collection of the Tax on Circulation of Goods and Services (ICMS) by encouraging people to ask for receipts when they buy goods and services within the state. Additionally, the program includes monthly raffles that distribute cash prizes to participating citizens.\nAs I regularly checked the Nota Fiscal Goiana portal and realized the website\u0026rsquo;s simplicity, I decided to write a scraper to collect the raffle results. Initially, I aimed to simplify the process of checking if I had won, but as I delved deeper, I realized the potential to analyze the dataset more comprehensively.\nEarly Design and Idea Phase The original idea was to scrape raffle results, store them in a database, and give people the option to access the data directly from the database or just view it through a good-looking Power BI dashboard.\nThis led me to start writing the code for scraping the website and parsing the tables. However, since there were approximately 50 published results not available on the website, I had to resort to checking the Diário Oficial do Estado de Goiás, where the official government documents of Goiás are published in Brazil.\nChallenges Encountered Not all results were completely published on the Diário Oficial, leading to some results being unavailable.\nThe project doesn\u0026rsquo;t have a budget, so automation can\u0026rsquo;t require VM and online databases. Which Power BI requires both.\nSolutions The results that I could find published on the Diário Oficial were saved in the database along with results available on the website.\nInitially, I was using free options of online databases, and the backup of the old data was on an SQLite available on the project repository. However, I realized that when running the scraper I could save the data on the SQLite and commit the changes, when I did the information persisted on the SQLite file and I could just read it.\nTo help automate the dashboard I resorted to using Streamlit instead of Power BI. Because Streamlit allows me to read the SQLite file directly from the GitHub repository and every time someone opens the Streamlit dashboard it reads the current data, so I don\u0026rsquo;t need to set up refreshes.\nCurrent State of the Project Currently, the project extracts monthly the raffle results from the Nota Fiscal Goiana and the state ICMS collection and saves the results in the SQLite file, using GitHub Actions. The Streamlit dashboard reads the SQLite file and displays the information on the Streamlit Community Cloud.\nLessons Learned While the Power BI dashboard was useful, I faced challenges with refreshing it easily. Initially using an SQLite database, I couldn\u0026rsquo;t connect to the file on the repository, necessitating manual updates on my local machine. To overcome this, I explored alternatives like MySQL and PostgreSQL, enabling automated monthly scraping with GitHub Actions and updating the dashboard through the Power BI website.\nThe SQLite database is really helpful and depending on the size and scope of your project, you should look at it as a possible option to store your data.\nGoing forward I intend to maintain the project, making future patches to the scraping script if the website changes. And improving the code as my knowledge matures.\nExplore Further GitHub Repository: Access the project\u0026rsquo;s source files and codes on our GitHub Repository. Currently, everything is in Portuguese, but I plan to translate it in the future.\nStreamlit Dashboard: Explore the live Streamlit dashboard here to interact with the project\u0026rsquo;s data visualization directly.\nAcademic Article: If you\u0026rsquo;re interested in a detailed academic analysis of this project, check out our article here. Please note that the article is in Portuguese.\n","permalink":"http://localhost:1313/blogs/bts-nota-fiscal-goiana/","summary":"A behind-the-scenes decision on the decision-making process of the project Nota Fiscal Goiana.","title":"[BTS] Nota Fiscal Goiana"},{"content":"Have you ever had a purchase blocked by your credit card issuer without you taking any action? If this happened to you, your credit card issuer probably had detected that your purchase didn\u0026rsquo;t match your normal spending habit, maybe you were at a different location or spend a lot of money suddenly. Anyway, the credit card company was using anomaly detection techniques to prevent fraudulent activities.\nWhat is Anomaly Detection? Anomaly detection refers to the problem of finding patterns in data that do not conform to expected behavior. 1\nIn statistics there is a famous distribution that is called \u0026ldquo;The Normal Distribution\u0026rdquo;, this name is not in vain. A lot of random things are normally distributed, for instance, if you select 100 random people and measure their height, and made a distribution plot, you\u0026rsquo;d end up with something looking like a symmetrical bell curve.\nEven though abnormal results are possible, it is expected that most people or things will fall within a normal distribution. When something is divergent from this normality it is said to be an outlier or an anomaly.\nWhen dealing with something like credit card fraud you can create an expectation of a person expanding habits, so if your daily routine consists of waking up and driving to your workplace and buying a coffee, then lunch in New York. Your credit card company would find it weird if you suddenly made a physical purchase in Rio de Janeiro.\nIn simple terms, that is what anomaly detection is trying to do, understand how something works so it can identify if it starts acting weird.\nAnomaly Detection x Class Imbalance in Datasets If you are aware of what class imbalance is, you may be wondering \u0026ldquo;can\u0026rsquo;t I use anomaly detection to solve this class imbalance problem?\u0026rdquo; - sometimes you can, but it\u0026rsquo;s not appropriate.\nClass imbalance occurs when one class has significantly more samples than the other class(es). This can lead to biased models that favor the majority class and perform poorly on minority classes. While anomaly detection focuses on identifying rare or abnormal data points that deviate significantly from the normal patterns in the dataset.\nEven though it has some overlap on what is trying to do, unless you have a class imbalance problem that the minority class can be considered an abnormal behavior, you won\u0026rsquo;t be able to use anomaly detection.\nI talked about class imbalance before on a churn rate problem in a fictitious telecommunication company, if you are interested you can check it here.\nChandola, V., Banerjee, A., and Kumar, V. 2009. Anomaly detection: A survey. ACM Comput. Surv. 41, 3, Article 15 (July 2009), 58 pages. DOI = 10.1145/1541880.1541882\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/blogs/anomaly-detection/","summary":"How can your credit card issuer know if a purchase is made by you or a cloned version of your credit card?","title":"What is Anomaly Detection?"},{"content":"Demo Demo ","permalink":"http://localhost:1313/blogs/cara-menggunakan-tensorflow/","summary":"Cara Menggunakan Tensorflow","title":"Cara Menggunakan Tensorflow"},{"content":"Panduan Preprocessing Data yang Baik dalam Machine Learning Preprocessing adalah langkah penting dalam alur kerja data science dan machine learning. Data yang baik adalah kunci untuk model yang akurat dan dapat diandalkan. Artikel ini akan membahas pentingnya preprocessing data dan beberapa teknik yang umum digunakan untuk mempersiapkan data sebelum digunakan dalam pelatihan model machine learning.\nApa itu Preprocessing Data? Preprocessing data adalah serangkaian langkah yang dilakukan untuk membersihkan, memformat, dan mengubah data mentah agar siap digunakan dalam analisis atau pembuatan model machine learning. Langkah-langkah ini bertujuan untuk meningkatkan kualitas data dan memastikan model yang dibangun dapat memberikan hasil yang optimal.\nMengapa Preprocessing Data Itu Penting? Data yang tidak diproses dengan baik dapat mengarah pada model yang buruk atau bahkan tidak dapat digunakan sama sekali. Berikut beberapa alasan mengapa preprocessing sangat penting:\nMembersihkan Data: Menghapus noise, menangani nilai yang hilang, atau memperbaiki kesalahan data. Meningkatkan Akurasi Model: Dengan data yang bersih dan relevan, model machine learning dapat belajar dengan lebih baik. Menghindari Bias: Preprocessing membantu menghindari bias dalam data yang dapat memengaruhi prediksi model. Meningkatkan Kecepatan Pelatihan: Beberapa teknik preprocessing dapat mempercepat waktu pelatihan model. Langkah-langkah Preprocessing yang Umum 1. Menangani Nilai yang Hilang Data yang hilang adalah masalah umum dalam dataset dunia nyata. Ada beberapa cara untuk menangani nilai yang hilang:\nMenghapus Baris atau Kolom: Jika proporsi data yang hilang terlalu besar, bisa saja lebih baik untuk menghapusnya. Imputasi: Mengisi nilai yang hilang dengan statistik seperti rata-rata, median, atau modus. Interpolasi: Menggunakan nilai terdekat untuk mengisi data yang hilang, sering digunakan dalam data time series. 1 2 3 4 import pandas as pd # Imputasi menggunakan rata-rata df.fillna(df.mean(), inplace=True) 2. Normalisasi dan Standarisasi Proses ini penting ketika fitur dalam dataset memiliki skala yang sangat berbeda. Model machine learning seperti K-Nearest Neighbors (KNN) atau SVM sangat dipengaruhi oleh skala fitur.\nNormalisasi: Menyelaraskan data ke dalam rentang [0, 1] atau [-1, 1]. Standarisasi: Mengubah data sehingga memiliki rata-rata 0 dan deviasi standar 1. 1 2 3 4 5 6 7 8 9 from sklearn.preprocessing import StandardScaler, MinMaxScaler # Standarisasi scaler = StandardScaler() df_scaled = scaler.fit_transform(df) # Normalisasi min_max_scaler = MinMaxScaler() df_normalized = min_max_scaler.fit_transform(df) 3. Pengkodean Kategorikal Data kategorikal harus dikonversi ke format numerik agar dapat digunakan dalam model machine learning. Ada beberapa cara untuk melakukan ini:\nLabel Encoding: Mengonversi kategori menjadi angka. One-Hot Encoding: Membuat kolom biner untuk setiap kategori. 1 2 3 4 5 6 7 8 from sklearn.preprocessing import LabelEncoder, OneHotEncoder # Label Encoding encoder = LabelEncoder() df[\u0026#39;encoded_column\u0026#39;] = encoder.fit_transform(df[\u0026#39;category_column\u0026#39;]) # One-Hot Encoding df_encoded = pd.get_dummies(df[\u0026#39;category_column\u0026#39;]) 4. Deteksi dan Penanganan Outlier Outlier adalah nilai ekstrem yang bisa mempengaruhi model dan analisis. Ada beberapa cara untuk menangani outlier:\nMenghapus Outlier: Menghapus nilai yang terlalu jauh dari rata-rata. Transformasi: Menggunakan transformasi matematis seperti log atau kuadrat untuk meredam efek outlier. 1 2 3 4 5 6 7 # Deteksi outlier menggunakan IQR Q1 = df[\u0026#39;column\u0026#39;].quantile(0.25) Q3 = df[\u0026#39;column\u0026#39;].quantile(0.75) IQR = Q3 - Q1 # Menghapus outlier df_no_outliers = df[(df[\u0026#39;column\u0026#39;] \u0026gt;= (Q1 - 1.5 * IQR)) \u0026amp; (df[\u0026#39;column\u0026#39;] \u0026lt;= (Q3 + 1.5 * IQR))] 5. Pembagian Data untuk Pelatihan dan Pengujian Sebelum melatih model, penting untuk membagi data menjadi dua set: data pelatihan dan data pengujian. Pembagian yang umum adalah 80% untuk pelatihan dan 20% untuk pengujian.\n1 2 3 4 from sklearn.model_selection import train_test_split # Membagi data X_train, X_test, y_train, y_test = train_test_split(df.drop(\u0026#39;target\u0026#39;, axis=1), df[\u0026#39;target\u0026#39;], test_size=0.2, random_state=42) Kesimpulan Preprocessing data adalah langkah krusial dalam pipeline machine learning. Dengan menangani nilai yang hilang, normalisasi, pengkodean kategorikal, dan deteksi outlier, kita dapat memastikan bahwa model machine learning kita dilatih dengan data yang berkualitas tinggi. Meskipun preprocessing bisa memakan waktu, langkah ini sangat berpengaruh terhadap kinerja dan akurasi model yang dihasilkan. Pastikan untuk selalu memeriksa dataset Anda dan menerapkan teknik preprocessing yang tepat untuk mencapai hasil yang terbaik.\n","permalink":"http://localhost:1313/blogs/cara-preprocessing-data-dengan-baikk/","summary":"Exploratory Data Analysis of Amazon\u0026rsquo;s top 50 bestselling books 2009 - 2019","title":"Cara Preprocessing Data dengan Baik"},{"content":"Introduction The Nota Fiscal Goiana is an economic program implemented by the state of Goiás, Brazil. This program is designed to boost the collection of the Tax on Circulation of Goods and Services (ICMS) by encouraging people to ask for receipts when they buy goods and services within the state. Additionally, the program includes monthly raffles that distribute cash prizes to participating citizens.\nAs I regularly checked the Nota Fiscal Goiana portal and realized the website\u0026rsquo;s simplicity, I decided to write a scraper to collect the raffle results. Initially, I aimed to simplify the process of checking if I had won, but as I delved deeper, I realized the potential to analyze the dataset more comprehensively.\nEarly Design and Idea Phase The original idea was to scrape raffle results, store them in a database, and give people the option to access the data directly from the database or just view it through a good-looking Power BI dashboard.\nThis led me to start writing the code for scraping the website and parsing the tables. However, since there were approximately 50 published results not available on the website, I had to resort to checking the Diário Oficial do Estado de Goiás, where the official government documents of Goiás are published in Brazil.\nChallenges Encountered Not all results were completely published on the Diário Oficial, leading to some results being unavailable.\nThe project doesn\u0026rsquo;t have a budget, so automation can\u0026rsquo;t require VM and online databases. Which Power BI requires both.\nSolutions The results that I could find published on the Diário Oficial were saved in the database along with results available on the website.\nInitially, I was using free options of online databases, and the backup of the old data was on an SQLite available on the project repository. However, I realized that when running the scraper I could save the data on the SQLite and commit the changes, when I did the information persisted on the SQLite file and I could just read it.\nTo help automate the dashboard I resorted to using Streamlit instead of Power BI. Because Streamlit allows me to read the SQLite file directly from the GitHub repository and every time someone opens the Streamlit dashboard it reads the current data, so I don\u0026rsquo;t need to set up refreshes.\nCurrent State of the Project Currently, the project extracts monthly the raffle results from the Nota Fiscal Goiana and the state ICMS collection and saves the results in the SQLite file, using GitHub Actions. The Streamlit dashboard reads the SQLite file and displays the information on the Streamlit Community Cloud.\nLessons Learned While the Power BI dashboard was useful, I faced challenges with refreshing it easily. Initially using an SQLite database, I couldn\u0026rsquo;t connect to the file on the repository, necessitating manual updates on my local machine. To overcome this, I explored alternatives like MySQL and PostgreSQL, enabling automated monthly scraping with GitHub Actions and updating the dashboard through the Power BI website.\nThe SQLite database is really helpful and depending on the size and scope of your project, you should look at it as a possible option to store your data.\nGoing forward I intend to maintain the project, making future patches to the scraping script if the website changes. And improving the code as my knowledge matures.\nExplore Further GitHub Repository: Access the project\u0026rsquo;s source files and codes on our GitHub Repository. Currently, everything is in Portuguese, but I plan to translate it in the future.\nStreamlit Dashboard: Explore the live Streamlit dashboard here to interact with the project\u0026rsquo;s data visualization directly.\nAcademic Article: If you\u0026rsquo;re interested in a detailed academic analysis of this project, check out our article here. Please note that the article is in Portuguese.\n","permalink":"http://localhost:1313/blogs/bts-nota-fiscal-goiana/","summary":"A behind-the-scenes decision on the decision-making process of the project Nota Fiscal Goiana.","title":"[BTS] Nota Fiscal Goiana"},{"content":"Have you ever had a purchase blocked by your credit card issuer without you taking any action? If this happened to you, your credit card issuer probably had detected that your purchase didn\u0026rsquo;t match your normal spending habit, maybe you were at a different location or spend a lot of money suddenly. Anyway, the credit card company was using anomaly detection techniques to prevent fraudulent activities.\nWhat is Anomaly Detection? Anomaly detection refers to the problem of finding patterns in data that do not conform to expected behavior. 1\nIn statistics there is a famous distribution that is called \u0026ldquo;The Normal Distribution\u0026rdquo;, this name is not in vain. A lot of random things are normally distributed, for instance, if you select 100 random people and measure their height, and made a distribution plot, you\u0026rsquo;d end up with something looking like a symmetrical bell curve.\nEven though abnormal results are possible, it is expected that most people or things will fall within a normal distribution. When something is divergent from this normality it is said to be an outlier or an anomaly.\nWhen dealing with something like credit card fraud you can create an expectation of a person expanding habits, so if your daily routine consists of waking up and driving to your workplace and buying a coffee, then lunch in New York. Your credit card company would find it weird if you suddenly made a physical purchase in Rio de Janeiro.\nIn simple terms, that is what anomaly detection is trying to do, understand how something works so it can identify if it starts acting weird.\nAnomaly Detection x Class Imbalance in Datasets If you are aware of what class imbalance is, you may be wondering \u0026ldquo;can\u0026rsquo;t I use anomaly detection to solve this class imbalance problem?\u0026rdquo; - sometimes you can, but it\u0026rsquo;s not appropriate.\nClass imbalance occurs when one class has significantly more samples than the other class(es). This can lead to biased models that favor the majority class and perform poorly on minority classes. While anomaly detection focuses on identifying rare or abnormal data points that deviate significantly from the normal patterns in the dataset.\nEven though it has some overlap on what is trying to do, unless you have a class imbalance problem that the minority class can be considered an abnormal behavior, you won\u0026rsquo;t be able to use anomaly detection.\nI talked about class imbalance before on a churn rate problem in a fictitious telecommunication company, if you are interested you can check it here.\nChandola, V., Banerjee, A., and Kumar, V. 2009. Anomaly detection: A survey. ACM Comput. Surv. 41, 3, Article 15 (July 2009), 58 pages. DOI = 10.1145/1541880.1541882\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/blogs/anomaly-detection/","summary":"How can your credit card issuer know if a purchase is made by you or a cloned version of your credit card?","title":"What is Anomaly Detection?"},{"content":"Demo Demo ","permalink":"http://localhost:1313/blogs/cara-menggunakan-tensorflow/","summary":"Cara Menggunakan Tensorflow","title":"Cara Menggunakan Tensorflow"},{"content":"Panduan Preprocessing Data yang Baik dalam Machine Learning Preprocessing adalah langkah penting dalam alur kerja data science dan machine learning. Data yang baik adalah kunci untuk model yang akurat dan dapat diandalkan. Artikel ini akan membahas pentingnya preprocessing data dan beberapa teknik yang umum digunakan untuk mempersiapkan data sebelum digunakan dalam pelatihan model machine learning.\nApa itu Preprocessing Data? Preprocessing data adalah serangkaian langkah yang dilakukan untuk membersihkan, memformat, dan mengubah data mentah agar siap digunakan dalam analisis atau pembuatan model machine learning. Langkah-langkah ini bertujuan untuk meningkatkan kualitas data dan memastikan model yang dibangun dapat memberikan hasil yang optimal.\nMengapa Preprocessing Data Itu Penting? Data yang tidak diproses dengan baik dapat mengarah pada model yang buruk atau bahkan tidak dapat digunakan sama sekali. Berikut beberapa alasan mengapa preprocessing sangat penting:\nMembersihkan Data: Menghapus noise, menangani nilai yang hilang, atau memperbaiki kesalahan data. Meningkatkan Akurasi Model: Dengan data yang bersih dan relevan, model machine learning dapat belajar dengan lebih baik. Menghindari Bias: Preprocessing membantu menghindari bias dalam data yang dapat memengaruhi prediksi model. Meningkatkan Kecepatan Pelatihan: Beberapa teknik preprocessing dapat mempercepat waktu pelatihan model. Langkah-langkah Preprocessing yang Umum 1. Menangani Nilai yang Hilang Data yang hilang adalah masalah umum dalam dataset dunia nyata. Ada beberapa cara untuk menangani nilai yang hilang:\nMenghapus Baris atau Kolom: Jika proporsi data yang hilang terlalu besar, bisa saja lebih baik untuk menghapusnya. Imputasi: Mengisi nilai yang hilang dengan statistik seperti rata-rata, median, atau modus. Interpolasi: Menggunakan nilai terdekat untuk mengisi data yang hilang, sering digunakan dalam data time series. 1 2 3 4 import pandas as pd # Imputasi menggunakan rata-rata df.fillna(df.mean(), inplace=True) 2. Normalisasi dan Standarisasi Proses ini penting ketika fitur dalam dataset memiliki skala yang sangat berbeda. Model machine learning seperti K-Nearest Neighbors (KNN) atau SVM sangat dipengaruhi oleh skala fitur.\nNormalisasi: Menyelaraskan data ke dalam rentang [0, 1] atau [-1, 1]. Standarisasi: Mengubah data sehingga memiliki rata-rata 0 dan deviasi standar 1. 1 2 3 4 5 6 7 8 9 from sklearn.preprocessing import StandardScaler, MinMaxScaler # Standarisasi scaler = StandardScaler() df_scaled = scaler.fit_transform(df) # Normalisasi min_max_scaler = MinMaxScaler() df_normalized = min_max_scaler.fit_transform(df) 3. Pengkodean Kategorikal Data kategorikal harus dikonversi ke format numerik agar dapat digunakan dalam model machine learning. Ada beberapa cara untuk melakukan ini:\nLabel Encoding: Mengonversi kategori menjadi angka. One-Hot Encoding: Membuat kolom biner untuk setiap kategori. 1 2 3 4 5 6 7 8 from sklearn.preprocessing import LabelEncoder, OneHotEncoder # Label Encoding encoder = LabelEncoder() df[\u0026#39;encoded_column\u0026#39;] = encoder.fit_transform(df[\u0026#39;category_column\u0026#39;]) # One-Hot Encoding df_encoded = pd.get_dummies(df[\u0026#39;category_column\u0026#39;]) 4. Deteksi dan Penanganan Outlier Outlier adalah nilai ekstrem yang bisa mempengaruhi model dan analisis. Ada beberapa cara untuk menangani outlier:\nMenghapus Outlier: Menghapus nilai yang terlalu jauh dari rata-rata. Transformasi: Menggunakan transformasi matematis seperti log atau kuadrat untuk meredam efek outlier. 1 2 3 4 5 6 7 # Deteksi outlier menggunakan IQR Q1 = df[\u0026#39;column\u0026#39;].quantile(0.25) Q3 = df[\u0026#39;column\u0026#39;].quantile(0.75) IQR = Q3 - Q1 # Menghapus outlier df_no_outliers = df[(df[\u0026#39;column\u0026#39;] \u0026gt;= (Q1 - 1.5 * IQR)) \u0026amp; (df[\u0026#39;column\u0026#39;] \u0026lt;= (Q3 + 1.5 * IQR))] 5. Pembagian Data untuk Pelatihan dan Pengujian Sebelum melatih model, penting untuk membagi data menjadi dua set: data pelatihan dan data pengujian. Pembagian yang umum adalah 80% untuk pelatihan dan 20% untuk pengujian.\n1 2 3 4 from sklearn.model_selection import train_test_split # Membagi data X_train, X_test, y_train, y_test = train_test_split(df.drop(\u0026#39;target\u0026#39;, axis=1), df[\u0026#39;target\u0026#39;], test_size=0.2, random_state=42) Kesimpulan Preprocessing data adalah langkah krusial dalam pipeline machine learning. Dengan menangani nilai yang hilang, normalisasi, pengkodean kategorikal, dan deteksi outlier, kita dapat memastikan bahwa model machine learning kita dilatih dengan data yang berkualitas tinggi. Meskipun preprocessing bisa memakan waktu, langkah ini sangat berpengaruh terhadap kinerja dan akurasi model yang dihasilkan. Pastikan untuk selalu memeriksa dataset Anda dan menerapkan teknik preprocessing yang tepat untuk mencapai hasil yang terbaik.\n","permalink":"http://localhost:1313/blogs/cara-preprocessing-data-dengan-baik/","summary":"Exploratory Data Analysis of Amazon\u0026rsquo;s top 50 bestselling books 2009 - 2019","title":"Cara Preprocessing Data dengan Baik"},{"content":"Introduction The Nota Fiscal Goiana is an economic program implemented by the state of Goiás, Brazil. This program is designed to boost the collection of the Tax on Circulation of Goods and Services (ICMS) by encouraging people to ask for receipts when they buy goods and services within the state. Additionally, the program includes monthly raffles that distribute cash prizes to participating citizens.\nAs I regularly checked the Nota Fiscal Goiana portal and realized the website\u0026rsquo;s simplicity, I decided to write a scraper to collect the raffle results. Initially, I aimed to simplify the process of checking if I had won, but as I delved deeper, I realized the potential to analyze the dataset more comprehensively.\nEarly Design and Idea Phase The original idea was to scrape raffle results, store them in a database, and give people the option to access the data directly from the database or just view it through a good-looking Power BI dashboard.\nThis led me to start writing the code for scraping the website and parsing the tables. However, since there were approximately 50 published results not available on the website, I had to resort to checking the Diário Oficial do Estado de Goiás, where the official government documents of Goiás are published in Brazil.\nChallenges Encountered Not all results were completely published on the Diário Oficial, leading to some results being unavailable.\nThe project doesn\u0026rsquo;t have a budget, so automation can\u0026rsquo;t require VM and online databases. Which Power BI requires both.\nSolutions The results that I could find published on the Diário Oficial were saved in the database along with results available on the website.\nInitially, I was using free options of online databases, and the backup of the old data was on an SQLite available on the project repository. However, I realized that when running the scraper I could save the data on the SQLite and commit the changes, when I did the information persisted on the SQLite file and I could just read it.\nTo help automate the dashboard I resorted to using Streamlit instead of Power BI. Because Streamlit allows me to read the SQLite file directly from the GitHub repository and every time someone opens the Streamlit dashboard it reads the current data, so I don\u0026rsquo;t need to set up refreshes.\nCurrent State of the Project Currently, the project extracts monthly the raffle results from the Nota Fiscal Goiana and the state ICMS collection and saves the results in the SQLite file, using GitHub Actions. The Streamlit dashboard reads the SQLite file and displays the information on the Streamlit Community Cloud.\nLessons Learned While the Power BI dashboard was useful, I faced challenges with refreshing it easily. Initially using an SQLite database, I couldn\u0026rsquo;t connect to the file on the repository, necessitating manual updates on my local machine. To overcome this, I explored alternatives like MySQL and PostgreSQL, enabling automated monthly scraping with GitHub Actions and updating the dashboard through the Power BI website.\nThe SQLite database is really helpful and depending on the size and scope of your project, you should look at it as a possible option to store your data.\nGoing forward I intend to maintain the project, making future patches to the scraping script if the website changes. And improving the code as my knowledge matures.\nExplore Further GitHub Repository: Access the project\u0026rsquo;s source files and codes on our GitHub Repository. Currently, everything is in Portuguese, but I plan to translate it in the future.\nStreamlit Dashboard: Explore the live Streamlit dashboard here to interact with the project\u0026rsquo;s data visualization directly.\nAcademic Article: If you\u0026rsquo;re interested in a detailed academic analysis of this project, check out our article here. Please note that the article is in Portuguese.\n","permalink":"http://localhost:1313/blogs/bts-nota-fiscal-goiana/","summary":"A behind-the-scenes decision on the decision-making process of the project Nota Fiscal Goiana.","title":"[BTS] Nota Fiscal Goiana"},{"content":"Have you ever had a purchase blocked by your credit card issuer without you taking any action? If this happened to you, your credit card issuer probably had detected that your purchase didn\u0026rsquo;t match your normal spending habit, maybe you were at a different location or spend a lot of money suddenly. Anyway, the credit card company was using anomaly detection techniques to prevent fraudulent activities.\nWhat is Anomaly Detection? Anomaly detection refers to the problem of finding patterns in data that do not conform to expected behavior. 1\nIn statistics there is a famous distribution that is called \u0026ldquo;The Normal Distribution\u0026rdquo;, this name is not in vain. A lot of random things are normally distributed, for instance, if you select 100 random people and measure their height, and made a distribution plot, you\u0026rsquo;d end up with something looking like a symmetrical bell curve.\nEven though abnormal results are possible, it is expected that most people or things will fall within a normal distribution. When something is divergent from this normality it is said to be an outlier or an anomaly.\nWhen dealing with something like credit card fraud you can create an expectation of a person expanding habits, so if your daily routine consists of waking up and driving to your workplace and buying a coffee, then lunch in New York. Your credit card company would find it weird if you suddenly made a physical purchase in Rio de Janeiro.\nIn simple terms, that is what anomaly detection is trying to do, understand how something works so it can identify if it starts acting weird.\nAnomaly Detection x Class Imbalance in Datasets If you are aware of what class imbalance is, you may be wondering \u0026ldquo;can\u0026rsquo;t I use anomaly detection to solve this class imbalance problem?\u0026rdquo; - sometimes you can, but it\u0026rsquo;s not appropriate.\nClass imbalance occurs when one class has significantly more samples than the other class(es). This can lead to biased models that favor the majority class and perform poorly on minority classes. While anomaly detection focuses on identifying rare or abnormal data points that deviate significantly from the normal patterns in the dataset.\nEven though it has some overlap on what is trying to do, unless you have a class imbalance problem that the minority class can be considered an abnormal behavior, you won\u0026rsquo;t be able to use anomaly detection.\nI talked about class imbalance before on a churn rate problem in a fictitious telecommunication company, if you are interested you can check it here.\nChandola, V., Banerjee, A., and Kumar, V. 2009. Anomaly detection: A survey. ACM Comput. Surv. 41, 3, Article 15 (July 2009), 58 pages. DOI = 10.1145/1541880.1541882\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/blogs/anomaly-detection/","summary":"How can your credit card issuer know if a purchase is made by you or a cloned version of your credit card?","title":"What is Anomaly Detection?"}]